{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2315ddc0-c788-4336-a983-215870fabc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Transformation: Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "## The Situation\n",
    "\n",
    "Your leadership team just dropped a bombshell: they want dashboards, interactive \"talk to my data\" capabilities, predictive maintenance models, and AI agent systems - **all by the end of the week**. Your IoT sensors on planes are generating massive amounts of data, and you need to get it production-ready, fast.\n",
    "\n",
    "Good news: Databricks has **Lakeflow Spark Declarative Pipelines** (formerly Delta Live Tables) that can help you build reliable, production-ready data pipelines in minutes, not days.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "âœ… What declarative pipelines are and why they matter  \n",
    "âœ… Create a streaming table to ingest and clean sensor data  \n",
    "âœ… Create a materialized view for aggregated metrics  \n",
    "âœ… Deploy your pipeline to production  \n",
    "\n",
    "**Time to Complete:** 20-30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## What are Declarative Pipelines?\n",
    "\n",
    "Instead of writing complex code to manage checkpoints, handle incremental processing, and track data quality, you simply declare **what** you want. The framework automatically handles:\n",
    "\n",
    "- âœ… **Incremental processing** - Only process new/changed data\n",
    "- âœ… **Dependency management** - Determine execution order automatically\n",
    "- âœ… **Data quality** - Built-in validation with expectations\n",
    "- âœ… **Monitoring** - Automatic lineage and observability\n",
    "- âœ… **Recovery** - Checkpoint management and error handling\n",
    "\n",
    "### Two Key Building Blocks\n",
    "\n",
    "**1. Streaming Tables**\n",
    "- For incremental, append-only data processing\n",
    "- Perfect for ingesting raw sensor data and cleaning it\n",
    "- Each row processed exactly once\n",
    "\n",
    "**2. Materialized Views**\n",
    "- For aggregations that need to update (not just append)\n",
    "- Perfect for business metrics and KPIs\n",
    "- Always return correct, up-to-date results\n",
    "\n",
    "---\n",
    "\n",
    "**Reference:** [Lakeflow Pipelines Documentation](https://docs.databricks.com/aws/en/ldp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c17d73f-2792-4e4e-9235-ab4d0d39a0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Create Your Streaming Table\n",
    "\n",
    "Let's build a streaming table that:\n",
    "1. Ingests raw sensor data from the volume\n",
    "2. Cleans the data (fixes negative air pressure values)\n",
    "3. Validates data quality with expectations\n",
    "\n",
    "This will be your **silver layer** - cleaned, validated sensor data ready for analysis.\n",
    "\n",
    "### Python Version\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import col, when, abs as abs_func, current_timestamp\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_silver\",\n",
    "    comment=\"Cleaned and validated aircraft sensor readings\",\n",
    "    # Data quality rules - track violations\n",
    "    expect={\n",
    "        \"valid_device_id\": \"device_id IS NOT NULL\",\n",
    "        \"valid_timestamp\": \"timestamp IS NOT NULL\",\n",
    "        \"valid_temperature_range\": \"temperature BETWEEN -50 AND 150\"\n",
    "    },\n",
    "    # Drop rows that fail critical validation\n",
    "    expect_or_drop={\n",
    "        \"positive_air_pressure\": \"air_pressure > 0\"\n",
    "    }\n",
    ")\n",
    "def sensor_silver():\n",
    "    \"\"\"\n",
    "    Ingest and clean sensor data with Auto Loader.\n",
    "    Auto Loader automatically handles:\n",
    "    - Schema inference\n",
    "    - New file discovery  \n",
    "    - Exactly-once processing\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/default/db_crash_course/sensor_data/\")\n",
    "        # Fix data quality issue: negative air pressure\n",
    "        .withColumn(\"air_pressure\",\n",
    "                   when(col(\"air_pressure\") < 0, abs_func(col(\"air_pressure\")))\n",
    "                   .otherwise(col(\"air_pressure\")))\n",
    "        # Add processing timestamp\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "```\n",
    "\n",
    "**What's happening here:**\n",
    "- `@dp.table` decorator defines a streaming table\n",
    "- `expect` tracks data quality violations (logs them but doesn't drop rows)\n",
    "- `expect_or_drop` drops rows that fail critical validation\n",
    "- Auto Loader (`cloudFiles`) automatically discovers new CSV files\n",
    "- We fix negative air pressure values inline\n",
    "- Framework handles all the checkpointing and incremental processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69185727-cd67-49dc-9887-9c2abd3943c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL Version (Same Logic)\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_silver (\n",
    "  CONSTRAINT valid_device_id EXPECT (device_id IS NOT NULL),\n",
    "  CONSTRAINT valid_timestamp EXPECT (timestamp IS NOT NULL),\n",
    "  CONSTRAINT valid_temperature_range EXPECT (temperature BETWEEN -50 AND 150),\n",
    "  CONSTRAINT positive_air_pressure EXPECT (air_pressure > 0)\n",
    ")\n",
    "COMMENT 'Cleaned and validated aircraft sensor readings'\n",
    "AS SELECT\n",
    "  device_id,\n",
    "  trip_id,\n",
    "  factory_id,\n",
    "  model_id,\n",
    "  timestamp,\n",
    "  airflow_rate,\n",
    "  rotation_speed,\n",
    "  CASE \n",
    "    WHEN air_pressure < 0 THEN ABS(air_pressure)\n",
    "    ELSE air_pressure\n",
    "  END as air_pressure,\n",
    "  temperature,\n",
    "  delay,\n",
    "  density,\n",
    "  current_timestamp() as processed_at\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/default/db_crash_course/sensor_data/',\n",
    "  format => 'csv',\n",
    "  header => 'true'\n",
    ");\n",
    "```\n",
    "\n",
    "âœ… **Result:** Clean, validated sensor data streaming into your silver table automatically as new files arrive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae3b616-0ee3-4219-8946-9c3753480e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Create Your Materialized View\n",
    "\n",
    "Now let's create aggregated metrics that your dashboards and Genie spaces will use. We'll build a **gold layer** materialized view with factory-level KPIs.\n",
    "\n",
    "Why a materialized view? Because we need aggregations that **update** when new data arrives (not just append).\n",
    "\n",
    "### Python Version\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import avg, max, count, countDistinct, round as spark_round\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"factory_kpis_gold\",\n",
    "    comment=\"Factory-level KPIs for aircraft maintenance dashboards\"\n",
    ")\n",
    "def factory_kpis_gold():\n",
    "    \"\"\"\n",
    "    Aggregate factory-level metrics.\n",
    "    This materialized view automatically recomputes when new data arrives.\n",
    "    Perfect for dashboards and reporting!\n",
    "    \"\"\"\n",
    "    # Read from silver layer (cleaned data)\n",
    "    sensors = spark.read.table(\"sensor_silver\")\n",
    "    \n",
    "    # Join with factory dimension for context\n",
    "    factories = spark.read.table(\"default.db_crash_course.dim_factories\")\n",
    "    \n",
    "    enriched = sensors.join(factories, \"factory_id\", \"left\")\n",
    "    \n",
    "    # Calculate factory-level KPIs\n",
    "    return (\n",
    "        enriched\n",
    "        .groupBy(\"factory_id\", \"factory_name\", \"region\", \"city\")\n",
    "        .agg(\n",
    "            countDistinct(\"device_id\").alias(\"total_devices\"),\n",
    "            count(\"*\").alias(\"total_readings\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "            spark_round(max(\"temperature\"), 2).alias(\"max_temperature\"),\n",
    "            spark_round(avg(\"air_pressure\"), 2).alias(\"avg_air_pressure\"),\n",
    "            spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "- `@dp.materialized_view` defines a view that auto-updates\n",
    "- We read from the silver table (already cleaned)\n",
    "- Join with factory dimensions for context\n",
    "- Calculate KPIs: device counts, avg/max temperature, etc.\n",
    "- These metrics automatically update as new sensor data arrives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbb69f13-94ae-4c45-8dbc-48eadd0bf8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL Version (Same Logic)\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH MATERIALIZED VIEW factory_kpis_gold\n",
    "COMMENT 'Factory-level KPIs for aircraft maintenance dashboards'\n",
    "AS SELECT\n",
    "  s.factory_id,\n",
    "  f.factory_name,\n",
    "  f.region,\n",
    "  f.city,\n",
    "  COUNT(DISTINCT s.device_id) as total_devices,\n",
    "  COUNT(*) as total_readings,\n",
    "  ROUND(AVG(s.temperature), 2) as avg_temperature,\n",
    "  ROUND(MAX(s.temperature), 2) as max_temperature,\n",
    "  ROUND(AVG(s.air_pressure), 2) as avg_air_pressure,\n",
    "  ROUND(AVG(s.rotation_speed), 2) as avg_rotation_speed\n",
    "FROM sensor_silver s\n",
    "LEFT JOIN default.db_crash_course.dim_factories f ON s.factory_id = f.factory_id\n",
    "GROUP BY s.factory_id, f.factory_name, f.region, f.city;\n",
    "```\n",
    "\n",
    "âœ… **Result:** Always-current factory KPIs ready for your dashboards, Genie spaces, and reports!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a821a65-143c-4190-9c5b-56263c2b5b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Deploy Your Pipeline\n",
    "\n",
    "Now let's get this into production! Here's how to create and deploy your pipeline.\n",
    "\n",
    "### Create the Pipeline in the UI\n",
    "\n",
    "1. **Click** `New` â†’ `ETL Pipeline` in the Databricks workspace\n",
    "\n",
    "2. **Configure:**\n",
    "   - **Name:** `iot_sensor_pipeline`\n",
    "   - **Target Catalog:** `default`\n",
    "   - **Target Schema:** `db_crash_course`\n",
    "   - **Language:** Choose Python or SQL\n",
    "\n",
    "3. **Paste your code:**\n",
    "   - Copy the complete Python or SQL code from the next cell\n",
    "   - Paste into the pipeline editor\n",
    "   - The editor will show you a visual graph of your pipeline\n",
    "\n",
    "4. **Configure pipeline settings:**\n",
    "   - **Mode:** \n",
    "     - `Triggered` - Runs on schedule or manual trigger (good for learning)\n",
    "     - `Continuous` - Always running, processes data immediately (production)\n",
    "   - **Cluster:** Accept defaults (auto-scaling recommended)\n",
    "\n",
    "5. **Start the pipeline:**\n",
    "   - Click `Start`\n",
    "   - Monitor progress in the pipeline graph\n",
    "   - View data quality metrics in real-time\n",
    "   - Check expectation violations in the dashboard\n",
    "\n",
    "### What You'll See\n",
    "\n",
    "The pipeline graph shows:\n",
    "- **Nodes:** Your streaming table and materialized view\n",
    "- **Edges:** Data flow between them\n",
    "- **Metrics:** Row counts, data quality, processing time\n",
    "- **Status:** Running, completed, or errors\n",
    "\n",
    "### Monitoring\n",
    "\n",
    "After deployment, you get automatic:\n",
    "- âœ… **Data quality dashboard** - Expectation pass/fail rates\n",
    "- âœ… **Event log** - Detailed execution history\n",
    "- âœ… **Lineage graph** - Visual data flow\n",
    "- âœ… **Performance metrics** - Processing speed, cluster utilization\n",
    "\n",
    "---\n",
    "\n",
    "**Reference:** [Multi-File Editor](https://docs.databricks.com/aws/en/ldp/multi-file-editor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "577a0c55-621c-4028-82ab-24a92fbb4b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Complete Pipeline Code\n",
    "\n",
    "Here's everything in one file you can copy/paste into the Lakeflow Pipeline Editor:\n",
    "\n",
    "### Python Complete Pipeline\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "IoT Aircraft Sensor Pipeline\n",
    "Complete declarative pipeline for production-ready sensor data processing\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, abs as abs_func, current_timestamp,\n",
    "    avg, max, count, countDistinct, round as spark_round\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# SILVER LAYER - Cleaned Sensor Data\n",
    "# ========================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_silver\",\n",
    "    comment=\"Cleaned and validated aircraft sensor readings\",\n",
    "    expect={\n",
    "        \"valid_device_id\": \"device_id IS NOT NULL\",\n",
    "        \"valid_timestamp\": \"timestamp IS NOT NULL\",\n",
    "        \"valid_temperature_range\": \"temperature BETWEEN -50 AND 150\"\n",
    "    },\n",
    "    expect_or_drop={\n",
    "        \"positive_air_pressure\": \"air_pressure > 0\"\n",
    "    }\n",
    ")\n",
    "def sensor_silver():\n",
    "    \"\"\"Ingest and clean sensor data with Auto Loader.\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/default/db_crash_course/sensor_data/\")\n",
    "        .withColumn(\"air_pressure\",\n",
    "                   when(col(\"air_pressure\") < 0, abs_func(col(\"air_pressure\")))\n",
    "                   .otherwise(col(\"air_pressure\")))\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# ========================================\n",
    "# GOLD LAYER - Factory KPIs\n",
    "# ========================================\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"factory_kpis_gold\",\n",
    "    comment=\"Factory-level KPIs for aircraft maintenance dashboards\"\n",
    ")\n",
    "def factory_kpis_gold():\n",
    "    \"\"\"Aggregate factory-level metrics.\"\"\"\n",
    "    sensors = spark.read.table(\"sensor_silver\")\n",
    "    factories = spark.read.table(\"default.db_crash_course.dim_factories\")\n",
    "    \n",
    "    enriched = sensors.join(factories, \"factory_id\", \"left\")\n",
    "    \n",
    "    return (\n",
    "        enriched\n",
    "        .groupBy(\"factory_id\", \"factory_name\", \"region\", \"city\")\n",
    "        .agg(\n",
    "            countDistinct(\"device_id\").alias(\"total_devices\"),\n",
    "            count(\"*\").alias(\"total_readings\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "            spark_round(max(\"temperature\"), 2).alias(\"max_temperature\"),\n",
    "            spark_round(avg(\"air_pressure\"), 2).alias(\"avg_air_pressure\"),\n",
    "            spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"âœ… Pipeline defined! Deploy in the Lakeflow Pipelines Editor.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48c5977-165f-4e72-8d63-9a49898f0d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL Complete Pipeline\n",
    "\n",
    "```sql\n",
    "-- IoT Aircraft Sensor Pipeline (SQL Version)\n",
    "\n",
    "-- SILVER LAYER: Cleaned sensor data\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_silver (\n",
    "  CONSTRAINT valid_device_id EXPECT (device_id IS NOT NULL),\n",
    "  CONSTRAINT valid_timestamp EXPECT (timestamp IS NOT NULL),\n",
    "  CONSTRAINT valid_temperature_range EXPECT (temperature BETWEEN -50 AND 150),\n",
    "  CONSTRAINT positive_air_pressure EXPECT OR DROP (air_pressure > 0)\n",
    ")\n",
    "COMMENT 'Cleaned and validated aircraft sensor readings'\n",
    "AS SELECT\n",
    "  device_id,\n",
    "  trip_id,\n",
    "  factory_id,\n",
    "  model_id,\n",
    "  timestamp,\n",
    "  airflow_rate,\n",
    "  rotation_speed,\n",
    "  CASE \n",
    "    WHEN air_pressure < 0 THEN ABS(air_pressure)\n",
    "    ELSE air_pressure\n",
    "  END as air_pressure,\n",
    "  temperature,\n",
    "  delay,\n",
    "  density,\n",
    "  current_timestamp() as processed_at\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/default/db_crash_course/sensor_data/',\n",
    "  format => 'csv',\n",
    "  header => 'true'\n",
    ");\n",
    "\n",
    "-- GOLD LAYER: Factory KPIs\n",
    "CREATE OR REFRESH MATERIALIZED VIEW factory_kpis_gold\n",
    "COMMENT 'Factory-level KPIs for aircraft maintenance dashboards'\n",
    "AS SELECT\n",
    "  s.factory_id,\n",
    "  f.factory_name,\n",
    "  f.region,\n",
    "  f.city,\n",
    "  COUNT(DISTINCT s.device_id) as total_devices,\n",
    "  COUNT(*) as total_readings,\n",
    "  ROUND(AVG(s.temperature), 2) as avg_temperature,\n",
    "  ROUND(MAX(s.temperature), 2) as max_temperature,\n",
    "  ROUND(AVG(s.air_pressure), 2) as avg_air_pressure,\n",
    "  ROUND(AVG(s.rotation_speed), 2) as avg_rotation_speed\n",
    "FROM sensor_silver s\n",
    "LEFT JOIN default.db_crash_course.dim_factories f ON s.factory_id = f.factory_id\n",
    "GROUP BY s.factory_id, f.factory_name, f.region, f.city;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95998f78-8e25-483a-85ef-d03bb547f41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've built a production-ready data pipeline with:\n",
    "\n",
    "âœ… **Streaming table** - Automatically ingests and cleans sensor data  \n",
    "âœ… **Data quality** - Tracks and enforces expectations  \n",
    "âœ… **Materialized view** - Always-current factory KPIs  \n",
    "âœ… **Deployment** - Running in production with monitoring  \n",
    "\n",
    "### What You Get For Free\n",
    "\n",
    "The framework automatically provides:\n",
    "- âœ… Incremental processing (only new data)\n",
    "- âœ… Checkpoint management (exactly-once guarantees)\n",
    "- âœ… Schema evolution (handles new columns gracefully)\n",
    "- âœ… Data quality tracking (expectations dashboard)\n",
    "- âœ… Lineage visualization (see data flow)\n",
    "- âœ… Error recovery (automatic retries)\n",
    "- âœ… Monitoring (performance and health metrics)\n",
    "\n",
    "### Next Steps for Your Week\n",
    "\n",
    "Now that you have clean, aggregated data:\n",
    "\n",
    "1. **Dashboards** - Use `factory_kpis_gold` for visualizations\n",
    "2. **Genie** - Connect to `sensor_silver` for natural language queries\n",
    "3. **AutoML** - Use `sensor_silver` for predictive maintenance models\n",
    "4. **Agents** - Build on clean data for AI systems\n",
    "\n",
    "You're well on your way to meeting leadership's deadline! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed69df8-f2f1-4f61-b166-1393e336a0cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Try This Out: Extend Your Pipeline\n",
    "\n",
    "Want to learn more? Here are some ideas to explore:\n",
    "\n",
    "### 1. Add a Bronze Layer\n",
    "Create a raw ingestion table before cleaning:\n",
    "\n",
    "```python\n",
    "@dp.table(name=\"sensor_bronze\")\n",
    "def sensor_bronze():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/default/db_crash_course/sensor_data/\")\n",
    "    )\n",
    "\n",
    "# Then update sensor_silver to read from sensor_bronze\n",
    "@dp.table(name=\"sensor_silver\")\n",
    "def sensor_silver():\n",
    "    return dp.read_stream(\"sensor_bronze\").withColumn(...)\n",
    "```\n",
    "\n",
    "### 2. Add Device-Level Metrics\n",
    "Create another materialized view for per-device KPIs:\n",
    "\n",
    "```python\n",
    "@dp.materialized_view(name=\"device_health_gold\")\n",
    "def device_health_gold():\n",
    "    return (\n",
    "        spark.read.table(\"sensor_silver\")\n",
    "        .groupBy(\"device_id\", \"factory_id\")\n",
    "        .agg(\n",
    "            avg(\"temperature\").alias(\"avg_temp\"),\n",
    "            count(\"*\").alias(\"reading_count\")\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "### 3. Add Inspection Data\n",
    "Create a parallel streaming table for inspection records:\n",
    "\n",
    "```python\n",
    "@dp.table(name=\"inspection_silver\")\n",
    "def inspection_silver():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/default/db_crash_course/inspection_data/\")\n",
    "    )\n",
    "```\n",
    "\n",
    "### 4. Explore Advanced Features\n",
    "\n",
    "- **Flows** - Multiple sources writing to one table\n",
    "- **CDC** - Handle change data capture\n",
    "- **Watermarks** - Handle late-arriving data\n",
    "- **Multi-file organization** - Separate bronze/silver/gold into different files\n",
    "\n",
    "**Documentation:**\n",
    "- [Flows](https://docs.databricks.com/aws/en/ldp/flows)\n",
    "- [Streaming Tables](https://docs.databricks.com/aws/en/ldp/streaming-tables)\n",
    "- [Materialized Views](https://docs.databricks.com/aws/en/ldp/materialized-views)\n",
    "- [Multi-File Editor](https://docs.databricks.com/aws/en/ldp/multi-file-editor)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "5 Data Transformation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
