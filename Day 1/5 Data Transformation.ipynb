{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2315ddc0-c788-4336-a983-215870fabc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Transformation: Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "## The Situation\n",
    "\n",
    "Your leadership team just dropped a bombshell: they want dashboards, interactive \"talk to my data\" capabilities, predictive maintenance models, and AI agent systems - **all by the end of the week**. Your IoT sensors on planes are generating massive amounts of data, and you need to get it production-ready, fast.\n",
    "\n",
    "Good news: Databricks has **Lakeflow Spark Declarative Pipelines** that can help you build reliable, production-ready data pipelines in minutes, not days.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "âœ… What declarative pipelines are and why they matter  \n",
    "âœ… Create a streaming table to ingest and clean sensor data  \n",
    "âœ… Create a materialized view for aggregated metrics  \n",
    "âœ… Deploy your pipeline to production  \n",
    "\n",
    "---\n",
    "\n",
    "## What are Lakeflow Spark Declarative Pipelines?\n",
    "\n",
    "Lakeflow Spark Declarative Pipelines (formerly Delta Live Tables) is a framework for building reliable, production-ready data pipelines. Instead of writing complex code to manage checkpoints, handle incremental processing, and track data quality, you simply declare **what** you want using either SQL or Python. The framework automatically handles:\n",
    "\n",
    "- âœ… **Incremental processing** - Only process new/changed data\n",
    "- âœ… **Dependency management** - Determine execution order automatically\n",
    "- âœ… **Data quality** - Built-in validation with expectations\n",
    "- âœ… **Monitoring** - Automatic lineage and observability\n",
    "- âœ… **Recovery** - Checkpoint management and error handling\n",
    "\n",
    "### Two Key Building Blocks\n",
    "\n",
    "**1. Streaming Tables**\n",
    "- For incremental, append-only data processing\n",
    "- Perfect for ingesting raw sensor data and cleaning it\n",
    "- Uses Auto Loader to automatically discover new files\n",
    "- Each row processed exactly once (exactly-once semantics)\n",
    "\n",
    "**2. Materialized Views**\n",
    "- For aggregations that need to update (not just append)\n",
    "- Perfect for business metrics and KPIs\n",
    "- Always return correct, up-to-date results\n",
    "- Automatically recompute when source data changes\n",
    "\n",
    "### About This Tutorial\n",
    "\n",
    "This tutorial explains how to create and deploy an ETL (extract, transform, and load) pipeline for data orchestration using Lakeflow Spark Declarative Pipelines and Auto Loader. You will use pipelines to:\n",
    "\n",
    "- Ingest raw sensor data from a volume into a streaming table\n",
    "- Transform and validate the data with data quality expectations\n",
    "- Create aggregated views for dashboards and analytics\n",
    "- Perform ad-hoc queries on the processed data\n",
    "- Automate the ETL pipeline with a scheduled job\n",
    "\n",
    "---\n",
    "\n",
    "**Reference:** [Lakeflow Pipelines Documentation](https://learn.microsoft.com/en-us/azure/databricks/ldp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f79fb506-5972-4f35-9d69-6d2b55fe05ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Create Your Pipeline\n",
    "\n",
    "Follow these steps to create a new Lakeflow Spark Declarative Pipeline:\n",
    "\n",
    "1. In your workspace, click **New** (Plus icon) in the sidebar, then select **ETL Pipeline**.\n",
    "2. Give your pipeline a unique name: `IoT Sensor Pipeline`\n",
    "3. Just below the name, select the default catalog and schema for the data that you generate:\n",
    "   - **Catalog:** `dwx_airops_insights_platform_dev_working`\n",
    "   - **Schema:** Your personal schema (should look like `jane_doe` - your username with special characters replaced with underscores)\n",
    "   - Note: You must have `ALL PRIVILEGES` or `USE CATALOG` and `CREATE SCHEMA` permissions\n",
    "4. For this tutorial, select **Start with an empty file**.\n",
    "5. In **Folder path**, specify a location for your source files, or accept the default (your user folder).\n",
    "6. Choose **Python** or **SQL** as the language for your first source file (a pipeline can mix and match languages, but each file must be in a single language).\n",
    "7. Click **Select**.\n",
    "\n",
    "The pipeline editor appears for the new pipeline. An empty source file for your language is created, ready for your first transformation.\n",
    "\n",
    "### What You'll See\n",
    "\n",
    "The **Lakeflow Pipelines Editor** has three main components:\n",
    "- **Left panel**: Code editor where you write your pipeline logic\n",
    "- **Right panel**: Pipeline graph (DAG) showing your data flow\n",
    "- **Bottom panel**: Table preview and data results\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** This tutorial uses serverless compute and Unity Catalog. For all configuration options that are not specified, use the default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c17d73f-2792-4e4e-9235-ab4d0d39a0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Develop Your Streaming Table\n",
    "\n",
    "In this step, you will use the **Lakeflow Pipelines Editor** to develop and validate source code for the pipeline interactively.\n",
    "\n",
    "The code uses **Auto Loader** for incremental data ingestion. Auto Loader automatically detects and processes new files as they arrive in cloud object storage.\n",
    "\n",
    "A blank source code file is automatically created and configured for the pipeline. The file is created in the **transformations** folder of your pipeline. By default, all `*.py` and `*.sql` files in the transformations folder are part of the source for your pipeline.\n",
    "\n",
    "Let's build a streaming table that:\n",
    "1. Ingests raw sensor data from the volume using Auto Loader\n",
    "2. Cleans the data (fixes negative air pressure values)\n",
    "3. Validates data quality with expectations\n",
    "\n",
    "This will be your **silver layer** - cleaned, validated sensor data ready for analysis.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Copy and paste the code below into your source file. Be sure to use the language that you selected for the file in Step 1.\n",
    "2. Click Play icon **Run file** or **Run pipeline** to start an update for the connected pipeline.\n",
    "\n",
    "When the update completes, the editor is updated with information about your pipeline:\n",
    "- The **pipeline graph (DAG)**, in the sidebar to the right of your code, shows your `sensor_silver` table\n",
    "- A **summary of the update** is shown at the top of the pipeline assets browser\n",
    "- **Details of the tables** that were generated are shown in the bottom pane, and you can browse data from the tables by selecting one\n",
    "\n",
    "---\n",
    "\n",
    "### SQL Version\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_silver (\n",
    "  CONSTRAINT valid_device_id EXPECT (device_id IS NOT NULL),\n",
    "  CONSTRAINT valid_timestamp EXPECT (timestamp IS NOT NULL),\n",
    "  CONSTRAINT valid_temperature_range EXPECT (temperature BETWEEN -50 AND 150),\n",
    "  CONSTRAINT positive_air_pressure EXPECT (air_pressure > 0)\n",
    ")\n",
    "COMMENT 'Cleaned and validated aircraft sensor readings'\n",
    "AS SELECT\n",
    "  device_id,\n",
    "  trip_id,\n",
    "  factory_id,\n",
    "  model_id,\n",
    "  timestamp,\n",
    "  airflow_rate,\n",
    "  rotation_speed,\n",
    "  CASE \n",
    "    WHEN air_pressure < 0 THEN ABS(air_pressure)\n",
    "    ELSE air_pressure\n",
    "  END as air_pressure,\n",
    "  temperature,\n",
    "  delay,\n",
    "  density,\n",
    "  current_timestamp() as processed_at\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/dwx_airops_insights_platform_dev_working/db_crash_course/sensor_data/',\n",
    "  format => 'csv',\n",
    "  header => 'true'\n",
    ");\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9d4637d-f369-487d-b95b-f8ee7659b007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Python Version\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import col, when, abs as abs_func, current_timestamp\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_silver\",\n",
    "    comment=\"Cleaned and validated aircraft sensor readings\",\n",
    "    # Data quality rules - track violations\n",
    "    expect={\n",
    "        \"valid_device_id\": \"device_id IS NOT NULL\",\n",
    "        \"valid_timestamp\": \"timestamp IS NOT NULL\",\n",
    "        \"valid_temperature_range\": \"temperature BETWEEN -50 AND 150\"\n",
    "    },\n",
    "    # Drop rows that fail critical validation\n",
    "    expect_or_drop={\n",
    "        \"positive_air_pressure\": \"air_pressure > 0\"\n",
    "    }\n",
    ")\n",
    "def sensor_silver():\n",
    "    \"\"\"\n",
    "    Ingest and clean sensor data with Auto Loader.\n",
    "    Auto Loader automatically handles:\n",
    "    - Schema inference\n",
    "    - New file discovery  \n",
    "    - Exactly-once processing\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/dwx_airops_insights_platform_dev_working/db_crash_course/sensor_data/\")\n",
    "        # Fix data quality issue: negative air pressure\n",
    "        .withColumn(\"air_pressure\",\n",
    "                   when(col(\"air_pressure\") < 0, abs_func(col(\"air_pressure\")))\n",
    "                   .otherwise(col(\"air_pressure\")))\n",
    "        # Add processing timestamp\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What's Happening Here\n",
    "\n",
    "**For SQL version:**\n",
    "- `CREATE OR REFRESH STREAMING TABLE` defines a table that processes streaming data\n",
    "- `CONSTRAINT ... EXPECT` validates data quality (rows that fail are logged but not dropped)\n",
    "- `STREAM read_files()` uses Auto Loader to automatically discover new CSV files\n",
    "- `CASE WHEN` fixes negative air pressure values inline\n",
    "- Framework handles all the checkpointing and incremental processing\n",
    "\n",
    "**For Python version:**\n",
    "- `@dp.table` decorator defines a streaming table\n",
    "- `expect` tracks data quality violations (logs them but doesn't drop rows)\n",
    "- `expect_or_drop` drops rows that fail critical validation\n",
    "- Auto Loader (`cloudFiles`) automatically discovers new CSV files\n",
    "- We fix negative air pressure values inline using `.withColumn()`\n",
    "- Framework handles all the checkpointing and incremental processing\n",
    "\n",
    "âœ… **Result:** Clean, validated sensor data streaming into your silver table automatically as new files arrive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae3b616-0ee3-4219-8946-9c3753480e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Add Your Materialized View\n",
    "\n",
    "Now let's create aggregated metrics that your dashboards and Genie spaces will use. We'll build a **gold layer** materialized view with factory-level KPIs.\n",
    "\n",
    "Why a materialized view? Because we need aggregations that **update** when new data arrives (not just append).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "You can add this query to the same source file, or create a new file to organize your pipeline code. The pipeline will process all queries together.\n",
    "\n",
    "**To add to the same file:**\n",
    "1. Scroll to the bottom of your existing source file\n",
    "2. Copy and paste the code below\n",
    "3. Click Play icon **Run file** or **Run pipeline** to update\n",
    "\n",
    "**To create a new file (optional):**\n",
    "1. From the pipeline assets browser sidebar, click Plus icon **Add** then **Source**\n",
    "2. Choose the same language (SQL or Python) as your first file\n",
    "3. Paste the code below\n",
    "4. Click Play icon **Run file**\n",
    "\n",
    "When the update completes:\n",
    "- The **pipeline graph** now shows two tables: `sensor_silver` and `factory_kpis_gold`\n",
    "- An edge connects them, showing `factory_kpis_gold` reads from `sensor_silver`\n",
    "- You can select `factory_kpis_gold` in the bottom panel to preview the aggregated data\n",
    "\n",
    "---\n",
    "\n",
    "### SQL Version\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH MATERIALIZED VIEW factory_kpis_gold\n",
    "COMMENT 'Factory-level KPIs for aircraft maintenance dashboards'\n",
    "AS SELECT\n",
    "  s.factory_id,\n",
    "  f.factory_name,\n",
    "  f.region,\n",
    "  f.city,\n",
    "  COUNT(DISTINCT s.device_id) as total_devices,\n",
    "  COUNT(*) as total_readings,\n",
    "  ROUND(AVG(s.temperature), 2) as avg_temperature,\n",
    "  ROUND(MAX(s.temperature), 2) as max_temperature,\n",
    "  ROUND(AVG(s.air_pressure), 2) as avg_air_pressure,\n",
    "  ROUND(AVG(s.rotation_speed), 2) as avg_rotation_speed\n",
    "FROM sensor_silver s\n",
    "LEFT JOIN dwx_airops_insights_platform_dev_working.db_crash_course.dim_factories f ON s.factory_id = f.factory_id\n",
    "GROUP BY s.factory_id, f.factory_name, f.region, f.city;\n",
    "```\n",
    "\n",
    "### Python Version\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import avg, max, count, countDistinct, round as spark_round\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"factory_kpis_gold\",\n",
    "    comment=\"Factory-level KPIs for aircraft maintenance dashboards\"\n",
    ")\n",
    "def factory_kpis_gold():\n",
    "    \"\"\"\n",
    "    Aggregate factory-level metrics.\n",
    "    This materialized view automatically recomputes when new data arrives.\n",
    "    Perfect for dashboards and reporting!\n",
    "    \"\"\"\n",
    "    # Read from silver layer (cleaned data)\n",
    "    sensors = spark.read.table(\"sensor_silver\")\n",
    "    \n",
    "    # Join with factory dimension for context\n",
    "    factories = spark.read.table(\"dwx_airops_insights_platform_dev_working.db_crash_course.dim_factories\")\n",
    "    \n",
    "    enriched = sensors.join(factories, \"factory_id\", \"left\")\n",
    "    \n",
    "    # Calculate factory-level KPIs\n",
    "    return (\n",
    "        enriched\n",
    "        .groupBy(\"factory_id\", \"factory_name\", \"region\", \"city\")\n",
    "        .agg(\n",
    "            countDistinct(\"device_id\").alias(\"total_devices\"),\n",
    "            count(\"*\").alias(\"total_readings\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "            spark_round(max(\"temperature\"), 2).alias(\"max_temperature\"),\n",
    "            spark_round(avg(\"air_pressure\"), 2).alias(\"avg_air_pressure\"),\n",
    "            spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What's Happening Here\n",
    "\n",
    "**For SQL version:**\n",
    "- `CREATE OR REFRESH MATERIALIZED VIEW` defines a view that automatically updates\n",
    "- `LEFT JOIN` enriches sensor data with factory dimension data (name, region, city)\n",
    "- `GROUP BY` aggregates metrics at the factory level\n",
    "- `COUNT(DISTINCT ...)` counts unique devices, `AVG()` calculates averages\n",
    "- The view automatically recomputes when new data arrives in `sensor_silver`\n",
    "\n",
    "**For Python version:**\n",
    "- `@dp.materialized_view` defines a view that auto-updates\n",
    "- `spark.read.table(\"sensor_silver\")` reads from the silver table (already cleaned)\n",
    "- `.join()` with factory dimensions for context\n",
    "- `.groupBy().agg()` calculates KPIs: device counts, avg/max temperature, etc.\n",
    "- These metrics automatically update as new sensor data arrives\n",
    "\n",
    "âœ… **Result:** Always-current factory KPIs ready for your dashboards, Genie spaces, and reports!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a821a65-143c-4190-9c5b-56263c2b5b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Explore the Datasets Created by Your Pipeline\n",
    "\n",
    "In this step, you perform ad-hoc queries on the data processed in the ETL pipeline to analyze the sensor data in an exploration notebook.\n",
    "\n",
    "### What is an Exploration Notebook?\n",
    "\n",
    "Exploration notebooks let you perform ad-hoc queries and analysis on your pipeline data without affecting the pipeline itself. These are SQL notebooks stored in an `explorations` folder. Files in the `explorations` folder are **not** run as part of a pipeline update by default.\n",
    "\n",
    "### Create an Exploration Notebook\n",
    "\n",
    "1. From the **pipeline assets browser sidebar**, click Plus icon **Add** then **Exploration**.\n",
    "2. Enter a **Name**: `Sensor Analysis`\n",
    "3. Select **SQL** for the exploration file.\n",
    "4. A SQL notebook is created in a new `explorations` folder.\n",
    "\n",
    "The SQL notebook has cells that you can run together or separately.\n",
    "\n",
    "### Query 1: Find High-Temperature Factories\n",
    "\n",
    "Let's find which factories have the highest average temperatures.\n",
    "\n",
    "1. Copy the following code into the first cell of your exploration notebook:\n",
    "\n",
    "```sql\n",
    "-- Which factories have the highest average temperature?\n",
    "SELECT \n",
    "  factory_name,\n",
    "  region,\n",
    "  city,\n",
    "  avg_temperature,\n",
    "  max_temperature,\n",
    "  total_devices\n",
    "FROM factory_kpis_gold\n",
    "WHERE avg_temperature > 75\n",
    "ORDER BY avg_temperature DESC;\n",
    "```\n",
    "\n",
    "2. Click Play icon or press `Shift + Enter` to run this query.\n",
    "\n",
    "### Query 2: Analyze Recent Critical Readings\n",
    "\n",
    "Let's find recent sensor readings that exceeded safe temperature thresholds.\n",
    "\n",
    "1. Add the following code to the next cell:\n",
    "\n",
    "```sql\n",
    "-- Find recent critical temperature readings\n",
    "SELECT \n",
    "  device_id,\n",
    "  factory_id,\n",
    "  timestamp,\n",
    "  temperature,\n",
    "  air_pressure,\n",
    "  rotation_speed\n",
    "FROM sensor_silver\n",
    "WHERE temperature > 85\n",
    "  AND timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "ORDER BY temperature DESC\n",
    "LIMIT 100;\n",
    "```\n",
    "\n",
    "2. Click Play icon or press `Shift + Enter` to run this query.\n",
    "\n",
    "### What You'll See\n",
    "\n",
    "The pipeline assets browser shows:\n",
    "- **Pipeline graph (DAG):** Your streaming table and materialized view with data flow\n",
    "- **Exploration notebooks:** Separate from pipeline logic for ad-hoc analysis\n",
    "- **Table preview:** Sample data from your tables\n",
    "- **Query results:** Results from your exploration queries\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:** You can create multiple exploration notebooks to organize different types of analysis (e.g., data quality checks, performance analysis, anomaly detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "577a0c55-621c-4028-82ab-24a92fbb4b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Create a Job to Run the Pipeline\n",
    "\n",
    "Next, create a workflow to automate data ingestion, processing, and analysis steps using a Databricks job that runs on a schedule.\n",
    "\n",
    "### Schedule Your Pipeline\n",
    "\n",
    "1. At the top of the editor, choose the **Schedule** button.\n",
    "2. If the **Schedules** dialog appears, choose **Add schedule**.\n",
    "3. This opens the **New schedule** dialog, where you can create a job to run your pipeline on a schedule.\n",
    "4. Optionally, give the job a name: `IoT Sensor Pipeline - Daily Update`\n",
    "5. By default, the schedule is set to run once per day. You can accept this default, or set your own schedule:\n",
    "   - Choosing **Advanced** gives you the option to set a specific time that the job will run\n",
    "   - Selecting **More options** allows you to create notifications when the job runs\n",
    "6. Select **Create** to apply the changes and create the job.\n",
    "\n",
    "Now the job will run daily to keep your pipeline up to date. You can choose **Schedule** again to view the list of schedules. You can manage schedules for your pipeline from that dialog, including adding, editing, or removing schedules.\n",
    "\n",
    "Clicking the name of the schedule (or job) takes you to the job's page in the **Jobs & pipelines** list. From there you can view details about job runs, including the history of runs, or run the job immediately with the **Run now** button.\n",
    "\n",
    "### Monitoring Your Pipeline\n",
    "\n",
    "After deployment, you get automatic observability:\n",
    "- âœ… **Pipeline graph** - Visual data flow showing nodes and edges\n",
    "- âœ… **Data quality dashboard** - Expectation pass/fail rates\n",
    "- âœ… **Event log** - Detailed execution history\n",
    "- âœ… **Performance metrics** - Processing speed, row counts, cluster utilization\n",
    "- âœ… **Lineage tracking** - Understand data dependencies\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Pipeline Code\n",
    "\n",
    "Here's everything in one file you can copy/paste into the Lakeflow Pipeline Editor. This code includes both the streaming table and materialized view in a single source file.\n",
    "\n",
    "**When to use this:**\n",
    "- If you want all pipeline logic in one place\n",
    "- For quick prototyping and testing\n",
    "- When your pipeline is relatively simple\n",
    "\n",
    "**Alternative approach:**\n",
    "- You can split this into multiple files (e.g., `silver_layer.py` and `gold_layer.py`)\n",
    "- Use the Plus icon **Add** then **Source** to create additional files\n",
    "- All files in the `transformations` folder are processed together\n",
    "\n",
    "---\n",
    "\n",
    "### Python Complete Pipeline\n",
    "\n",
    "**Instructions:**\n",
    "1. Copy the entire code block below\n",
    "2. Paste into your pipeline source file in the Lakeflow Pipelines Editor\n",
    "3. Click Play icon **Run file** or **Run pipeline** to execute\n",
    "4. View the pipeline graph on the right to see `sensor_silver` and `factory_kpis_gold` nodes\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "IoT Aircraft Sensor Pipeline\n",
    "Complete declarative pipeline for production-ready sensor data processing\n",
    "\n",
    "Configuration:\n",
    "- READ_CATALOG: dwx_airops_insights_platform_dev_working\n",
    "- READ_SCHEMA: db_crash_course (shared read schema)\n",
    "- WRITE_SCHEMA: <your_username> (personal write schema, special chars replaced with _)\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, abs as abs_func, current_timestamp,\n",
    "    avg, max, count, countDistinct, round as spark_round\n",
    ")\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "READ_CATALOG = \"dwx_airops_insights_platform_dev_working\"\n",
    "READ_SCHEMA = \"db_crash_course\"\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "WRITE_SCHEMA = re.sub(r'[@\\.]', '_', username)\n",
    "\n",
    "# ========================================\n",
    "# SILVER LAYER - Cleaned Sensor Data\n",
    "# ========================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_silver\",\n",
    "    comment=\"Cleaned and validated aircraft sensor readings\",\n",
    "    expect={\n",
    "        \"valid_device_id\": \"device_id IS NOT NULL\",\n",
    "        \"valid_timestamp\": \"timestamp IS NOT NULL\",\n",
    "        \"valid_temperature_range\": \"temperature BETWEEN -50 AND 150\"\n",
    "    },\n",
    "    expect_or_drop={\n",
    "        \"positive_air_pressure\": \"air_pressure > 0\"\n",
    "    }\n",
    ")\n",
    "def sensor_silver():\n",
    "    \"\"\"Ingest and clean sensor data with Auto Loader.\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"/Volumes/{READ_CATALOG}/{READ_SCHEMA}/sensor_data/\")\n",
    "        .withColumn(\"air_pressure\",\n",
    "                   when(col(\"air_pressure\") < 0, abs_func(col(\"air_pressure\")))\n",
    "                   .otherwise(col(\"air_pressure\")))\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# ========================================\n",
    "# GOLD LAYER - Factory KPIs\n",
    "# ========================================\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"factory_kpis_gold\",\n",
    "    comment=\"Factory-level KPIs for aircraft maintenance dashboards\"\n",
    ")\n",
    "def factory_kpis_gold():\n",
    "    \"\"\"Aggregate factory-level metrics.\"\"\"\n",
    "    sensors = spark.read.table(\"sensor_silver\")\n",
    "    factories = spark.read.table(f\"{READ_CATALOG}.{READ_SCHEMA}.dim_factories\")\n",
    "    \n",
    "    enriched = sensors.join(factories, \"factory_id\", \"left\")\n",
    "    \n",
    "    return (\n",
    "        enriched\n",
    "        .groupBy(\"factory_id\", \"factory_name\", \"region\", \"city\")\n",
    "        .agg(\n",
    "            countDistinct(\"device_id\").alias(\"total_devices\"),\n",
    "            count(\"*\").alias(\"total_readings\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "            spark_round(max(\"temperature\"), 2).alias(\"max_temperature\"),\n",
    "            spark_round(avg(\"air_pressure\"), 2).alias(\"avg_air_pressure\"),\n",
    "            spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"âœ… Pipeline defined! Deploy in the Lakeflow Pipelines Editor.\")\n",
    "print(f\"ðŸ“– Reading from: {READ_CATALOG}.{READ_SCHEMA}\")\n",
    "print(f\"âœï¸  Writing to: {READ_CATALOG}.{WRITE_SCHEMA}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48c5977-165f-4e72-8d63-9a49898f0d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL Complete Pipeline\n",
    "\n",
    "**Instructions:**\n",
    "1. Copy the entire code block below\n",
    "2. Paste into your pipeline source file in the Lakeflow Pipelines Editor\n",
    "3. Click Play icon **Run file** or **Run pipeline** to execute\n",
    "4. View the pipeline graph on the right to see `sensor_silver` and `factory_kpis_gold` nodes\n",
    "\n",
    "```sql\n",
    "-- IoT Aircraft Sensor Pipeline (SQL Version)\n",
    "-- \n",
    "-- Configuration:\n",
    "-- READ_CATALOG: dwx_airops_insights_platform_dev_working\n",
    "-- READ_SCHEMA: db_crash_course (shared)\n",
    "-- WRITE_SCHEMA: <your_username> (personal, special chars replaced with _)\n",
    "--\n",
    "-- Note: Update the target catalog and schema in the pipeline settings to:\n",
    "-- Target: dwx_airops_insights_platform_dev_working.<your_username>\n",
    "\n",
    "-- SILVER LAYER: Cleaned sensor data\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_silver (\n",
    "  CONSTRAINT valid_device_id EXPECT (device_id IS NOT NULL),\n",
    "  CONSTRAINT valid_timestamp EXPECT (timestamp IS NOT NULL),\n",
    "  CONSTRAINT valid_temperature_range EXPECT (temperature BETWEEN -50 AND 150),\n",
    "  CONSTRAINT positive_air_pressure EXPECT OR DROP (air_pressure > 0)\n",
    ")\n",
    "COMMENT 'Cleaned and validated aircraft sensor readings'\n",
    "AS SELECT\n",
    "  device_id,\n",
    "  trip_id,\n",
    "  factory_id,\n",
    "  model_id,\n",
    "  timestamp,\n",
    "  airflow_rate,\n",
    "  rotation_speed,\n",
    "  CASE \n",
    "    WHEN air_pressure < 0 THEN ABS(air_pressure)\n",
    "    ELSE air_pressure\n",
    "  END as air_pressure,\n",
    "  temperature,\n",
    "  delay,\n",
    "  density,\n",
    "  current_timestamp() as processed_at\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/dwx_airops_insights_platform_dev_working/db_crash_course/sensor_data/',\n",
    "  format => 'csv',\n",
    "  header => 'true'\n",
    ");\n",
    "\n",
    "-- GOLD LAYER: Factory KPIs\n",
    "CREATE OR REFRESH MATERIALIZED VIEW factory_kpis_gold\n",
    "COMMENT 'Factory-level KPIs for aircraft maintenance dashboards'\n",
    "AS SELECT\n",
    "  s.factory_id,\n",
    "  f.factory_name,\n",
    "  f.region,\n",
    "  f.city,\n",
    "  COUNT(DISTINCT s.device_id) as total_devices,\n",
    "  COUNT(*) as total_readings,\n",
    "  ROUND(AVG(s.temperature), 2) as avg_temperature,\n",
    "  ROUND(MAX(s.temperature), 2) as max_temperature,\n",
    "  ROUND(AVG(s.air_pressure), 2) as avg_air_pressure,\n",
    "  ROUND(AVG(s.rotation_speed), 2) as avg_rotation_speed\n",
    "FROM sensor_silver s\n",
    "LEFT JOIN dwx_airops_insights_platform_dev_working.db_crash_course.dim_factories f \n",
    "  ON s.factory_id = f.factory_id\n",
    "GROUP BY s.factory_id, f.factory_name, f.region, f.city;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95998f78-8e25-483a-85ef-d03bb547f41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've built a production-ready data pipeline with Lakeflow Spark Declarative Pipelines!\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "âœ… **Created a pipeline** using the new Lakeflow Pipelines Editor  \n",
    "âœ… **Built a streaming table** - Automatically ingests and cleans sensor data with Auto Loader  \n",
    "âœ… **Added data quality** - Tracks and enforces expectations  \n",
    "âœ… **Created a materialized view** - Always-current factory KPIs  \n",
    "âœ… **Performed ad-hoc analysis** - Created exploration notebooks for queries  \n",
    "âœ… **Scheduled automation** - Set up a job to run your pipeline daily  \n",
    "\n",
    "### What You Get For Free\n",
    "\n",
    "The Lakeflow framework automatically provides:\n",
    "- âœ… **Incremental processing** - Only processes new data\n",
    "- âœ… **Checkpoint management** - Exactly-once guarantees\n",
    "- âœ… **Schema evolution** - Handles new columns gracefully\n",
    "- âœ… **Data quality tracking** - Expectations dashboard\n",
    "- âœ… **Lineage visualization** - See data flow between tables\n",
    "- âœ… **Error recovery** - Automatic retries\n",
    "- âœ… **Monitoring** - Performance and health metrics\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Declarative approach** - You define WHAT you want, not HOW to get it\n",
    "2. **Auto Loader** - Automatically discovers and processes new files\n",
    "3. **Streaming tables** - For append-only incremental processing\n",
    "4. **Materialized views** - For aggregations that update\n",
    "5. **Exploration notebooks** - For ad-hoc queries separate from pipeline logic\n",
    "6. **Built-in observability** - Automatic monitoring and data quality tracking\n",
    "\n",
    "### Next Steps for Your Week\n",
    "\n",
    "Now that you have clean, aggregated data:\n",
    "\n",
    "1. **Dashboards** (Day 1) - Use `factory_kpis_gold` for visualizations\n",
    "2. **Genie** (Day 1) - Connect to `sensor_silver` for natural language queries\n",
    "3. **AutoML** (Day 2) - Use `sensor_silver` for predictive maintenance models\n",
    "4. **AI Agents** (Day 2) - Build on clean data for AI systems\n",
    "\n",
    "You're well on your way to meeting leadership's deadline! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Lakeflow Spark Declarative Pipelines](https://learn.microsoft.com/en-us/azure/databricks/ldp/)\n",
    "- [What is Auto Loader?](https://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/)\n",
    "- [Pipeline Development Guide](https://learn.microsoft.com/en-us/azure/databricks/ldp/develop)\n",
    "- [Multi-File Editor](https://learn.microsoft.com/en-us/azure/databricks/ldp/multi-file-editor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed69df8-f2f1-4f61-b166-1393e336a0cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Try This Out: Extend Your Pipeline\n",
    "\n",
    "Want to learn more? Here are some ideas to explore:\n",
    "\n",
    "### 1. Add Device-Level Metrics\n",
    "\n",
    "Create another materialized view for per-device KPIs to track individual device health.\n",
    "\n",
    "**Steps:**\n",
    "1. In your pipeline editor, add the following code to your source file (or create a new source file)\n",
    "2. Click Play icon **Run file** to update the pipeline\n",
    "\n",
    "**Python:**\n",
    "```python\n",
    "@dp.materialized_view(name=\"device_health_gold\")\n",
    "def device_health_gold():\n",
    "    return (\n",
    "        spark.read.table(\"sensor_silver\")\n",
    "        .groupBy(\"device_id\", \"factory_id\")\n",
    "        .agg(\n",
    "            avg(\"temperature\").alias(\"avg_temp\"),\n",
    "            count(\"*\").alias(\"reading_count\")\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "CREATE OR REFRESH MATERIALIZED VIEW device_health_gold\n",
    "AS SELECT \n",
    "  device_id,\n",
    "  factory_id,\n",
    "  AVG(temperature) as avg_temp,\n",
    "  COUNT(*) as reading_count\n",
    "FROM sensor_silver\n",
    "GROUP BY device_id, factory_id;\n",
    "```\n",
    "\n",
    "The pipeline graph will update to show three nodes: `sensor_silver` â†’ `factory_kpis_gold` and `device_health_gold`.\n",
    "\n",
    "### 2. Add Inspection Data Pipeline\n",
    "\n",
    "Create a parallel streaming table for inspection records.\n",
    "\n",
    "**Steps:**\n",
    "1. From the pipeline assets browser, click Plus icon **Add** then **Source**\n",
    "2. Choose your language (Python or SQL)\n",
    "3. Paste the code below\n",
    "4. Click Play icon **Run file**\n",
    "\n",
    "**Python:**\n",
    "```python\n",
    "@dp.table(name=\"inspection_silver\")\n",
    "def inspection_silver():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/dwx_airops_insights_platform_dev_working/db_crash_course/inspection_data/\")\n",
    "    )\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE inspection_silver\n",
    "AS SELECT *\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/dwx_airops_insights_platform_dev_working/db_crash_course/inspection_data/',\n",
    "  format => 'csv',\n",
    "  header => 'true'\n",
    ");\n",
    "```\n",
    "\n",
    "### 3. Organize with Multiple Files\n",
    "\n",
    "Use the multi-file editor to organize your pipeline by layer:\n",
    "\n",
    "**Steps:**\n",
    "1. Click Plus icon **Add** then **Source** to create new files\n",
    "2. Name them: `bronze_layer.py`, `silver_layer.py`, `gold_layer.py`\n",
    "3. Move related code to each file\n",
    "4. Run Play icon **Run pipeline** to process all files together\n",
    "\n",
    "Benefits:\n",
    "- Better organization\n",
    "- Easier collaboration\n",
    "- Clear separation of concerns\n",
    "- All files in the `transformations` folder are processed together\n",
    "\n",
    "### 4. Create More Exploration Notebooks\n",
    "\n",
    "Practice ad-hoc queries with additional exploration notebooks:\n",
    "\n",
    "**Ideas:**\n",
    "- **Data Quality Report**: Check for data issues and anomalies\n",
    "- **Performance Analysis**: Analyze sensor performance trends\n",
    "- **Factory Comparison**: Compare metrics across regions\n",
    "\n",
    "**Steps:**\n",
    "1. Click Plus icon **Add** then **Exploration**\n",
    "2. Name your notebook\n",
    "3. Write SQL queries to analyze your data\n",
    "4. These don't affect the pipeline, only for analysis\n",
    "\n",
    "### 5. Explore Advanced Features\n",
    "\n",
    "- **Change Data Capture (CDC)** - Handle updates and deletes\n",
    "- **Watermarks** - Handle late-arriving data\n",
    "- **Flows** - Multiple sources writing to one table\n",
    "- **Expectations with actions** - Drop or quarantine bad data\n",
    "\n",
    "**Documentation:**\n",
    "- [Streaming Tables](https://learn.microsoft.com/en-us/azure/databricks/ldp/streaming-tables)\n",
    "- [Materialized Views](https://learn.microsoft.com/en-us/azure/databricks/ldp/materialized-views)\n",
    "- [Multi-File Editor](https://learn.microsoft.com/en-us/azure/databricks/ldp/multi-file-editor)\n",
    "- [Data Quality Expectations](https://learn.microsoft.com/en-us/azure/databricks/ldp/expectations)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "5 Data Transformation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
