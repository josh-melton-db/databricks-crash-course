{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04e42aad-4405-42e3-962c-fc1b6be2dd41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Notebooks: PySpark Data Processing\n",
    "\n",
    "**Databricks Notebooks** are interactive, collaborative documents for data engineering, data science, and machine learning. They support Python, SQL, Scala, and R.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "âœ… Read data from Unity Catalog tables  \n",
    "âœ… Perform transformations with PySpark  \n",
    "âœ… Use DataFrame API for data manipulation  \n",
    "âœ… Display and visualize results  \n",
    "âœ… Write processed data back to tables  \n",
    "\n",
    "---\n",
    "\n",
    "## Use Case: IoT Data Processing\n",
    "\n",
    "We'll use PySpark to:\n",
    "- Read sensor data from Unity Catalog\n",
    "- Clean and transform the data\n",
    "- Calculate aggregations and metrics\n",
    "- Create visualizations\n",
    "- Save results for downstream use\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Notebook Basics\n",
    "2. Reading Data\n",
    "3. DataFrame Operations\n",
    "4. Aggregations and Window Functions\n",
    "5. Joining Tables\n",
    "6. Visualizations\n",
    "7. Writing Data\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Notebooks Documentation](https://docs.databricks.com/aws/en/notebooks/)\n",
    "- [Notebooks Code](https://docs.databricks.com/aws/en/notebooks/notebooks-code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96013d49-6d05-4e42-a627-82cf39294190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG = 'default'\n",
    "SCHEMA = 'db_crash_course'\n",
    "\n",
    "print(f\"Using: {CATALOG}.{SCHEMA}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "473066a3-05ba-40fe-92f7-b496c65ac317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Notebook Basics <a id=\"basics\"></a>\n",
    "\n",
    "### What are Databricks Notebooks?\n",
    "\n",
    "Notebooks are interactive documents containing:\n",
    "- **Code cells** - Execute Python, SQL, Scala, or R code\n",
    "- **Markdown cells** - Documentation and explanations\n",
    "- **Visualizations** - Built-in plotting capabilities\n",
    "- **Results** - Output from code execution\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "âœ… **Multi-language support** - Switch between languages in the same notebook  \n",
    "âœ… **Collaboration** - Real-time co-editing with teammates  \n",
    "âœ… **Version control** - Git integration for tracking changes  \n",
    "âœ… **Scheduling** - Run notebooks as automated jobs  \n",
    "âœ… **Interactive visualizations** - Built-in charting  \n",
    "\n",
    "### Magic Commands:\n",
    "\n",
    "- `%python` - Python code (default)\n",
    "- `%sql` - SQL queries\n",
    "- `%scala` - Scala code\n",
    "- `%r` - R code\n",
    "- `%md` - Markdown for documentation\n",
    "- `%sh` - Shell commands\n",
    "- `%pip` - Install Python packages\n",
    "\n",
    "### Keyboard Shortcut:\n",
    "\n",
    "- `Shift + Enter` - Run cell and move to next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa536bb1-89a6-4d75-a1b0-1d9c52cd078d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Reading Data <a id=\"reading\"></a>\n",
    "\n",
    "### Reading from Unity Catalog Tables\n",
    "\n",
    "The simplest way to read data is using `spark.table()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06cc42b0-e49f-49ad-995b-e6f47c638410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read a table from Unity Catalog\n",
    "sensors_df = spark.table(f\"{CATALOG}.{SCHEMA}.sensor_bronze\")\n",
    "\n",
    "# Show schema\n",
    "print(\"Schema:\")\n",
    "sensors_df.printSchema()\n",
    "\n",
    "# Display first few rows\n",
    "sensors_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a287a4-9d7d-41c6-8088-11b367928d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading from Files in Volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e198ea3-6137-4b64-a010-16e4206d8b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV files from a volume\n",
    "csv_path = f\"/Volumes/{CATALOG}/{SCHEMA}/sensor_data/\"\n",
    "\n",
    "df_from_volume = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(csv_path)\n",
    ")\n",
    "\n",
    "print(f\"Records read from volume: {df_from_volume.count():,}\")\n",
    "df_from_volume.limit(3).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f39f45-ebd2-4cc7-920f-16747819915a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Basic DataFrame Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfca87dd-fe63-498c-a85c-57ac6a84d2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get DataFrame information\n",
    "print(f\"Total rows: {sensors_df.count():,}\")\n",
    "print(f\"Total columns: {len(sensors_df.columns)}\")\n",
    "print(f\"\\nColumns: {sensors_df.columns}\")\n",
    "\n",
    "# Show summary statistics\n",
    "sensors_df.select(\"temperature\", \"rotation_speed\", \"air_pressure\").summary().display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5a4047-33d6-4170-9442-9ff8fff009c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. DataFrame Operations <a id=\"dataframe\"></a>\n",
    "\n",
    "### Selecting Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d09e461-fa57-4fd3-a49a-97a40690f9f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select specific columns\n",
    "selected_df = sensors_df.select(\n",
    "    \"device_id\",\n",
    "    \"timestamp\",\n",
    "    \"temperature\",\n",
    "    \"rotation_speed\",\n",
    "    \"factory_id\"\n",
    ")\n",
    "\n",
    "selected_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fb4ac0e-1da7-4632-8611-ddff0f8caee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtering Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f964302-466d-4108-84c8-5ce652dea077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for high temperatures\n",
    "high_temp_df = sensors_df.filter(col(\"temperature\") > 80)\n",
    "\n",
    "print(f\"High temperature readings: {high_temp_df.count():,}\")\n",
    "high_temp_df.limit(5).display()\n",
    "\n",
    "# Multiple conditions\n",
    "critical_df = sensors_df.filter(\n",
    "    (col(\"temperature\") > 80) & \n",
    "    (col(\"rotation_speed\") > 600)\n",
    ")\n",
    "\n",
    "print(f\"\\nCritical readings (high temp AND high speed): {critical_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63d22373-f5f6-473e-b305-c11edbc8d401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding and Transforming Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e96cdd3c-714f-4647-8b49-d8676e743f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, round as spark_round\n",
    "\n",
    "# Add temperature in Celsius\n",
    "transformed_df = sensors_df.withColumn(\n",
    "    \"temperature_celsius\",\n",
    "    spark_round((col(\"temperature\") - 32) * 5/9, 2)\n",
    ")\n",
    "\n",
    "# Add a status flag\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    \"temperature_status\",\n",
    "    when(col(\"temperature\") > 85, \"Critical\")\n",
    "    .when(col(\"temperature\") > 75, \"Warning\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# Calculate derived metric\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    \"performance_index\",\n",
    "    spark_round(col(\"rotation_speed\") / col(\"air_pressure\") * 100, 2)\n",
    ")\n",
    "\n",
    "transformed_df.select(\n",
    "    \"device_id\",\n",
    "    \"temperature\",\n",
    "    \"temperature_celsius\",\n",
    "    \"temperature_status\",\n",
    "    \"performance_index\"\n",
    ").limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9cddef4-a7ec-4079-9fb5-8405764fe3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling Null Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b938152b-6df0-41b0-bed8-cebd60eb3e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "from pyspark.sql.functions import count, when, col, isnan\n",
    "\n",
    "null_counts = sensors_df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in sensors_df.columns\n",
    "])\n",
    "\n",
    "print(\"Null counts per column:\")\n",
    "null_counts.display()\n",
    "\n",
    "# Drop rows with any nulls\n",
    "clean_df = sensors_df.na.drop()\n",
    "\n",
    "# Fill nulls with specific values\n",
    "filled_df = sensors_df.fillna({\n",
    "    \"temperature\": 0,\n",
    "    \"air_pressure\": sensors_df.agg({\"air_pressure\": \"mean\"}).first()[0]\n",
    "})\n",
    "\n",
    "print(f\"\\nOriginal: {sensors_df.count():,} rows\")\n",
    "print(f\"After dropping nulls: {clean_df.count():,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0ea61f7-faf5-4fc1-ab73-96bf229678fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Aggregations and Window Functions <a id=\"aggregations\"></a>\n",
    "\n",
    "### Basic Aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eef73ec-5017-4029-a383-a1d58148bff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, max, min, count, stddev, percentile_approx\n",
    "\n",
    "# Simple aggregations\n",
    "overall_stats = sensors_df.agg(\n",
    "    count(\"*\").alias(\"total_readings\"),\n",
    "    avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "    max(\"temperature\").alias(\"max_temperature\"),\n",
    "    min(\"temperature\").alias(\"min_temperature\"),\n",
    "    stddev(\"temperature\").alias(\"stddev_temperature\")\n",
    ")\n",
    "\n",
    "print(\"Overall Statistics:\")\n",
    "overall_stats.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f290d3f6-7db4-44c0-9857-33d32e1f7376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Group By Aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db4701f-d8e7-4c5e-b2ac-78e9ccdf5feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by factory\n",
    "factory_stats = (\n",
    "    sensors_df\n",
    "    .groupBy(\"factory_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "        avg(\"rotation_speed\").alias(\"avg_rotation_speed\"),\n",
    "        max(\"air_pressure\").alias(\"max_air_pressure\")\n",
    "    )\n",
    "    .orderBy(col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "print(\"Statistics by Factory:\")\n",
    "factory_stats.display()\n",
    "\n",
    "# Group by device\n",
    "device_stats = (\n",
    "    sensors_df\n",
    "    .groupBy(\"device_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temp\"),\n",
    "        spark_round(max(\"temperature\"), 2).alias(\"max_temp\")\n",
    "    )\n",
    "    .orderBy(col(\"max_temp\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 Devices by Max Temperature:\")\n",
    "device_stats.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "478f010c-fef4-4365-a9ed-44f6be5b0451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Window Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d20a545-a841-4752-b5f7-8602a66f4de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, lag, lead, avg as avg_func\n",
    "\n",
    "# Define window specifications\n",
    "device_window = Window.partitionBy(\"device_id\").orderBy(\"timestamp\")\n",
    "\n",
    "# Add row number within each device\n",
    "windowed_df = sensors_df.withColumn(\n",
    "    \"reading_number\",\n",
    "    row_number().over(device_window)\n",
    ")\n",
    "\n",
    "# Add previous and next temperatures\n",
    "windowed_df = windowed_df.withColumn(\n",
    "    \"prev_temperature\",\n",
    "    lag(\"temperature\", 1).over(device_window)\n",
    ").withColumn(\n",
    "    \"next_temperature\",\n",
    "    lead(\"temperature\", 1).over(device_window)\n",
    ")\n",
    "\n",
    "# Calculate moving average\n",
    "windowed_df = windowed_df.withColumn(\n",
    "    \"temp_moving_avg_3\",\n",
    "    spark_round(\n",
    "        avg_func(\"temperature\").over(\n",
    "            device_window.rowsBetween(-1, 1)  # Window of 3 rows\n",
    "        ),\n",
    "        2\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show temperature changes\n",
    "windowed_df.select(\n",
    "    \"device_id\",\n",
    "    \"timestamp\",\n",
    "    \"temperature\",\n",
    "    \"prev_temperature\",\n",
    "    \"temp_moving_avg_3\",\n",
    "    \"reading_number\"\n",
    ").filter(col(\"device_id\") == 1).limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36479c38-9295-4321-a933-1088f580344b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Joining Tables <a id=\"joins\"></a>\n",
    "\n",
    "### Inner Join with Dimension Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e55223-d02d-4e0a-a00f-c208ef32eab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load dimension tables\n",
    "dim_factories = spark.table(f\"{CATALOG}.{SCHEMA}.dim_factories\")\n",
    "dim_models = spark.table(f\"{CATALOG}.{SCHEMA}.dim_models\")\n",
    "\n",
    "# Join sensor data with factories\n",
    "enriched_df = (\n",
    "    sensors_df\n",
    "    .join(dim_factories, \"factory_id\", \"inner\")\n",
    "    .join(dim_models, \"model_id\", \"inner\")\n",
    "    .select(\n",
    "        \"device_id\",\n",
    "        \"timestamp\",\n",
    "        \"temperature\",\n",
    "        \"rotation_speed\",\n",
    "        \"factory_name\",\n",
    "        \"region\",\n",
    "        \"city\",\n",
    "        \"model_name\",\n",
    "        \"model_category\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Enriched sensor data:\")\n",
    "enriched_df.limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd35929d-b5d8-4377-8d72-b2881d038968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Aggregate Enriched Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da40e437-0cbf-43b5-8598-3a4fe7d5486a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performance by region and model category\n",
    "region_model_stats = (\n",
    "    enriched_df\n",
    "    .groupBy(\"region\", \"model_category\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "        spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "    )\n",
    "    .orderBy(\"region\", col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "print(\"Performance by Region and Model Category:\")\n",
    "region_model_stats.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b12fea98-1b9a-46f1-a34c-5ff0c17cacbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Visualizations <a id=\"visualizations\"></a>\n",
    "\n",
    "### Built-in Display Visualizations\n",
    "\n",
    "Databricks notebooks have built-in visualization capabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b58da6-fd7a-4539-ac5b-1b5bcece0531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "factory_temp_viz = (\n",
    "    enriched_df\n",
    "    .groupBy(\"factory_name\")\n",
    "    .agg(\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "        spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "    )\n",
    "    .orderBy(col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "# Display - click the chart icon to create visualizations\n",
    "factory_temp_viz.display()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Š Try These Visualizations:\n",
    "1. Click the bar chart icon below the table\n",
    "2. Drag 'factory_name' to Keys\n",
    "3. Drag 'avg_temperature' to Values\n",
    "4. Try different chart types: bar, line, pie, scatter\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea8f28d8-f57c-422f-9dfa-bd58781eb129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Writing Data <a id=\"writing\"></a>\n",
    "\n",
    "### Save Transformed Data as a New Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbc713ed-767d-4436-8794-13a1e1fc65a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a processed dataset\n",
    "processed_df = (\n",
    "    enriched_df\n",
    "    .withColumn(\"temperature_celsius\", spark_round((col(\"temperature\") - 32) * 5/9, 2))\n",
    "    .withColumn(\n",
    "        \"temperature_category\",\n",
    "        when(col(\"temperature\") > 85, \"Critical\")\n",
    "        .when(col(\"temperature\") > 75, \"High\")\n",
    "        .when(col(\"temperature\") > 65, \"Normal\")\n",
    "        .otherwise(\"Low\")\n",
    "    )\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Save as Delta table\n",
    "output_table = f\"{CATALOG}.{SCHEMA}.sensor_processed\"\n",
    "\n",
    "processed_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(output_table)\n",
    "\n",
    "print(f\"âœ… Data saved to: {output_table}\")\n",
    "print(f\"   Total records: {processed_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49cf3e76-8b1e-4587-bdba-6f521651eca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Append vs Overwrite Modes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aebd038d-80d3-4443-a6ce-b05fd5661cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Example: Append mode (adds new records)\n",
    "# processed_df.write.format(\"delta\").mode(\"append\").saveAsTable(output_table)\n",
    "\n",
    "# Example: Overwrite mode (replaces all data)\n",
    "# processed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_table)\n",
    "\n",
    "# Example: Write to a specific location\n",
    "# processed_df.write.format(\"delta\").mode(\"overwrite\").save(f\"/Volumes/{CATALOG}/{SCHEMA}/processed_data/\")\n",
    "\n",
    "print(\"\"\"\n",
    "Write Modes:\n",
    "- overwrite: Replace existing data\n",
    "- append: Add new records\n",
    "- ignore: Skip if table exists\n",
    "- error: Fail if table exists (default)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2493b24-b584-4b7a-90c6-f5b04db84c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… **Notebook basics** - Magic commands, keyboard shortcuts, collaboration  \n",
    "âœ… **Reading data** - From Unity Catalog tables and volumes  \n",
    "âœ… **DataFrame operations** - Select, filter, transform columns  \n",
    "âœ… **Aggregations** - GroupBy, window functions, statistics  \n",
    "âœ… **Joins** - Enrich data with dimension tables  \n",
    "âœ… **Visualizations** - Built-in charts and custom HTML  \n",
    "âœ… **Writing data** - Save transformed data to Delta tables  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **DataFrames are immutable** - Each transformation returns a new DataFrame\n",
    "2. **Lazy evaluation** - Transformations are planned, not executed until an action\n",
    "3. **display()** - Best way to view results with automatic visualizations\n",
    "4. **Chaining operations** - Use parentheses for readable multi-line transformations\n",
    "5. **Unity Catalog** - Simplifies data access with three-level namespace\n",
    "\n",
    "### PySpark Best Practices:\n",
    "\n",
    "**Performance:**\n",
    "- Use `filter()` early to reduce data volume\n",
    "- Avoid `collect()` on large datasets (brings all data to driver)\n",
    "- Use `cache()` for DataFrames accessed multiple times\n",
    "- Partition output data appropriately\n",
    "\n",
    "**Code Quality:**\n",
    "- Use explicit column references with `col()`\n",
    "- Chain transformations for readability\n",
    "- Add comments for complex logic\n",
    "- Use meaningful variable names\n",
    "\n",
    "**Data Quality:**\n",
    "- Check for nulls and handle them explicitly\n",
    "- Validate data types match expectations\n",
    "- Use `na.drop()` or `fillna()` strategically\n",
    "- Add data quality checks before writing\n",
    "\n",
    "### Common DataFrame Actions:\n",
    "\n",
    "**Transformations (lazy):**\n",
    "- `select()`, `filter()`, `withColumn()`, `groupBy()`, `join()`\n",
    "\n",
    "**Actions (trigger execution):**\n",
    "- `display()`, `show()`, `count()`, `collect()`, `write()`\n",
    "\n",
    "### Try These:\n",
    "\n",
    "- Explore **SQL magic** (`%sql`) for SQL queries in notebooks\n",
    "- Learn about **Delta Lake** features (time travel, MERGE, OPTIMIZE)\n",
    "- Try **Structured Streaming** for real-time data processing\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Notebooks Documentation](https://docs.databricks.com/aws/en/notebooks/)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Delta Lake Guide](https://docs.databricks.com/aws/en/delta/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b26c58-e8f2-4b78-b9c4-848be2bb970d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "5 Notebooks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
