{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6def1a2d-befe-47f7-a3df-ebfbfcfec666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Orchestration: Automating Your IoT Data Pipeline\n",
    "\n",
    "**Scenario:** Your leadership wants dashboards, predictions, and AI systems ready by end of week. Manual execution won't cut it. You need automation.\n",
    "\n",
    "**Databricks Jobs** let you schedule and orchestrate everything you've built:\n",
    "- Data transformation notebooks\n",
    "- Lakeflow pipelines\n",
    "- Dashboard refreshes\n",
    "- ML training workflows\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "âœ… Create scheduled jobs in the UI \n",
    "âœ… Add notebook tasks to your workflow  \n",
    "âœ… Add pipeline tasks to your workflow  \n",
    "âœ… Schedule dashboard refreshes  \n",
    "âœ… Build multi-task workflows with dependencies  \n",
    "âœ… Monitor and debug job runs  \n",
    "\n",
    "---\n",
    "\n",
    "## Why Orchestration?\n",
    "\n",
    "**Manual execution** doesn't scale when:\n",
    "- Data arrives 24/7 from aircraft sensors\n",
    "- Dashboards need fresh data every hour\n",
    "- Leadership expects daily reports by 8 AM\n",
    "- Pipelines have dependencies (bronze â†’ silver â†’ gold)\n",
    "\n",
    "**Databricks Jobs** provide:\n",
    "- â° Scheduled execution\n",
    "- ğŸ”— Task dependencies\n",
    "- ğŸ”„ Automatic retries\n",
    "- ğŸ“Š Centralized monitoring\n",
    "- ğŸ’° Cost tracking per job\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Jobs Overview\n",
    "2. Creating Your First Job\n",
    "3. Adding a Notebook Task\n",
    "4. Adding a Pipeline Task\n",
    "5. Adding a Dashboard Refresh Task\n",
    "6. Building Multi-Task Workflows\n",
    "7. Scheduling\n",
    "8. Monitoring and Debugging\n",
    "9. Try This Out\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Jobs Documentation](https://docs.databricks.com/aws/en/jobs/)\n",
    "- [Jobs Quickstart](https://docs.databricks.com/aws/en/jobs/jobs-quickstart)\n",
    "- [Task Types](https://docs.databricks.com/aws/en/jobs/create-jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51ac056b-7a14-438c-8eb4-d5212edd963e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Jobs Overview\n",
    "\n",
    "### What is a Databricks Job?\n",
    "\n",
    "A **Job** is an automated workflow that can:\n",
    "- Run notebooks\n",
    "- Execute pipelines (Lakeflow/DLT)\n",
    "- Refresh dashboards\n",
    "- Run Python/SQL scripts\n",
    "- Call external APIs\n",
    "\n",
    "### Job Components\n",
    "\n",
    "**1. Tasks**: Individual units of work\n",
    "- Notebook execution\n",
    "- Pipeline updates\n",
    "- Dashboard refreshes\n",
    "- SQL queries\n",
    "- Python scripts\n",
    "\n",
    "**2. Triggers**: When jobs run\n",
    "- Scheduled (cron expressions)\n",
    "- Continuous\n",
    "- Manual\n",
    "- File arrival (advanced)\n",
    "\n",
    "**3. Clusters**: Where jobs run\n",
    "- Job clusters (ephemeral, cost-effective)\n",
    "- All-purpose clusters (shared)\n",
    "- Serverless (recommended)\n",
    "\n",
    "**4. Dependencies**: Task execution order\n",
    "- Linear: A â†’ B â†’ C\n",
    "- Parallel: A â†’ (B, C, D) â†’ E\n",
    "- Conditional: Based on task outcomes\n",
    "\n",
    "### Our IoT Pipeline Architecture\n",
    "\n",
    "We'll build a job that orchestrates:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  IoT Data Job                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Run Data Transformation Notebook                â”‚\n",
    "â”‚     â””â”€> Process incoming sensor data                â”‚\n",
    "â”‚           â†“                                          â”‚\n",
    "â”‚  2. Update Lakeflow Pipeline                        â”‚\n",
    "â”‚     â””â”€> Bronze â†’ Silver â†’ Gold                      â”‚\n",
    "â”‚           â†“                                          â”‚\n",
    "â”‚  3. Refresh Dashboard                               â”‚\n",
    "â”‚     â””â”€> Update IoT monitoring dashboard             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "This ensures:\n",
    "- Data is transformed first\n",
    "- Pipelines process the latest data\n",
    "- Dashboards show current state\n",
    "- Everything runs automatically on schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def17e00-5086-42aa-9df8-c02c5cb2b6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Creating Your First Job\n",
    "\n",
    "Let's create a job that orchestrates your IoT data pipeline.\n",
    "\n",
    "### Step 1: Navigate to Workflows\n",
    "\n",
    "1. In the Databricks workspace left sidebar, click **Jobs & Pipelinees**\n",
    "2. Click **Create Job** (blue button, top right)\n",
    "\n",
    "### Step 2: Configure Job Settings\n",
    "\n",
    "You'll see the job creation interface with a default task.\n",
    "\n",
    "**At the top of the page:**\n",
    "1. Click on the job name (default: \"New Job ....\")\n",
    "2. Rename it to: `IoT Data Pipeline - Daily`\n",
    "\n",
    "**Job Description (optional but recommended):**\n",
    "1. Click **Add description**\n",
    "2. Enter: `Daily orchestration of IoT sensor data: transformation â†’ pipeline â†’ dashboard`\n",
    "\n",
    "### What You Should See\n",
    "\n",
    "- Job name: \"IoT Data Pipeline - Daily\"\n",
    "- Empty graph view with an option to add a task\n",
    "- Empty schedule (we'll add this later)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tip:** Give jobs descriptive names that explain **what** they do and **when** they run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86255171-4c5e-4c2f-b0cd-d1acf90201bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Adding a Notebook Task\n",
    "\n",
    "First, we'll add a task to run your data transformation notebook from Day 1.\n",
    "\n",
    "### Configure the First Task\n",
    "\n",
    "You should see an option to add a task your job. Let's configure it:\n",
    "\n",
    "**Create Task:**\n",
    "1. Click \"Notebook\" task type\n",
    "2. Click on the task name field\n",
    "3. Enter: `transform_sensor_data`\n",
    "\n",
    "**Task Type:**\n",
    "1. Click the **Type** dropdown (may show \"Notebook\" by default)\n",
    "2. Select **Notebook** if not already selected\n",
    "\n",
    "**Notebook Path:**\n",
    "1. Click the **Select Notebook** dropdown in the Path selector\n",
    "2. Navigate to: `/Day 1/5 Data Transformation`\n",
    "3. Select the notebook\n",
    "\n",
    "**Cluster Configuration:**\n",
    "\n",
    "For this task, we'll use a **job cluster** (only runs for the time this job takes to ensure cost-effectiveness):\n",
    "\n",
    "1. Under **Compute**, click **Add new job cluster**\n",
    "2. Configure the **Policy**: Use the default cluster policy\n",
    "3. Click **Confirm**\n",
    "\n",
    "### What You Should See\n",
    "\n",
    "Your first task is now configured:\n",
    "- âœ… Task name: `transform_sensor_data`\n",
    "- âœ… Type: Notebook\n",
    "- âœ… Path: `/Day 1/5 Data Transformation`\n",
    "- âœ… Compute: Configured\n",
    "\n",
    "**Don't click \"Run now\" yet** - we'll add more tasks first!\n",
    "\n",
    "---\n",
    "\n",
    "**âš ï¸ Note:** The notebook path should match where you created your transformation notebook in Day 1. Adjust if your path is different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b2eedc-3efb-451a-8fff-bf44286f4c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Adding a Pipeline Task\n",
    "\n",
    "Now let's add a task to trigger your Lakeflow pipeline.\n",
    "\n",
    "### Add a New Task\n",
    "\n",
    "1. Click the **Add task** button (bottom of the task configuration or in the graph view)\n",
    "2. A new task configuration panel will appear\n",
    "3. Select `ETL Pipeline`\n",
    "\n",
    "### Configure the Pipeline Task\n",
    "\n",
    "**Task Name:**\n",
    "1. Enter: `update_iot_pipeline`\n",
    "\n",
    "**Pipeline Selection:**\n",
    "1. Click the **Pipeline** dropdown\n",
    "2. Select your IoT pipeline created in the previous section of the training\n",
    "   - Look for: \"IoT Data Pipeline\" or similar name you created\n",
    "3. If you haven't created a pipeline yet:\n",
    "   - You can create a placeholder pipeline, or\n",
    "   - Skip this task and come back after creating your pipeline\n",
    "\n",
    "**Depends On:**\n",
    "1. Click the **Depends on** dropdown\n",
    "2. If not already selected, select: `transform_sensor_data`\n",
    "3. This ensures the notebook runs **before** the pipeline\n",
    "\n",
    "**Pipeline Update Mode (Advanced):**\n",
    "1. Under **Pipeline settings**, you may see:\n",
    "   - **Full refresh**: Reprocess all data (slower, complete rebuild)\n",
    "   - **Incremental**: Process only new/changed data (faster, default)\n",
    "2. Keep **Incremental** selected for daily runs\n",
    "\n",
    "### What You Should See\n",
    "\n",
    "Your job now has two tasks:\n",
    "\n",
    "```\n",
    "transform_sensor_data (Notebook)\n",
    "          â†“\n",
    "update_iot_pipeline (Pipeline)\n",
    "```\n",
    "\n",
    "The graph view should show:\n",
    "- âœ… Two tasks\n",
    "- âœ… Arrow connecting them (dependency)\n",
    "- âœ… Task 2 depends on Task 1\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Why This Order?**\n",
    "- Notebook runs first to prepare/validate data\n",
    "- Pipeline then processes the prepared data through Bronze â†’ Silver â†’ Gold\n",
    "- This ensures data quality and proper sequencing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5825ac66-c31f-45d3-bfb1-60736a9a7c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Adding a Dashboard Refresh Task\n",
    "\n",
    "\n",
    "Finally, let's add a task to refresh your IoT monitoring dashboard.\n",
    "\n",
    "### Add Another Task\n",
    "\n",
    "1. Click **Add task** again\n",
    "2. A new task configuration panel appears\n",
    "\n",
    "### Configure the Dashboard Task\n",
    "\n",
    "\n",
    "\n",
    "**Task Type:**\n",
    "1. Click the **Type** dropdown\n",
    "2. Select **Dashboard**\n",
    "\n",
    "**Task Name:**\n",
    "1. Enter: `refresh_dashboard`\n",
    "\n",
    "**Dashboard Selection:**\n",
    "1. Click the **Dashboard** dropdown\n",
    "2. Browse and select your IoT dashboard created earlier in Day 1\n",
    "3. The task will automatically refresh all queries in the dashboard\n",
    "\n",
    "**Depends On:**\n",
    "1. Click the **Depends on** dropdown\n",
    "2. If not already selected, select: `update_iot_pipeline`\n",
    "3. This ensures the pipeline completes **before** refreshing the dashboard\n",
    "\n",
    "Click **Save Task!**\n",
    "\n",
    "### What You Should See\n",
    "\n",
    "Your complete job now has three tasks:\n",
    "\n",
    "```\n",
    "transform_sensor_data (Notebook)\n",
    "          â†“\n",
    "update_iot_pipeline (Pipeline)\n",
    "          â†“\n",
    "refresh_dashboard (Dashboard/SQL)\n",
    "```\n",
    "\n",
    "The graph view should show:\n",
    "- âœ… Three tasks in sequence\n",
    "- âœ… Arrows showing dependencies\n",
    "- âœ… Linear flow from top to bottom\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Why This Matters:**\n",
    "- Leadership sees updated dashboards every morning\n",
    "- Data is always current (within your schedule)\n",
    "- No manual refresh needed\n",
    "- Automatic end-to-end data flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b459d797-6502-4d51-8936-fab1a65b2b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Building Multi-Task Workflows\n",
    "\n",
    "You've just built a **linear workflow** (A â†’ B â†’ C). Let's explore other patterns - no need to follow along, this is an outline of the \"art of the possible\".\n",
    "\n",
    "### Common Workflow Patterns\n",
    "\n",
    "**1. Linear Pipeline** (What we just built)\n",
    "```\n",
    "Task A â†’ Task B â†’ Task C â†’ Task D\n",
    "```\n",
    "**Use case:** Sequential data processing\n",
    "\n",
    "**2. Parallel Processing**\n",
    "```\n",
    "        â”Œâ†’ Task B â”\n",
    "Task A â”€â”¼â†’ Task C â”¼â†’ Task E\n",
    "        â””â†’ Task D â”˜\n",
    "```\n",
    "**Use case:** Process multiple data sources simultaneously\n",
    "\n",
    "**3. Fan-In Pattern**\n",
    "```\n",
    "Task A â”\n",
    "Task B â”¼â†’ Task D\n",
    "Task C â”˜\n",
    "```\n",
    "**Use case:** Merge results from multiple processing tasks\n",
    "\n",
    "### Building a Parallel Workflow\n",
    "\n",
    "Let's say you want to process multiple aircraft types in parallel:\n",
    "\n",
    "**Step 1: Add Multiple Tasks**\n",
    "1. Click **Add task** three times to create Tasks 4, 5, and 6\n",
    "\n",
    "**Step 2: Configure Parallel Tasks**\n",
    "\n",
    "**Task 4: Process Boeing Data**\n",
    "- Name: `process_boeing_sensors`\n",
    "- Type: Notebook\n",
    "- Path: Your Boeing-specific notebook\n",
    "- Depends on: `update_iot_pipeline`\n",
    "\n",
    "**Task 5: Process Airbus Data**\n",
    "- Name: `process_airbus_sensors`\n",
    "- Type: Notebook\n",
    "- Path: Your Airbus-specific notebook\n",
    "- Depends on: `update_iot_pipeline`\n",
    "\n",
    "**Task 6: Merge Reports**\n",
    "- Name: `generate_combined_report`\n",
    "- Type: Notebook\n",
    "- Depends on: `process_boeing_sensors`, `process_airbus_sensors`\n",
    "\n",
    "**Result:**\n",
    "```\n",
    "transform_sensor_data\n",
    "          â†“\n",
    "update_iot_pipeline\n",
    "          â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    "    â†“           â†“\n",
    "process_boeing  process_airbus\n",
    "    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "          â†“\n",
    "generate_combined_report\n",
    "          â†“\n",
    "   refresh_dashboard\n",
    "```\n",
    "\n",
    "### Task Configuration Tips\n",
    "\n",
    "**Shared vs Independent Clusters:**\n",
    "- Parallel tasks can use **different clusters** for true parallelism\n",
    "- Click **Edit** on each task's compute to configure separately\n",
    "- Or use **Serverless** to automatically scale\n",
    "\n",
    "**Task Retry Configuration:**\n",
    "1. Click on any task\n",
    "2. Scroll to **Advanced** section\n",
    "3. Set:\n",
    "   - **Max retries**: `2` (retry up to 2 times on failure)\n",
    "   - **Retry delay**: `30` seconds\n",
    "   - **Timeout**: `3600` seconds (1 hour)\n",
    "\n",
    "**Task Dependencies:**\n",
    "- A task can depend on **multiple tasks**\n",
    "- It will wait for **all** dependencies to complete\n",
    "- Use the **Depends on** dropdown to select multiple\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Best Practice:** Start simple (linear), then add parallelism where it makes sense (independent data sources, different aircraft types, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "525bed34-c86e-49fd-9c91-36914622b6e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Scheduling \n",
    "\n",
    "Now that your workflow is built, let's schedule it to run automatically.\n",
    "\n",
    "### Add a Schedule\n",
    "\n",
    "1. At the top of the job page, find the **Trigger** section\n",
    "2. Click **Add trigger** (or **Edit** if a trigger exists)\n",
    "\n",
    "### Configure Scheduled Trigger\n",
    "\n",
    "**Trigger Type:**\n",
    "1. Select **Scheduled**\n",
    "\n",
    "**Schedule Configuration:**\n",
    "1. Choose between:\n",
    "   - **Simple** - Simple interval-based (every X hours/days)\n",
    "   - **Cron** - Advanced scheduling with cron expressions\n",
    "\n",
    "### Simple Schedule\n",
    "\n",
    "**Example: Every day at 6 AM**\n",
    "1. Select **Simple**\n",
    "2. Set **Period**: `1 day`\n",
    "3. Save\n",
    "\n",
    "### Pause and Resume\n",
    "\n",
    "**To temporarily disable the schedule:**\n",
    "1. Click **Pause** on the job page\n",
    "2. The job won't run automatically (manual runs still work)\n",
    "\n",
    "**To re-enable:**\n",
    "1. Click **Resume**\n",
    "2. Scheduled runs will resume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c615d6d6-24d3-4f7a-82c7-01c8ae917927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Monitoring and Debugging \n",
    "\n",
    "Now let's learn to monitor and debug your job:\n",
    "\n",
    "### Running Your Job Manually (Test)\n",
    "\n",
    "Instead of waiting for the schedule, test your job:\n",
    "\n",
    "1. On the job page, click **Run now** (top right)\n",
    "2. The job will start immediately\n",
    "3. Select `Runs` (in the top left next to `Tasks`)\n",
    "\n",
    "### Understanding the Run View\n",
    "\n",
    "**Graph View:**\n",
    "- Shows all tasks and their status\n",
    "- Flashing Green: Running\n",
    "- âœ… Green: Success\n",
    "- âŒ Red: Failed\n",
    "- â¸ï¸ Gray: Waiting\n",
    "\n",
    "**Task Progress:**\n",
    "- Click any task to see its output\n",
    "- View logs in real-time\n",
    "- See compute metrics (CPU, memory)\n",
    "\n",
    "### Viewing Logs\n",
    "\n",
    "**To see task logs:**\n",
    "1. Click on a task in the graph\n",
    "2. The right panel opens with:\n",
    "   - **Output**: Task execution results\n",
    "   - **Driver logs**: Detailed execution logs\n",
    "   - **Spark UI**: Advanced Spark metrics\n",
    "\n",
    "**Log Sections:**\n",
    "```\n",
    "ğŸ“‹ Output\n",
    "   â””â”€ Notebook cell outputs or script results\n",
    "   \n",
    "ğŸ“ Driver Logs\n",
    "   â””â”€ System logs (stdout, stderr)\n",
    "   \n",
    "âš¡ Spark UI\n",
    "   â””â”€ Query execution details\n",
    "```\n",
    "\n",
    "### Job Run History\n",
    "\n",
    "**To view past runs:**\n",
    "1. On the job page, click the **Runs** tab\n",
    "2. You'll see a table of all runs:\n",
    "   - Start time\n",
    "   - Duration\n",
    "   - Status (Success/Failed)\n",
    "   - Trigger type (Scheduled/Manual)\n",
    "\n",
    "**Filtering Runs:**\n",
    "- Filter by status: Success, Failed, Running\n",
    "- Filter by date range\n",
    "- Search by run ID\n",
    "\n",
    "### Debugging Failed Tasks\n",
    "\n",
    "**Common Failure Scenarios:**\n",
    "\n",
    "**1. Notebook Error**\n",
    "```\n",
    "âŒ Task: transform_sensor_data\n",
    "Error: AnalysisException: Table not found\n",
    "```\n",
    "**Fix:**\n",
    "- Click on the failed task\n",
    "- Read the error in **Output**\n",
    "- Check table names in your notebook\n",
    "- Verify catalog/schema configuration\n",
    "\n",
    "**2. Cluster Start Timeout**\n",
    "```\n",
    "âŒ Task: update_iot_pipeline\n",
    "Error: Failed to start cluster\n",
    "```\n",
    "**Fix:**\n",
    "- Check your cluster configuration\n",
    "- Verify quota limits in your workspace\n",
    "- Consider using Serverless\n",
    "- Try a smaller cluster size\n",
    "\n",
    "**3. Pipeline Update Failed**\n",
    "```\n",
    "âŒ Task: update_iot_pipeline\n",
    "Error: Update failed with FAILED status\n",
    "```\n",
    "**Fix:**\n",
    "- Click **View Pipeline** to see pipeline errors\n",
    "- Check pipeline expectations (data quality rules)\n",
    "- Review pipeline logs\n",
    "\n",
    "### Setting Up Alerts\n",
    "\n",
    "Get notified when jobs fail:\n",
    "\n",
    "1. On the job `Tasks` page, scroll to **Notifications**\n",
    "2. Click **Add notification**\n",
    "3. Configure:\n",
    "   - **Trigger**: `On failure`, `On success`, `On start`, etc.\n",
    "   - **Type**: Email or Webhook\n",
    "   - **Recipients**: Your email or team distribution list\n",
    "\n",
    "**Example Configuration:**\n",
    "```\n",
    "Trigger: On failure\n",
    "Type: Email\n",
    "Recipients: data-team@company.com\n",
    "```\n",
    "\n",
    "### Job Performance Metrics\n",
    "\n",
    "**Key metrics to track:**\n",
    "- **Average duration**: How long jobs typically take\n",
    "- **Success rate**: Percentage of successful runs\n",
    "- **Task duration**: Which tasks take the longest\n",
    "- **Queue time**: Time waiting for cluster start\n",
    "\n",
    "**To view metrics:**\n",
    "1. Go to the **Runs** tab\n",
    "2. Look at the duration column\n",
    "3. Click into runs to see task-level timings\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Pro Tip:** Set up failure alerts immediately. Finding out about failures hours later defeats the purpose of automation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "151cfab4-949d-465b-8358-dd00db099ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Try This Out \n",
    "\n",
    "Want to go deeper? Try these advanced orchestration scenarios:\n",
    "\n",
    "### Advanced Topic 1: File Arrival Triggers\n",
    "\n",
    "**Use Case:** Process new sensor files immediately when they arrive in cloud storage.\n",
    "\n",
    "**Setup:**\n",
    "1. Create a new job: \"IoT File Processing\"\n",
    "2. Add your data processing notebook\n",
    "3. In **Trigger**, select **File arrival**\n",
    "4. Configure:\n",
    "   ```\n",
    "   Location: /Volumes/default/db_crash_course/raw_sensor_data/\n",
    "   File pattern: *.csv\n",
    "   Wait duration: 5 minutes\n",
    "   Max wait: 1 hour\n",
    "   ```\n",
    "\n",
    "**How it Works:**\n",
    "- Monitors the specified path\n",
    "- Detects new files matching `*.csv`\n",
    "- Waits 5 minutes for additional files (batch processing)\n",
    "- Triggers job with file paths as parameters\n",
    "\n",
    "**Access File Paths in Notebook:**\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Get file paths from job trigger\n",
    "file_paths = json.loads(dbutils.widgets.get(\"file_paths\"))\n",
    "\n",
    "for path in file_paths:\n",
    "    print(f\"Processing: {path}\")\n",
    "    df = spark.read.csv(path, header=True)\n",
    "    # Process the file\n",
    "    df.write.mode(\"append\").saveAsTable(\"sensor_data_raw\")\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- Event-driven processing\n",
    "- Unpredictable data arrival\n",
    "- Multiple file uploads throughout the day\n",
    "- Low-latency requirements\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Topic 2: Task Parameters and Values\n",
    "\n",
    "**Use Case:** Pass data between tasks in a workflow.\n",
    "\n",
    "**Scenario:** Count rows processed, then use that count in the next task.\n",
    "\n",
    "**Task 1: Set Values**\n",
    "```python\n",
    "# In your transformation notebook\n",
    "df = spark.table(\"sensor_data_silver\")\n",
    "row_count = df.count()\n",
    "\n",
    "# Set task value for next task\n",
    "dbutils.jobs.taskValues.set(\"row_count\", row_count)\n",
    "dbutils.jobs.taskValues.set(\"status\", \"success\")\n",
    "dbutils.jobs.taskValues.set(\"table_name\", \"sensor_data_silver\")\n",
    "```\n",
    "\n",
    "**Task 2: Read Values**\n",
    "```python\n",
    "# In your downstream notebook\n",
    "# Get values from previous task\n",
    "row_count = dbutils.jobs.taskValues.get(\n",
    "    taskKey=\"transform_sensor_data\",\n",
    "    key=\"row_count\",\n",
    "    default=0\n",
    ")\n",
    "\n",
    "status = dbutils.jobs.taskValues.get(\n",
    "    taskKey=\"transform_sensor_data\",\n",
    "    key=\"status\"\n",
    ")\n",
    "\n",
    "print(f\"Previous task processed {row_count} rows\")\n",
    "print(f\"Status: {status}\")\n",
    "\n",
    "# Use in conditional logic\n",
    "if row_count > 1000000:\n",
    "    print(\"Large dataset detected, using optimized processing\")\n",
    "```\n",
    "\n",
    "**Parameters to Tasks:**\n",
    "You can also pass parameters when triggering a job:\n",
    "\n",
    "1. On task configuration, click **Parameters**\n",
    "2. Add key-value pairs:\n",
    "   ```\n",
    "   environment: production\n",
    "   date: 2024-01-15\n",
    "   factory_id: A001\n",
    "   ```\n",
    "\n",
    "3. Access in notebook:\n",
    "```python\n",
    "# Get parameters\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "date = dbutils.widgets.get(\"date\")\n",
    "factory_id = dbutils.widgets.get(\"factory_id\")\n",
    "\n",
    "print(f\"Running in {environment} for factory {factory_id}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Topic 3: Conditional Task Execution\n",
    "\n",
    "**Use Case:** Run different tasks based on previous task outcomes.\n",
    "\n",
    "**Pattern:**\n",
    "```\n",
    "Task A: Data Quality Check\n",
    "    â†“\n",
    "    â”œâ”€ If PASS â†’ Task B: Process Data\n",
    "    â””â”€ If FAIL â†’ Task C: Send Alert\n",
    "```\n",
    "\n",
    "**Setup:**\n",
    "1. Task A sets exit code:\n",
    "```python\n",
    "# In quality check notebook\n",
    "if data_quality_passed:\n",
    "    dbutils.notebook.exit(\"PASSED\")\n",
    "else:\n",
    "    dbutils.notebook.exit(\"FAILED\")\n",
    "```\n",
    "\n",
    "2. Task B and C have conditional dependencies:\n",
    "   - Task B: Depends on Task A with condition `OUTCOME == \"PASSED\"`\n",
    "   - Task C: Depends on Task A with condition `OUTCOME == \"FAILED\"`\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Topic 4: Webhook Triggers\n",
    "\n",
    "**Use Case:** Trigger jobs from external systems (CI/CD, monitoring tools).\n",
    "\n",
    "**Setup:**\n",
    "1. Create a job\n",
    "2. In **Trigger**, select **Webhook**\n",
    "3. Copy the webhook URL\n",
    "\n",
    "**Trigger via API:**\n",
    "```bash\n",
    "curl -X POST \\\n",
    "  -H \"Authorization: Bearer <your-token>\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"job_id\": 12345,\n",
    "    \"notebook_params\": {\n",
    "      \"date\": \"2024-01-15\",\n",
    "      \"environment\": \"production\"\n",
    "    }\n",
    "  }' \\\n",
    "  https://<workspace>.cloud.databricks.com/api/2.1/jobs/run-now\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- GitOps workflows (trigger on code merge)\n",
    "- External monitoring systems\n",
    "- Manual triggers from other applications\n",
    "- Integration with third-party tools\n",
    "\n",
    "---\n",
    "\n",
    "### Practice Exercise: Build Your Own\n",
    "\n",
    "**Challenge:** Create a production-ready IoT orchestration workflow:\n",
    "\n",
    "1. **Morning Pipeline** (6 AM daily):\n",
    "   - Validate data quality\n",
    "   - Run transformation notebook\n",
    "   - Update Lakeflow pipeline\n",
    "   - Train ML model (AutoML)\n",
    "   - Refresh all dashboards\n",
    "   - Send summary email\n",
    "\n",
    "2. **Hourly Updates** (Every hour, 8 AM - 6 PM):\n",
    "   - Incremental data refresh\n",
    "   - Update real-time dashboard\n",
    "\n",
    "3. **Weekly Report** (Mondays, 9 AM):\n",
    "   - Generate weekly summary\n",
    "   - Run anomaly detection\n",
    "   - Export to data warehouse\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… All three jobs created\n",
    "- âœ… Proper task dependencies\n",
    "- âœ… Schedules configured\n",
    "- âœ… Alerts set up\n",
    "- âœ… Test runs successful\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“š Additional Resources:**\n",
    "- [File Arrival Triggers](https://docs.databricks.com/aws/en/jobs/file-arrival-triggers)\n",
    "- [Task Values](https://docs.databricks.com/aws/en/jobs/share-task-context)\n",
    "- [Jobs API](https://docs.databricks.com/api/workspace/jobs)\n",
    "- [Best Practices](https://docs.databricks.com/aws/en/jobs/jobs-best-practices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb62024-3451-4197-90db-a1c07c760570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "ğŸ‰ **You've learned to orchestrate your IoT data pipeline!**\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "âœ… **Created a scheduled job** that runs your entire pipeline automatically  \n",
    "âœ… **Added notebook tasks** to run data transformations  \n",
    "âœ… **Added pipeline tasks** to update your Lakeflow pipeline  \n",
    "âœ… **Added dashboard refresh** to keep visualizations current  \n",
    "âœ… **Built task dependencies** to ensure proper execution order  \n",
    "âœ… **Configured schedules** with cron expressions  \n",
    "âœ… **Set up monitoring** and alerts for failures  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Jobs automate repetitive tasks** - No more manual execution\n",
    "2. **UI-based configuration** - No JSON required for most workflows\n",
    "3. **Task dependencies** ensure proper sequencing - Data â†’ Pipeline â†’ Dashboard\n",
    "4. **Monitoring is critical** - Set alerts for failures\n",
    "5. **Start simple, add complexity** - Linear workflows first, then parallelism\n",
    "\n",
    "### Your IoT Pipeline is Now Production-Ready\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    IoT Data Pipeline - Running Automatically        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ğŸ“Š Daily at 6 AM:                                  â”‚\n",
    "â”‚    1. Transform sensor data                         â”‚\n",
    "â”‚    2. Update Lakeflow pipeline                      â”‚\n",
    "â”‚    3. Refresh dashboards                            â”‚\n",
    "â”‚                                                      â”‚\n",
    "â”‚  âœ… Leadership has fresh data by 8 AM               â”‚\n",
    "â”‚  ğŸ”” Alerts notify you of any failures               â”‚\n",
    "â”‚  ğŸ’° Job clusters optimize costs                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "Before going to production:\n",
    "\n",
    "- [ ] **Descriptive names** - Jobs and tasks clearly describe what they do\n",
    "- [ ] **Job clusters** - Use ephemeral clusters for cost savings\n",
    "- [ ] **Retries configured** - At least 2 retries for transient failures\n",
    "- [ ] **Alerts set up** - Email on failure to your team\n",
    "- [ ] **Schedules tested** - Run manually first to verify\n",
    "- [ ] **Dependencies verified** - Tasks run in the correct order\n",
    "- [ ] **Monitoring enabled** - Check job run history regularly\n",
    "- [ ] **Documentation added** - Job description explains purpose\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ Mission Accomplished:** Your IoT pipeline now runs automatically, keeping dashboards fresh and leadership happy - all without manual intervention!\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Jobs Documentation](https://docs.databricks.com/aws/en/jobs/)\n",
    "- [Jobs Quickstart](https://docs.databricks.com/aws/en/jobs/jobs-quickstart)\n",
    "- [Task Types](https://docs.databricks.com/aws/en/jobs/create-jobs)\n",
    "- [Scheduling](https://docs.databricks.com/aws/en/jobs/schedule-jobs)\n",
    "- [Best Practices](https://docs.databricks.com/aws/en/jobs/jobs-best-practices)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "6 Orchestration",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
