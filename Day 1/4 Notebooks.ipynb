{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e42aad-4405-42e3-962c-fc1b6be2dd41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Notebooks: Data Processing with Python and SQL\n",
    "\n",
    "**Scenario:** You've found the aircraft IoT sensor data. Now leadership needs you to analyze it, find patterns, and prepare it for dashboards and ML models. All before end of week!\n",
    "\n",
    "**Databricks Notebooks** are interactive documents where you'll write Python and SQL to process data.\n",
    "\n",
    "## Note: connect to a Cluster instead of a SQL Warehouse for this notebook\n",
    "\n",
    "## Pre-Read: Python in 60 Seconds\n",
    "\n",
    "**New to Python?** Here's all you need to get started:\n",
    "\n",
    "### Variables\n",
    "Variables store data. No declaration needed:\n",
    "```python\n",
    "temperature = 75.5          # Number\n",
    "factory_name = \"Boeing\"     # Text (string)\n",
    "is_critical = True          # Boolean (True/False)\n",
    "device_ids = [1, 2, 3, 4]   # List\n",
    "```\n",
    "\n",
    "### Functions\n",
    "Functions are reusable blocks of code:\n",
    "```python\n",
    "def calculate_average(numbers):\n",
    "    total = sum(numbers)\n",
    "    count = len(numbers)\n",
    "    return total / count\n",
    "\n",
    "result = calculate_average([10, 20, 30])  # result = 20.0\n",
    "```\n",
    "\n",
    "**That's it!** You now know enough Python for this notebook. We'll explain everything else as we go, and you can ask Databricks Assistant questions if you're stuck.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "âœ… Read data from Unity Catalog  \n",
    "âœ… Transform data with PySpark (Python + Spark)  \n",
    "âœ… Calculate aggregations  \n",
    "âœ… Create visualizations  \n",
    "âœ… Save processed data  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Notebook Basics\n",
    "2. Reading Data\n",
    "3. DataFrame Operations\n",
    "4. Joining Tables\n",
    "5. Visualizations\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Notebooks Documentation](https://docs.databricks.com/aws/en/notebooks/)\n",
    "- [Notebooks Code](https://docs.databricks.com/aws/en/notebooks/notebooks-code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96013d49-6d05-4e42-a627-82cf39294190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THESE VALUES!\n",
    "import re\n",
    "\n",
    "CATALOG = 'dwx_express_insights_platform_dev_working'  # Catalog name\n",
    "READ_SCHEMA = 'db_crash_course'  # Schema to read from (shared across all users)\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username_base = username.split('@')[0]  # Extract username before @ symbol\n",
    "WRITE_SCHEMA = re.sub(r'[^a-zA-Z0-9_]', '_', username_base)  # Replace special chars with _\n",
    "\n",
    "# Create your personal write schema\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {CATALOG}.{WRITE_SCHEMA}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Using catalog: {CATALOG}\")\n",
    "print(f\"ðŸ“– Reading from schema: {READ_SCHEMA} (shared)\")\n",
    "print(f\"âœï¸  Writing to schema: {WRITE_SCHEMA} (your personal schema)\")\n",
    "print(f\"ðŸ“Š Written tables will be at: {CATALOG}.{WRITE_SCHEMA}.table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "473066a3-05ba-40fe-92f7-b496c65ac317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Notebook Basics\n",
    "\n",
    "### What are Databricks Notebooks?\n",
    "\n",
    "Notebooks are interactive documents containing:\n",
    "- **Code cells** - Execute Python, SQL, Scala, or R code\n",
    "- **Markdown cells** - Documentation and explanations\n",
    "- **Visualizations** - Built-in plotting capabilities\n",
    "- **Results** - Output from code execution\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "âœ… **Multi-language support** - Switch between languages in the same notebook  \n",
    "âœ… **Collaboration** - Real-time co-editing with teammates  \n",
    "âœ… **Version control** - Git integration for tracking changes  \n",
    "âœ… **Scheduling** - Run notebooks as automated jobs  \n",
    "âœ… **Interactive visualizations** - Built-in charting  \n",
    "\n",
    "### Magic Commands:\n",
    "\n",
    "- `%python` - Python code (default)\n",
    "- `%sql` - SQL queries\n",
    "- `%scala` - Scala code\n",
    "- `%r` - R code\n",
    "- `%md` - Markdown for documentation\n",
    "- `%sh` - Shell commands\n",
    "- `%pip` - Install Python packages\n",
    "\n",
    "### Keyboard Shortcut:\n",
    "\n",
    "- `Shift + Enter` - Run cell and move to next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa536bb1-89a6-4d75-a1b0-1d9c52cd078d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Reading Data\n",
    "\n",
    "### Reading from Unity Catalog Tables\n",
    "\n",
    "The simplest way to read data is using `spark.table()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06cc42b0-e49f-49ad-995b-e6f47c638410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read a table from Unity Catalog\n",
    "sensors_df = spark.table(f\"{CATALOG}.{READ_SCHEMA}.sensor_bronze\")\n",
    "\n",
    "# Show schema\n",
    "print(\"Schema:\")\n",
    "sensors_df.printSchema()\n",
    "\n",
    "# Display first few rows\n",
    "sensors_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a287a4-9d7d-41c6-8088-11b367928d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading from Files in Volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e198ea3-6137-4b64-a010-16e4206d8b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV files from a volume\n",
    "csv_path = f\"/Volumes/{CATALOG}/{READ_SCHEMA}/sensor_data/\"\n",
    "\n",
    "df_from_volume = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(csv_path)\n",
    ")\n",
    "\n",
    "print(f\"Records read from volume: {df_from_volume.count():,}\")\n",
    "df_from_volume.limit(3).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f39f45-ebd2-4cc7-920f-16747819915a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Basic DataFrame Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfca87dd-fe63-498c-a85c-57ac6a84d2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get DataFrame information\n",
    "print(f\"Total rows: {sensors_df.count():,}\")\n",
    "print(f\"Total columns: {len(sensors_df.columns)}\")\n",
    "print(f\"\\nColumns: {sensors_df.columns}\")\n",
    "\n",
    "# Show summary statistics\n",
    "sensors_df.select(\"temperature\", \"rotation_speed\", \"air_pressure\").summary().display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5a4047-33d6-4170-9442-9ff8fff009c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. DataFrame Operations\n",
    "\n",
    "### Selecting Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d09e461-fa57-4fd3-a49a-97a40690f9f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select specific columns\n",
    "selected_df = sensors_df.select(\n",
    "    \"device_id\",\n",
    "    \"timestamp\",\n",
    "    \"temperature\",\n",
    "    \"rotation_speed\",\n",
    "    \"factory_id\"\n",
    ")\n",
    "\n",
    "selected_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb4ac0e-1da7-4632-8611-ddff0f8caee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtering Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f964302-466d-4108-84c8-5ce652dea077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for high temperatures\n",
    "high_temp_df = sensors_df.filter(col(\"temperature\") > 80)\n",
    "\n",
    "print(f\"High temperature readings: {high_temp_df.count():,}\")\n",
    "high_temp_df.limit(5).display()\n",
    "\n",
    "# Multiple conditions\n",
    "critical_df = sensors_df.filter(\n",
    "    (col(\"temperature\") > 80) & \n",
    "    (col(\"rotation_speed\") > 600)\n",
    ")\n",
    "\n",
    "print(f\"\\nCritical readings (high temp AND high speed): {critical_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d22373-f5f6-473e-b305-c11edbc8d401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding and Transforming Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96cdd3c-714f-4647-8b49-d8676e743f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, round as spark_round\n",
    "\n",
    "# Add temperature in Celsius\n",
    "transformed_df = sensors_df.withColumn(\n",
    "    \"temperature_celsius\",\n",
    "    spark_round((col(\"temperature\") - 32) * 5/9, 2)\n",
    ")\n",
    "\n",
    "# Add a status flag\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    \"temperature_status\",\n",
    "    when(col(\"temperature\") > 85, \"Critical\")\n",
    "    .when(col(\"temperature\") > 75, \"Warning\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# Calculate derived metric\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    \"performance_index\",\n",
    "    spark_round(col(\"rotation_speed\") / col(\"air_pressure\") * 100, 2)\n",
    ")\n",
    "\n",
    "transformed_df.select(\n",
    "    \"device_id\",\n",
    "    \"temperature\",\n",
    "    \"temperature_celsius\",\n",
    "    \"temperature_status\",\n",
    "    \"performance_index\"\n",
    ").limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9cddef4-a7ec-4079-9fb5-8405764fe3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling Null Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b938152b-6df0-41b0-bed8-cebd60eb3e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "from pyspark.sql.functions import count, when, col, isnan\n",
    "\n",
    "null_counts = sensors_df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in sensors_df.columns\n",
    "])\n",
    "\n",
    "print(\"Null counts per column:\")\n",
    "null_counts.display()\n",
    "\n",
    "# Drop rows with any nulls\n",
    "clean_df = sensors_df.na.drop()\n",
    "\n",
    "# Fill nulls with specific values\n",
    "filled_df = sensors_df.fillna({\n",
    "    \"temperature\": 0,\n",
    "    \"air_pressure\": sensors_df.agg({\"air_pressure\": \"mean\"}).first()[0]\n",
    "})\n",
    "\n",
    "print(f\"\\nOriginal: {sensors_df.count():,} rows\")\n",
    "print(f\"After dropping nulls: {clean_df.count():,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ea61f7-faf5-4fc1-ab73-96bf229678fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Joining Tables\n",
    "\n",
    "### Inner Join with Dimension Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eef73ec-5017-4029-a383-a1d58148bff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load dimension tables\n",
    "dim_factories = spark.table(f\"{CATALOG}.{READ_SCHEMA}.dim_factories\")\n",
    "dim_models = spark.table(f\"{CATALOG}.{READ_SCHEMA}.dim_models\")\n",
    "\n",
    "# Join sensor data with factories\n",
    "enriched_df = (\n",
    "    sensors_df\n",
    "    .join(dim_factories, \"factory_id\", \"inner\")\n",
    "    .join(dim_models, \"model_id\", \"inner\")\n",
    "    .select(\n",
    "        \"device_id\",\n",
    "        \"timestamp\",\n",
    "        \"temperature\",\n",
    "        \"rotation_speed\",\n",
    "        \"factory_name\",\n",
    "        \"region\",\n",
    "        \"city\",\n",
    "        \"model_name\",\n",
    "        \"model_category\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Enriched sensor data:\")\n",
    "enriched_df.limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f290d3f6-7db4-44c0-9857-33d32e1f7376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Calculate Aggregations on Enriched Data\n",
    "\n",
    "Now that we have enriched data with factory and model names, let's calculate metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db4701f-d8e7-4c5e-b2ac-78e9ccdf5feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, round as spark_round\n",
    "\n",
    "# Performance by region and model category\n",
    "region_model_stats = (\n",
    "    enriched_df\n",
    "    .groupBy(\"region\", \"model_category\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "        spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "    )\n",
    "    .orderBy(\"region\", col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "print(\"Performance by Region and Model Category:\")\n",
    "region_model_stats.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478f010c-fef4-4365-a9ed-44f6be5b0451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Visualizations\n",
    "\n",
    "### Built-in Display Visualizations\n",
    "\n",
    "Databricks notebooks have built-in visualization capabilities. Let's create some charts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d20a545-a841-4752-b5f7-8602a66f4de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBQcmVwYXJlIGRhdGEgZm9yIHZpc3VhbGl6YXRpb24KZmFjdG9yeV90ZW1wX3ZpeiA9ICgKICAgIGVucmljaGVkX2RmCiAgICAuZ3JvdXBCeSgiZmFjdG9yeV9uYW1lIikKICAgIC5hZ2coCiAgICAgICAgc3Bhcmtfcm91bmQoYXZnKCJ0ZW1wZXJhdHVyZSIpLCAyKS5hbGlhcygiYXZnX3RlbXBlcmF0dXJlIiksCiAgICAgICAgc3Bhcmtfcm91bmQoYXZnKCJyb3RhdGlvbl9zcGVlZCIpLCAyKS5hbGlhcygiYXZnX3JvdGF0aW9uX3NwZWVkIikKICAgICkKICAgIC5vcmRlckJ5KGNvbCgiYXZnX3RlbXBlcmF0dXJlIikuZGVzYygpKQopCgojIERpc3BsYXkgLSBjbGljayB0aGUgY2hhcnQgaWNvbiB0byBjcmVhdGUgdmlzdWFsaXphdGlvbnMKZGlzcGxheShmYWN0b3J5X3RlbXBfdml6KQoKcHJpbnQoIlxuXG5CZWZvcmUgbW92aW5nIG9uOiIpCgpwcmludCgiIiIK8J+TiiBUcnkgVGhlc2UgVmlzdWFsaXphdGlvbnM6CjEuIENsaWNrIHRoZSArIGljb24gbmV4dCB0byB0aGUgIlRhYmxlIiBsYWJlbCBhbmQgY3JlYXRlIGEgbmV3IHZpc3VhbGl6YXRpb24KMi4gU2VsZWN0ICJCYXIiIGZyb20gdmlzdWFsaXphdGlvbiB0eXBlCjMuIFNlbGVjdCAnZmFjdG9yeV9uYW1lJyBpbiBYIGF4aXMKMy4gU2VsZWN0ICdhdmdfdGVtcGVyYXR1cmUnIGluIFkgYXhpcwo0LiBUcnkgZGlmZmVyZW50IGNoYXJ0IHR5cGVzOiBiYXIsIGxpbmUsIHBpZSwgc2NhdHRlcgoiIiIpCgoKcHJpbnQoIiIiCvCfk4ogVHJ5IERhdGEgRXhwbG9yYXRpb246CjEuIENsaWNrIHRoZSArIGljb24gbmV4dCB0byB0aGUgIlRhYmxlIiBsYWJlbAoyLiBTZWxlY3QgIkRhdGEgUHJvZmlsZSIKIiIiKQoK\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1769019333728,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "ansi",
         393
        ],
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "f6f8c6e6-ff3a-4b66-bf42-48284b7fc1e1",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 24.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1769019330021,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1769019268709,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data for visualization\n",
    "factory_temp_viz = (\n",
    "    enriched_df\n",
    "    .groupBy(\"factory_name\")\n",
    "    .agg(\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "        spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "    )\n",
    "    .orderBy(col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "# Display - click the chart icon to create visualizations\n",
    "display(factory_temp_viz)\n",
    "\n",
    "print(\"\\n\\nBefore moving on:\")\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Š Try These Visualizations:\n",
    "1. Click the + icon next to the \"Table\" label and create a new visualization\n",
    "2. Select \"Bar\" from visualization type\n",
    "3. Select 'factory_name' in X axis\n",
    "3. Select 'avg_temperature' in Y axis\n",
    "4. Try different chart types: bar, line, pie, scatter\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Š Try Data Exploration:\n",
    "1. Click the + icon next to the \"Table\" label\n",
    "2. Select \"Data Profile\"\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "babaeec0-7b41-4055-b86c-fddbe0939a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Saving Results\n",
    "\n",
    "After exploring and transforming your data, you can save the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdf447e-12d3-4d0e-bfcb-28f54eff63b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_table = f\"{CATALOG}.{WRITE_SCHEMA}.factory_temp_viz\"\n",
    "factory_temp_viz.write.mode(\"overwrite\").saveAsTable(output_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44619365-3b9d-4192-9fad-971d1ab8bfc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now that the results are saved, anyone can read the table (from other notebooks, dashboards, or more)\n",
    "read_table_df = spark.read.table(output_table)\n",
    "read_table_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36479c38-9295-4321-a933-1088f580344b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned the basics of data processing with Databricks Notebooks.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "âœ… **Understood Python basics** - Variables and functions  \n",
    "âœ… **Read data from Unity Catalog** - Loaded sensor tables  \n",
    "âœ… **Transformed data** - Filtered, selected, and added columns  \n",
    "âœ… **Joined tables** - Combined sensor data with dimension tables  \n",
    "âœ… **Created aggregations** - Calculated metrics by region and model  \n",
    "âœ… **Visualized data** - Built charts in the notebook  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **DataFrames are immutable** - Each transformation returns a new DataFrame\n",
    "2. **PySpark is like SQL** - Similar operations (select, filter, groupBy, join)\n",
    "3. **display()** shows results with automatic visualizations\n",
    "4. **Joins enrich data** - Combine fact tables with dimensions for context\n",
    "5. **Aggregations summarize** - groupBy + agg to calculate metrics\n",
    "\n",
    "### What's Next\n",
    "\n",
    "Now you can:\n",
    "- Build more complex transformations (Day 1, Notebook 5)\n",
    "- Create dashboards from your processed data (Day 1, Notebook 3)\n",
    "- Use this data for ML models (Day 2)\n",
    "\n",
    "**You're making great progress toward leadership's deadline!** ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "## Try This Out\n",
    "\n",
    "Want more practice? Try these exercises using what you learned:\n",
    "\n",
    "### 1. Use Databricks Assistant to Add a Window Function\n",
    "\n",
    "**Challenge:** Calculate a 3-row moving average of temperature for each device.\n",
    "\n",
    "**Steps:**\n",
    "1. Select a cell with DataFrame code\n",
    "2. Click the AI Assistant icon\n",
    "3. Type: \"Add a column with 3-row moving average of temperature partitioned by device_id\"\n",
    "4. Review and run the generated code\n",
    "\n",
    "**Hint:** You'll need `Window.partitionBy(\"device_id\").orderBy(\"timestamp\").rowsBetween(-1, 1)`\n",
    "\n",
    "### 2. Try Different Aggregations\n",
    "\n",
    "Practice more aggregation functions:\n",
    "```python\n",
    "# Find devices with high temperature variance\n",
    "from pyspark.sql.functions import stddev, min, max\n",
    "\n",
    "device_variance = (\n",
    "    sensors_df\n",
    "    .groupBy(\"device_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        stddev(\"temperature\").alias(\"stddev_temp\"),\n",
    "        min(\"temperature\").alias(\"min_temp\"),\n",
    "        max(\"temperature\").alias(\"max_temp\")\n",
    "    )\n",
    "    .orderBy(col(\"stddev_temp\").desc())\n",
    ")\n",
    "\n",
    "device_variance.limit(10).display()\n",
    "```\n",
    "\n",
    "**Questions to explore:**\n",
    "- Which devices have the highest temperature variance?\n",
    "- Is high variance correlated with defects?\n",
    "- Do certain factories have more variance?\n",
    "\n",
    "### 3. Create More Visualizations\n",
    "\n",
    "Try different chart types:\n",
    "- **Line chart** - Temperature trends over time by factory\n",
    "- **Pie chart** - Distribution of readings by region\n",
    "- **Scatter plot** - Temperature vs. rotation_speed (looking for correlations)\n",
    "- **Heatmap** - Average temperature by factory and model\n",
    "\n",
    "### 4. Practice SQL Magic\n",
    "\n",
    "Did you know you can write SQL directly in notebooks?\n",
    "\n",
    "Create a new cell and type:\n",
    "```sql\n",
    "%sql\n",
    "SELECT \n",
    "    factory_id,\n",
    "    COUNT(*) as reading_count,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM dwx_express_insights_platform_dev_working.db_crash_course.sensor_bronze\n",
    "WHERE timestamp >= current_date() - 7\n",
    "GROUP BY factory_id\n",
    "ORDER BY avg_temp DESC\n",
    "```\n",
    "\n",
    "### 5. Explore More Tables\n",
    "\n",
    "Apply what you learned to other tables:\n",
    "- `inspection_bronze` - Quality inspection data\n",
    "- `anomaly_detected` - Flagged anomalies\n",
    "- `inspection_silver` - Processed inspection data\n",
    "\n",
    "**Try:**\n",
    "- Join inspections with sensor data\n",
    "- Calculate defect rates by factory\n",
    "- Find correlation between sensor readings and defects\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Notebooks Documentation](https://docs.databricks.com/aws/en/notebooks/)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Databricks Assistant Guide](https://docs.databricks.com/aws/en/notebooks/databricks-assistant)\n",
    "\n",
    "**Great job!** You now have the skills to process IoT data and create insights for leadership. Keep going! ðŸ’ª"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "4 Notebooks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
