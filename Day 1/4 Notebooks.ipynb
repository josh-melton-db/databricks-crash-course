{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04e42aad-4405-42e3-962c-fc1b6be2dd41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Notebooks: Data Processing with Python and SQL\n",
    "\n",
    "**Scenario:** You've found the aircraft IoT sensor data. Now leadership needs you to analyze it, find patterns, and prepare it for dashboards and ML models. All before end of week!\n",
    "\n",
    "**Databricks Notebooks** are interactive documents where you'll write Python and SQL to process data.\n",
    "\n",
    "## Pre-Read: Python in 60 Seconds\n",
    "\n",
    "**New to Python?** Here's all you need to get started:\n",
    "\n",
    "### Variables\n",
    "Variables store data. No declaration needed:\n",
    "```python\n",
    "temperature = 75.5          # Number\n",
    "factory_name = \"Boeing\"     # Text (string)\n",
    "is_critical = True          # Boolean (True/False)\n",
    "device_ids = [1, 2, 3, 4]   # List\n",
    "```\n",
    "\n",
    "### Functions\n",
    "Functions are reusable blocks of code:\n",
    "```python\n",
    "def calculate_average(numbers):\n",
    "    total = sum(numbers)\n",
    "    count = len(numbers)\n",
    "    return total / count\n",
    "\n",
    "result = calculate_average([10, 20, 30])  # result = 20.0\n",
    "```\n",
    "\n",
    "**That's it!** You now know enough Python for this notebook. We'll explain everything else as we go.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "âœ… Read data from Unity Catalog  \n",
    "âœ… Transform data with PySpark  \n",
    "âœ… Calculate aggregations  \n",
    "âœ… Create visualizations  \n",
    "âœ… Save processed data  \n",
    "\n",
    "**Time to Complete:** 25-30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Notebook Basics\n",
    "2. Reading Data\n",
    "3. DataFrame Operations\n",
    "4. Joining Tables\n",
    "5. Visualizations\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Notebooks Documentation](https://docs.databricks.com/aws/en/notebooks/)\n",
    "- [Notebooks Code](https://docs.databricks.com/aws/en/notebooks/notebooks-code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96013d49-6d05-4e42-a627-82cf39294190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THESE VALUES!\n",
    "CATALOG = 'your_catalog'  # Update: Change to your catalog name\n",
    "SCHEMA = 'your_username'  # Update: Use your username (without special characters)\n",
    "\n",
    "print(f\"âœ… Using catalog: {CATALOG}\")\n",
    "print(f\"âœ… Using schema: {SCHEMA}\")\n",
    "print(f\"ðŸ“Š Tables will be at: {CATALOG}.{SCHEMA}.table_name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "473066a3-05ba-40fe-92f7-b496c65ac317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Notebook Basics\n",
    "\n",
    "### What are Databricks Notebooks?\n",
    "\n",
    "Notebooks are interactive documents containing:\n",
    "- **Code cells** - Execute Python, SQL, Scala, or R code\n",
    "- **Markdown cells** - Documentation and explanations\n",
    "- **Visualizations** - Built-in plotting capabilities\n",
    "- **Results** - Output from code execution\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "âœ… **Multi-language support** - Switch between languages in the same notebook  \n",
    "âœ… **Collaboration** - Real-time co-editing with teammates  \n",
    "âœ… **Version control** - Git integration for tracking changes  \n",
    "âœ… **Scheduling** - Run notebooks as automated jobs  \n",
    "âœ… **Interactive visualizations** - Built-in charting  \n",
    "\n",
    "### Magic Commands:\n",
    "\n",
    "- `%python` - Python code (default)\n",
    "- `%sql` - SQL queries\n",
    "- `%scala` - Scala code\n",
    "- `%r` - R code\n",
    "- `%md` - Markdown for documentation\n",
    "- `%sh` - Shell commands\n",
    "- `%pip` - Install Python packages\n",
    "\n",
    "### Keyboard Shortcut:\n",
    "\n",
    "- `Shift + Enter` - Run cell and move to next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa536bb1-89a6-4d75-a1b0-1d9c52cd078d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Reading Data\n",
    "\n",
    "### Reading from Unity Catalog Tables\n",
    "\n",
    "The simplest way to read data is using `spark.table()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06cc42b0-e49f-49ad-995b-e6f47c638410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read a table from Unity Catalog\n",
    "sensors_df = spark.table(f\"{CATALOG}.{SCHEMA}.sensor_bronze\")\n",
    "\n",
    "# Show schema\n",
    "print(\"Schema:\")\n",
    "sensors_df.printSchema()\n",
    "\n",
    "# Display first few rows\n",
    "sensors_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52a287a4-9d7d-41c6-8088-11b367928d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading from Files in Volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e198ea3-6137-4b64-a010-16e4206d8b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV files from a volume\n",
    "csv_path = f\"/Volumes/{CATALOG}/{SCHEMA}/sensor_data/\"\n",
    "\n",
    "df_from_volume = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(csv_path)\n",
    ")\n",
    "\n",
    "print(f\"Records read from volume: {df_from_volume.count():,}\")\n",
    "df_from_volume.limit(3).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f39f45-ebd2-4cc7-920f-16747819915a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Basic DataFrame Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfca87dd-fe63-498c-a85c-57ac6a84d2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get DataFrame information\n",
    "print(f\"Total rows: {sensors_df.count():,}\")\n",
    "print(f\"Total columns: {len(sensors_df.columns)}\")\n",
    "print(f\"\\nColumns: {sensors_df.columns}\")\n",
    "\n",
    "# Show summary statistics\n",
    "sensors_df.select(\"temperature\", \"rotation_speed\", \"air_pressure\").summary().display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5a4047-33d6-4170-9442-9ff8fff009c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. DataFrame Operations\n",
    "\n",
    "### Selecting Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d09e461-fa57-4fd3-a49a-97a40690f9f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select specific columns\n",
    "selected_df = sensors_df.select(\n",
    "    \"device_id\",\n",
    "    \"timestamp\",\n",
    "    \"temperature\",\n",
    "    \"rotation_speed\",\n",
    "    \"factory_id\"\n",
    ")\n",
    "\n",
    "selected_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fb4ac0e-1da7-4632-8611-ddff0f8caee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filtering Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f964302-466d-4108-84c8-5ce652dea077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for high temperatures\n",
    "high_temp_df = sensors_df.filter(col(\"temperature\") > 80)\n",
    "\n",
    "print(f\"High temperature readings: {high_temp_df.count():,}\")\n",
    "high_temp_df.limit(5).display()\n",
    "\n",
    "# Multiple conditions\n",
    "critical_df = sensors_df.filter(\n",
    "    (col(\"temperature\") > 80) & \n",
    "    (col(\"rotation_speed\") > 600)\n",
    ")\n",
    "\n",
    "print(f\"\\nCritical readings (high temp AND high speed): {critical_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63d22373-f5f6-473e-b305-c11edbc8d401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding and Transforming Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e96cdd3c-714f-4647-8b49-d8676e743f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, round as spark_round\n",
    "\n",
    "# Add temperature in Celsius\n",
    "transformed_df = sensors_df.withColumn(\n",
    "    \"temperature_celsius\",\n",
    "    spark_round((col(\"temperature\") - 32) * 5/9, 2)\n",
    ")\n",
    "\n",
    "# Add a status flag\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    \"temperature_status\",\n",
    "    when(col(\"temperature\") > 85, \"Critical\")\n",
    "    .when(col(\"temperature\") > 75, \"Warning\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# Calculate derived metric\n",
    "transformed_df = transformed_df.withColumn(\n",
    "    \"performance_index\",\n",
    "    spark_round(col(\"rotation_speed\") / col(\"air_pressure\") * 100, 2)\n",
    ")\n",
    "\n",
    "transformed_df.select(\n",
    "    \"device_id\",\n",
    "    \"temperature\",\n",
    "    \"temperature_celsius\",\n",
    "    \"temperature_status\",\n",
    "    \"performance_index\"\n",
    ").limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9cddef4-a7ec-4079-9fb5-8405764fe3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling Null Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b938152b-6df0-41b0-bed8-cebd60eb3e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "from pyspark.sql.functions import count, when, col, isnan\n",
    "\n",
    "null_counts = sensors_df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in sensors_df.columns\n",
    "])\n",
    "\n",
    "print(\"Null counts per column:\")\n",
    "null_counts.display()\n",
    "\n",
    "# Drop rows with any nulls\n",
    "clean_df = sensors_df.na.drop()\n",
    "\n",
    "# Fill nulls with specific values\n",
    "filled_df = sensors_df.fillna({\n",
    "    \"temperature\": 0,\n",
    "    \"air_pressure\": sensors_df.agg({\"air_pressure\": \"mean\"}).first()[0]\n",
    "})\n",
    "\n",
    "print(f\"\\nOriginal: {sensors_df.count():,} rows\")\n",
    "print(f\"After dropping nulls: {clean_df.count():,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0ea61f7-faf5-4fc1-ab73-96bf229678fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Joining Tables\n",
    "\n",
    "### Inner Join with Dimension Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eef73ec-5017-4029-a383-a1d58148bff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load dimension tables\n",
    "dim_factories = spark.table(f\"{CATALOG}.{SCHEMA}.dim_factories\")\n",
    "dim_models = spark.table(f\"{CATALOG}.{SCHEMA}.dim_models\")\n",
    "\n",
    "# Join sensor data with factories\n",
    "enriched_df = (\n",
    "    sensors_df\n",
    "    .join(dim_factories, \"factory_id\", \"inner\")\n",
    "    .join(dim_models, \"model_id\", \"inner\")\n",
    "    .select(\n",
    "        \"device_id\",\n",
    "        \"timestamp\",\n",
    "        \"temperature\",\n",
    "        \"rotation_speed\",\n",
    "        \"factory_name\",\n",
    "        \"region\",\n",
    "        \"city\",\n",
    "        \"model_name\",\n",
    "        \"model_category\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Enriched sensor data:\")\n",
    "enriched_df.limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f290d3f6-7db4-44c0-9857-33d32e1f7376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Calculate Aggregations on Enriched Data\n",
    "\n",
    "Now that we have enriched data with factory and model names, let's calculate metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db4701f-d8e7-4c5e-b2ac-78e9ccdf5feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count, round as spark_round\n",
    "\n",
    "# Performance by region and model category\n",
    "region_model_stats = (\n",
    "    enriched_df\n",
    "    .groupBy(\"region\", \"model_category\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "        spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "    )\n",
    "    .orderBy(\"region\", col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "print(\"Performance by Region and Model Category:\")\n",
    "region_model_stats.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "478f010c-fef4-4365-a9ed-44f6be5b0451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Visualizations\n",
    "\n",
    "### Built-in Display Visualizations\n",
    "\n",
    "Databricks notebooks have built-in visualization capabilities. Let's create some charts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d20a545-a841-4752-b5f7-8602a66f4de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "factory_temp_viz = (\n",
    "    enriched_df\n",
    "    .groupBy(\"factory_name\")\n",
    "    .agg(\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "        spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "    )\n",
    "    .orderBy(col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "# Display - click the chart icon to create visualizations\n",
    "factory_temp_viz.display()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Š Try These Visualizations:\n",
    "1. Click the bar chart icon below the table\n",
    "2. Drag 'factory_name' to Keys\n",
    "3. Drag 'avg_temperature' to Values\n",
    "4. Try different chart types: bar, line, pie, scatter\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36479c38-9295-4321-a933-1088f580344b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned the basics of data processing with Databricks Notebooks.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "âœ… **Understood Python basics** - Variables and functions  \n",
    "âœ… **Read data from Unity Catalog** - Loaded sensor tables  \n",
    "âœ… **Transformed data** - Filtered, selected, and added columns  \n",
    "âœ… **Joined tables** - Combined sensor data with dimension tables  \n",
    "âœ… **Created aggregations** - Calculated metrics by region and model  \n",
    "âœ… **Visualized data** - Built charts in the notebook  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **DataFrames are immutable** - Each transformation returns a new DataFrame\n",
    "2. **PySpark is like SQL** - Similar operations (select, filter, groupBy, join)\n",
    "3. **display()** shows results with automatic visualizations\n",
    "4. **Joins enrich data** - Combine fact tables with dimensions for context\n",
    "5. **Aggregations summarize** - groupBy + agg to calculate metrics\n",
    "\n",
    "### What's Next\n",
    "\n",
    "Now you can:\n",
    "- Build more complex transformations (Day 1, Notebook 5)\n",
    "- Create dashboards from your processed data (Day 1, Notebook 3)\n",
    "- Use this data for ML models (Day 2)\n",
    "\n",
    "**You're making great progress toward leadership's deadline!** ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "## Try This Out\n",
    "\n",
    "Want more practice? Try these exercises using what you learned:\n",
    "\n",
    "### 1. Use Databricks Assistant to Add a Window Function\n",
    "\n",
    "**Challenge:** Calculate a 3-row moving average of temperature for each device.\n",
    "\n",
    "**Steps:**\n",
    "1. Select a cell with DataFrame code\n",
    "2. Click the AI Assistant icon (or press Cmd/Ctrl + Shift + Space)\n",
    "3. Type: \"Add a column with 3-row moving average of temperature partitioned by device_id\"\n",
    "4. Review and run the generated code\n",
    "\n",
    "**Hint:** You'll need `Window.partitionBy(\"device_id\").orderBy(\"timestamp\").rowsBetween(-1, 1)`\n",
    "\n",
    "### 2. Try Different Aggregations\n",
    "\n",
    "Practice more aggregation functions:\n",
    "```python\n",
    "# Find devices with high temperature variance\n",
    "from pyspark.sql.functions import stddev, min, max\n",
    "\n",
    "device_variance = (\n",
    "    sensors_df\n",
    "    .groupBy(\"device_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        stddev(\"temperature\").alias(\"stddev_temp\"),\n",
    "        min(\"temperature\").alias(\"min_temp\"),\n",
    "        max(\"temperature\").alias(\"max_temp\")\n",
    "    )\n",
    "    .orderBy(col(\"stddev_temp\").desc())\n",
    ")\n",
    "\n",
    "device_variance.limit(10).display()\n",
    "```\n",
    "\n",
    "**Questions to explore:**\n",
    "- Which devices have the highest temperature variance?\n",
    "- Is high variance correlated with defects?\n",
    "- Do certain factories have more variance?\n",
    "\n",
    "### 3. Create More Visualizations\n",
    "\n",
    "Try different chart types:\n",
    "- **Line chart** - Temperature trends over time by factory\n",
    "- **Pie chart** - Distribution of readings by region\n",
    "- **Scatter plot** - Temperature vs. rotation_speed (looking for correlations)\n",
    "- **Heatmap** - Average temperature by factory and model\n",
    "\n",
    "### 4. Practice SQL Magic\n",
    "\n",
    "Did you know you can write SQL directly in notebooks?\n",
    "\n",
    "Create a new cell and type:\n",
    "```sql\n",
    "%sql\n",
    "SELECT \n",
    "    factory_id,\n",
    "    COUNT(*) as reading_count,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM your_catalog.your_schema.sensor_bronze\n",
    "WHERE timestamp >= current_date() - 7\n",
    "GROUP BY factory_id\n",
    "ORDER BY avg_temp DESC\n",
    "```\n",
    "\n",
    "### 5. Explore More Tables\n",
    "\n",
    "Apply what you learned to other tables:\n",
    "- `inspection_bronze` - Quality inspection data\n",
    "- `anomaly_detected` - Flagged anomalies\n",
    "- `inspection_silver` - Processed inspection data\n",
    "\n",
    "**Try:**\n",
    "- Join inspections with sensor data\n",
    "- Calculate defect rates by factory\n",
    "- Find correlation between sensor readings and defects\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Notebooks Documentation](https://docs.databricks.com/aws/en/notebooks/)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Databricks Assistant Guide](https://docs.databricks.com/aws/en/notebooks/databricks-assistant)\n",
    "\n",
    "**Great job!** You now have the skills to process IoT data and create insights for leadership. Keep going! ðŸ’ª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e55223-d02d-4e0a-a00f-c208ef32eab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load dimension tables\n",
    "dim_factories = spark.table(f\"{CATALOG}.{SCHEMA}.dim_factories\")\n",
    "dim_models = spark.table(f\"{CATALOG}.{SCHEMA}.dim_models\")\n",
    "\n",
    "# Join sensor data with factories\n",
    "enriched_df = (\n",
    "    sensors_df\n",
    "    .join(dim_factories, \"factory_id\", \"inner\")\n",
    "    .join(dim_models, \"model_id\", \"inner\")\n",
    "    .select(\n",
    "        \"device_id\",\n",
    "        \"timestamp\",\n",
    "        \"temperature\",\n",
    "        \"rotation_speed\",\n",
    "        \"factory_name\",\n",
    "        \"region\",\n",
    "        \"city\",\n",
    "        \"model_name\",\n",
    "        \"model_category\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Enriched sensor data:\")\n",
    "enriched_df.limit(10).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd35929d-b5d8-4377-8d72-b2881d038968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Aggregate Enriched Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da40e437-0cbf-43b5-8598-3a4fe7d5486a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performance by region and model category\n",
    "region_model_stats = (\n",
    "    enriched_df\n",
    "    .groupBy(\"region\", \"model_category\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"reading_count\"),\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "        spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "    )\n",
    "    .orderBy(\"region\", col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "print(\"Performance by Region and Model Category:\")\n",
    "region_model_stats.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b12fea98-1b9a-46f1-a34c-5ff0c17cacbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "### Built-in Display Visualizations\n",
    "\n",
    "Databricks notebooks have built-in visualization capabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b58da6-fd7a-4539-ac5b-1b5bcece0531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "factory_temp_viz = (\n",
    "    enriched_df\n",
    "    .groupBy(\"factory_name\")\n",
    "    .agg(\n",
    "        spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "        spark_round(avg(\"rotation_speed\"), 2).alias(\"avg_rotation_speed\")\n",
    "    )\n",
    "    .orderBy(col(\"avg_temperature\").desc())\n",
    ")\n",
    "\n",
    "# Display - click the chart icon to create visualizations\n",
    "factory_temp_viz.display()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Š Try These Visualizations:\n",
    "1. Click the bar chart icon below the table\n",
    "2. Drag 'factory_name' to Keys\n",
    "3. Drag 'avg_temperature' to Values\n",
    "4. Try different chart types: bar, line, pie, scatter\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea8f28d8-f57c-422f-9dfa-bd58781eb129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Writing Data\n",
    "\n",
    "### Save Transformed Data as a New Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbc713ed-767d-4436-8794-13a1e1fc65a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a processed dataset\n",
    "processed_df = (\n",
    "    enriched_df\n",
    "    .withColumn(\"temperature_celsius\", spark_round((col(\"temperature\") - 32) * 5/9, 2))\n",
    "    .withColumn(\n",
    "        \"temperature_category\",\n",
    "        when(col(\"temperature\") > 85, \"Critical\")\n",
    "        .when(col(\"temperature\") > 75, \"High\")\n",
    "        .when(col(\"temperature\") > 65, \"Normal\")\n",
    "        .otherwise(\"Low\")\n",
    "    )\n",
    "    .withColumn(\"processing_timestamp\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Save as Delta table\n",
    "output_table = f\"{CATALOG}.{SCHEMA}.sensor_processed\"\n",
    "\n",
    "processed_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(output_table)\n",
    "\n",
    "print(f\"âœ… Data saved to: {output_table}\")\n",
    "print(f\"   Total records: {processed_df.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49cf3e76-8b1e-4587-bdba-6f521651eca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Append vs Overwrite Modes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aebd038d-80d3-4443-a6ce-b05fd5661cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Example: Append mode (adds new records)\n",
    "# processed_df.write.format(\"delta\").mode(\"append\").saveAsTable(output_table)\n",
    "\n",
    "# Example: Overwrite mode (replaces all data)\n",
    "# processed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_table)\n",
    "\n",
    "# Example: Write to a specific location\n",
    "# processed_df.write.format(\"delta\").mode(\"overwrite\").save(f\"/Volumes/{CATALOG}/{SCHEMA}/processed_data/\")\n",
    "\n",
    "print(\"\"\"\n",
    "Write Modes:\n",
    "- overwrite: Replace existing data\n",
    "- append: Add new records\n",
    "- ignore: Skip if table exists\n",
    "- error: Fail if table exists (default)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2493b24-b584-4b7a-90c6-f5b04db84c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "âœ… **Notebook basics** - Magic commands, keyboard shortcuts, collaboration  \n",
    "âœ… **Reading data** - From Unity Catalog tables and volumes  \n",
    "âœ… **DataFrame operations** - Select, filter, transform columns  \n",
    "âœ… **Aggregations** - GroupBy, window functions, statistics  \n",
    "âœ… **Joins** - Enrich data with dimension tables  \n",
    "âœ… **Visualizations** - Built-in charts and custom HTML  \n",
    "âœ… **Writing data** - Save transformed data to Delta tables  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **DataFrames are immutable** - Each transformation returns a new DataFrame\n",
    "2. **Lazy evaluation** - Transformations are planned, not executed until an action\n",
    "3. **display()** - Best way to view results with automatic visualizations\n",
    "4. **Chaining operations** - Use parentheses for readable multi-line transformations\n",
    "5. **Unity Catalog** - Simplifies data access with three-level namespace\n",
    "\n",
    "### PySpark Best Practices:\n",
    "\n",
    "**Performance:**\n",
    "- Use `filter()` early to reduce data volume\n",
    "- Avoid `collect()` on large datasets (brings all data to driver)\n",
    "- Use `cache()` for DataFrames accessed multiple times\n",
    "- Partition output data appropriately\n",
    "\n",
    "**Code Quality:**\n",
    "- Use explicit column references with `col()`\n",
    "- Chain transformations for readability\n",
    "- Add comments for complex logic\n",
    "- Use meaningful variable names\n",
    "\n",
    "**Data Quality:**\n",
    "- Check for nulls and handle them explicitly\n",
    "- Validate data types match expectations\n",
    "- Use `na.drop()` or `fillna()` strategically\n",
    "- Add data quality checks before writing\n",
    "\n",
    "### Common DataFrame Actions:\n",
    "\n",
    "**Transformations (lazy):**\n",
    "- `select()`, `filter()`, `withColumn()`, `groupBy()`, `join()`\n",
    "\n",
    "**Actions (trigger execution):**\n",
    "- `display()`, `show()`, `count()`, `collect()`, `write()`\n",
    "\n",
    "### Try These:\n",
    "\n",
    "- Explore **SQL magic** (`%sql`) for SQL queries in notebooks\n",
    "- Learn about **Delta Lake** features (time travel, MERGE, OPTIMIZE)\n",
    "- Try **Structured Streaming** for real-time data processing\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Notebooks Documentation](https://docs.databricks.com/aws/en/notebooks/)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Delta Lake Guide](https://docs.databricks.com/aws/en/delta/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b26c58-e8f2-4b78-b9c4-848be2bb970d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "4 Notebooks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
