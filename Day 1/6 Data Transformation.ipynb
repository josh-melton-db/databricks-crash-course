{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation: Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "**Lakeflow Spark Declarative Pipelines** (formerly Delta Live Tables) is a declarative framework for building reliable, maintainable, and testable data pipelines. With DLT, you focus on **what** transformations you want, not **how** to execute them.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "✅ Create declarative pipelines with streaming tables and materialized views  \n",
    "✅ Use the new multi-file editor for organized pipeline development  \n",
    "✅ Implement incremental data processing with flows  \n",
    "✅ Apply data quality expectations automatically  \n",
    "✅ Build Bronze-Silver-Gold architecture declaratively  \n",
    "\n",
    "---\n",
    "\n",
    "## Why Declarative Pipelines?\n",
    "\n",
    "**Traditional Approach:**\n",
    "- Manual checkpointing and state management\n",
    "- Custom incremental processing logic\n",
    "- Manual data quality checks\n",
    "- Complex error handling\n",
    "\n",
    "**Declarative Approach:**\n",
    "- Automatic checkpointing and recovery\n",
    "- Built-in incremental processing\n",
    "- Declarative data quality expectations\n",
    "- Simplified error handling and monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## Use Case: Production-Ready IoT Pipeline\n",
    "\n",
    "We'll build a complete IoT data pipeline that:\n",
    "- Ingests raw sensor data incrementally\n",
    "- Cleans and validates data with expectations\n",
    "- Enriches with dimensional data\n",
    "- Creates business-level aggregations\n",
    "- Handles late-arriving data automatically\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Understanding Declarative Pipelines](#understanding)\n",
    "2. [Multi-File Editor Setup](#multi-file)\n",
    "3. [Streaming Tables vs Materialized Views](#tables-vs-views)\n",
    "4. [Building the Bronze Layer](#bronze)\n",
    "5. [Building the Silver Layer](#silver)\n",
    "6. [Building the Gold Layer](#gold)\n",
    "7. [Data Quality Expectations](#expectations)\n",
    "8. [Complete Pipeline Example](#complete-example)\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Multi-File Editor](https://docs.databricks.com/aws/en/ldp/multi-file-editor)\n",
    "- [Streaming Tables](https://docs.databricks.com/aws/en/ldp/streaming-tables)\n",
    "- [Flows](https://docs.databricks.com/aws/en/ldp/flows)\n",
    "- [Materialized Views](https://docs.databricks.com/aws/en/ldp/materialized-views)\n",
    "- [Load Data](https://docs.databricks.com/aws/en/ldp/load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Understanding Declarative Pipelines <a id=\"understanding\"></a>\n",
    "\n",
    "### What are Lakeflow Spark Declarative Pipelines?\n",
    "\n",
    "Declarative pipelines let you define **what** data transformations you want, and the framework automatically handles:\n",
    "- **Incremental processing** - Only process new/changed data\n",
    "- **Dependency management** - Automatically determine execution order\n",
    "- **Data quality** - Built-in validation with expectations\n",
    "- **Monitoring** - Automatic lineage and observability\n",
    "- **Recovery** - Checkpoint management and error handling\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Streaming Tables:**\n",
    "- For append-only, incremental data ingestion\n",
    "- Each input row processed only once\n",
    "- Low-latency streaming transformations\n",
    "- Ideal for bronze and silver layers\n",
    "\n",
    "**Materialized Views:**\n",
    "- For stateful aggregations and joins\n",
    "- Automatically recompute when source data changes\n",
    "- Always return correct, consistent results\n",
    "- Ideal for gold layer aggregations\n",
    "\n",
    "**Flows:**\n",
    "- Define how data moves between tables\n",
    "- Support multiple sources writing to one target\n",
    "- Automatic incremental processing\n",
    "- Named for checkpoint management\n",
    "\n",
    "### Benefits\n",
    "\n",
    "✅ **Less code** - Framework handles boilerplate  \n",
    "✅ **Automatic scaling** - Adapts to data volume  \n",
    "✅ **Built-in quality** - Expectations track data quality  \n",
    "✅ **Production-ready** - Monitoring, alerts, lineage included  \n",
    "✅ **Incremental by default** - Efficient processing  \n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Source Data (Volumes/Kafka/Tables)\n",
    "         ↓\n",
    "Streaming Tables (Bronze) - Ingest raw data\n",
    "         ↓\n",
    "Streaming Tables (Silver) - Clean & enrich\n",
    "         ↓\n",
    "Materialized Views (Gold) - Aggregate metrics\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-File Editor Setup <a id=\"multi-file\"></a>\n",
    "\n",
    "### What is the Multi-File Editor?\n",
    "\n",
    "The **Lakeflow Pipelines Editor** is an integrated development environment for building pipelines. It provides:\n",
    "- **Multiple file support** - Organize code into modules\n",
    "- **Visual pipeline graph** - See data flow and dependencies\n",
    "- **Data preview** - Inspect data at any pipeline stage\n",
    "- **Selective execution** - Test individual transformations\n",
    "- **Debugging tools** - Identify and fix issues quickly\n",
    "\n",
    "### Creating a Pipeline\n",
    "\n",
    "**Step 1: Create New ETL Pipeline**\n",
    "\n",
    "1. Click **New** at the top of the sidebar\n",
    "2. Select **ETL pipeline**\n",
    "3. Provide a unique name: `iot_sensor_pipeline`\n",
    "4. Specify default catalog: `default`\n",
    "5. Specify default schema: `db_crash_course`\n",
    "6. Choose to start with sample code in **Python** or **SQL**\n",
    "7. Click **Create**\n",
    "\n",
    "**Step 2: Explore the Editor**\n",
    "\n",
    "You'll see:\n",
    "- **File browser** (left) - Organize pipeline code into multiple files\n",
    "- **Code editor** (center) - Write pipeline definitions\n",
    "- **Pipeline graph** (bottom) - Visual DAG showing data flow\n",
    "- **Data preview** - Preview results at each stage\n",
    "\n",
    "### File Organization Best Practices\n",
    "\n",
    "```\n",
    "iot_sensor_pipeline/\n",
    "├── bronze/\n",
    "│   ├── sensor_ingest.py\n",
    "│   └── inspection_ingest.py\n",
    "├── silver/\n",
    "│   ├── sensor_clean.py\n",
    "│   └── inspection_enriched.py\n",
    "├── gold/\n",
    "│   ├── factory_kpis.py\n",
    "│   └── device_health.py\n",
    "└── common/\n",
    "    └── expectations.py\n",
    "```\n",
    "\n",
    "### Benefits of Multi-File Organization:\n",
    "\n",
    "✅ **Modularity** - Separate concerns into logical files  \n",
    "✅ **Reusability** - Share common functions across files  \n",
    "✅ **Collaboration** - Multiple developers work on different files  \n",
    "✅ **Testing** - Test individual components  \n",
    "✅ **Maintenance** - Easier to understand and update  \n",
    "\n",
    "**Reference:** [Multi-File Editor Documentation](https://docs.databricks.com/aws/en/ldp/multi-file-editor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Tables vs Materialized Views <a id=\"tables-vs-views\"></a>\n",
    "\n",
    "### Streaming Tables\n",
    "\n",
    "**Use for:** Incremental, append-only data processing\n",
    "\n",
    "**Characteristics:**\n",
    "- Each row processed exactly once\n",
    "- Low-latency streaming\n",
    "- Supports stateful operations with watermarks\n",
    "- Best for bronze and silver layers\n",
    "\n",
    "**When to use:**\n",
    "- Data ingestion from files, Kafka, etc.\n",
    "- Append-only transformations\n",
    "- Stateful streaming (with bounded state)\n",
    "- ETL jobs that process new data only\n",
    "\n",
    "**Example use cases:**\n",
    "- Ingest CSV files from cloud storage\n",
    "- Read from Kafka topics\n",
    "- Clean and filter streaming data\n",
    "\n",
    "---\n",
    "\n",
    "### Materialized Views\n",
    "\n",
    "**Use for:** Stateful aggregations that need to recompute\n",
    "\n",
    "**Characteristics:**\n",
    "- Always return correct, up-to-date results\n",
    "- Automatically recompute when sources change\n",
    "- Support complex joins and aggregations\n",
    "- Best for gold layer aggregations\n",
    "\n",
    "**When to use:**\n",
    "- Aggregations that need to update (not just append)\n",
    "- Joins with slowly changing dimensions\n",
    "- Views that need to be always correct\n",
    "- Gold layer business metrics\n",
    "\n",
    "**Example use cases:**\n",
    "- Daily/hourly aggregations\n",
    "- Complex joins across multiple tables\n",
    "- Business KPIs and reports\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | Streaming Table | Materialized View |\n",
    "|---------|----------------|-------------------|\n",
    "| **Processing** | Incremental (each row once) | Recomputes as needed |\n",
    "| **Joins** | Stream-snapshot (point-in-time) | Always correct |\n",
    "| **Updates** | Append-only | Updates/deletes |\n",
    "| **Latency** | Low (continuous) | Higher (batch) |\n",
    "| **Use case** | Ingestion, ETL | Aggregations, reporting |\n",
    "\n",
    "**Reference:** \n",
    "- [Streaming Tables](https://docs.databricks.com/aws/en/ldp/streaming-tables)\n",
    "- [Materialized Views](https://docs.databricks.com/aws/en/ldp/materialized-views)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Building the Bronze Layer <a id=\"bronze\"></a>\n",
    "\n",
    "### Bronze Layer: Streaming Tables for Data Ingestion\n",
    "\n",
    "The bronze layer ingests raw data using **streaming tables** that process new files incrementally.\n",
    "\n",
    "### Python Example: Ingest Sensor Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Python pipeline code - save as bronze/sensor_ingest.py\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import pipelines as dp\n",
    "\n",
    "# Configuration (would typically come from pipeline settings)\n",
    "CATALOG = \"default\"\n",
    "SCHEMA = \"db_crash_course\"\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_bronze\",\n",
    "    comment=\"Raw sensor readings ingested from CSV files in volumes\"\n",
    ")\n",
    "def sensor_bronze():\n",
    "    \"\"\"\n",
    "    Ingest sensor data from volumes using Auto Loader.\n",
    "    Auto Loader automatically handles:\n",
    "    - Schema inference and evolution\n",
    "    - Checkpoint management\n",
    "    - Exactly-once processing\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"/Volumes/{CATALOG}/{SCHEMA}/sensor_data/\")\n",
    "    )\n",
    "\n",
    "# SQL Equivalent:\n",
    "sql_example = \"\"\"\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_bronze\n",
    "  COMMENT 'Raw sensor readings ingested from CSV files'\n",
    "AS SELECT * FROM STREAM read_files(\n",
    "  '/Volumes/default/db_crash_course/sensor_data/',\n",
    "  format => 'csv',\n",
    "  header => 'true'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "print(\"Streaming table for bronze layer sensor ingestion:\")\n",
    "print(sql_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python Example: Ingest Inspection Data\n",
    "\n",
    "\"\"\"\n",
    "Save as bronze/inspection_ingest.py\n",
    "\"\"\"\n",
    "\n",
    "@dp.table(\n",
    "    name=\"inspection_bronze\",\n",
    "    comment=\"Raw inspection records from volumes\"\n",
    ")\n",
    "def inspection_bronze():\n",
    "    \"\"\"Ingest inspection data with Auto Loader.\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"/Volumes/{CATALOG}/{SCHEMA}/inspection_data/\")\n",
    "    )\n",
    "\n",
    "print(\"\"\"\n",
    "✅ Bronze Layer Benefits with Streaming Tables:\n",
    "\n",
    "1. **Auto Loader** handles file discovery automatically\n",
    "2. **Schema inference** detects column types\n",
    "3. **Schema evolution** handles new columns gracefully\n",
    "4. **Exactly-once** processing guaranteed\n",
    "5. **Checkpointing** automatic - no manual state management\n",
    "\n",
    "The framework monitors the source location and processes new files as they arrive!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Silver Layer <a id=\"silver\"></a>\n",
    "\n",
    "### Silver Layer: Data Quality and Enrichment\n",
    "\n",
    "The silver layer cleans, validates, and enriches data using streaming tables.\n",
    "\n",
    "### Data Quality with Expectations\n",
    "\n",
    "**Expectations** are declarative data quality rules that:\n",
    "- Validate data automatically\n",
    "- Track quality metrics over time\n",
    "- Handle violations with specified actions (fail, drop, warn)\n",
    "\n",
    "### Python Example: Clean and Validate Sensor Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save as silver/sensor_clean.py\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col, when, abs as abs_func\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_silver\",\n",
    "    comment=\"Cleaned and validated sensor readings\",\n",
    "    # Data quality expectations\n",
    "    expect={\n",
    "        \"valid_device_id\": \"device_id IS NOT NULL\",\n",
    "        \"valid_timestamp\": \"timestamp IS NOT NULL\",\n",
    "        \"valid_temperature_range\": \"temperature BETWEEN -50 AND 150\"\n",
    "    },\n",
    "    # Drop rows that fail expectations\n",
    "    expect_or_drop={\n",
    "        \"positive_air_pressure_after_fix\": \"air_pressure > 0\"\n",
    "    }\n",
    ")\n",
    "def sensor_silver():\n",
    "    \"\"\"\n",
    "    Clean sensor bronze data:\n",
    "    - Fix negative air pressure values\n",
    "    - Validate data quality with expectations\n",
    "    - Add derived fields\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "    \n",
    "    return (\n",
    "        dp.read_stream(\"sensor_bronze\")  # Read from bronze streaming table\n",
    "        # Fix negative air pressure (data quality issue)\n",
    "        .withColumn(\"air_pressure\",\n",
    "                   when(col(\"air_pressure\") < 0, abs_func(col(\"air_pressure\")))\n",
    "                   .otherwise(col(\"air_pressure\")))\n",
    "        # Add processing timestamp\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "print(\"\"\"\n",
    "✅ Data Quality Expectations Applied:\n",
    "\n",
    "- valid_device_id: Ensures device_id is not null\n",
    "- valid_timestamp: Ensures timestamp is present  \n",
    "- valid_temperature_range: Validates temperature is realistic (-50 to 150°F)\n",
    "- positive_air_pressure_after_fix: Drops rows with invalid pressure after fix\n",
    "\n",
    "Failed expectations are logged and tracked automatically!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Example: Same Transformation in SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Save as silver/sensor_clean.sql\n",
    "\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_silver (\n",
    "  CONSTRAINT valid_device_id EXPECT (device_id IS NOT NULL),\n",
    "  CONSTRAINT valid_timestamp EXPECT (timestamp IS NOT NULL),\n",
    "  CONSTRAINT valid_temperature_range EXPECT (temperature BETWEEN -50 AND 150),\n",
    "  CONSTRAINT positive_air_pressure EXPECT OR DROP (air_pressure > 0)\n",
    ")\n",
    "COMMENT 'Cleaned and validated sensor readings'\n",
    "AS SELECT\n",
    "  device_id,\n",
    "  trip_id,\n",
    "  factory_id,\n",
    "  model_id,\n",
    "  timestamp,\n",
    "  CASE \n",
    "    WHEN air_pressure < 0 THEN ABS(air_pressure)\n",
    "    ELSE air_pressure\n",
    "  END as air_pressure,\n",
    "  temperature,\n",
    "  rotation_speed,\n",
    "  airflow_rate,\n",
    "  delay,\n",
    "  density,\n",
    "  current_timestamp() as processed_at\n",
    "FROM STREAM(sensor_bronze);\n",
    "\n",
    "print(\"SQL version provides same functionality with declarative syntax!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich with Dimension Tables (Stream-Snapshot Join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save as silver/sensor_enriched.py\n",
    "\"\"\"\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_enriched\",\n",
    "    comment=\"Sensor data enriched with dimension tables\"\n",
    ")\n",
    "def sensor_enriched():\n",
    "    \"\"\"\n",
    "    Enrich sensor data with factory, model, and device dimensions.\n",
    "    \n",
    "    Note: This is a stream-snapshot join. The dimension tables are\n",
    "    read as snapshots when the stream starts. Changes to dimensions\n",
    "    won't reflect unless the stream is restarted or refreshed.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import round as spark_round\n",
    "    \n",
    "    # Read streaming data\n",
    "    sensors = dp.read_stream(\"sensor_silver\")\n",
    "    \n",
    "    # Read dimension tables as static snapshots\n",
    "    dim_factories = spark.read.table(f\"{CATALOG}.{SCHEMA}.dim_factories\")\n",
    "    dim_models = spark.read.table(f\"{CATALOG}.{SCHEMA}.dim_models\")\n",
    "    dim_devices = spark.read.table(f\"{CATALOG}.{SCHEMA}.dim_devices\")\n",
    "    \n",
    "    # Join with dimensions\n",
    "    enriched = (\n",
    "        sensors\n",
    "        .join(dim_devices.select(\"device_id\", \"installation_date\", \"status\"), \n",
    "              \"device_id\", \"left\")\n",
    "        .join(dim_factories.select(\"factory_id\", \"factory_name\", \"region\", \"city\"),\n",
    "              \"factory_id\", \"left\")\n",
    "        .join(dim_models.select(\"model_id\", \"model_name\", \"model_family\", \"model_category\"),\n",
    "              \"model_id\", \"left\")\n",
    "    )\n",
    "    \n",
    "    # Add business calculations\n",
    "    return (\n",
    "        enriched\n",
    "        .withColumn(\"temperature_celsius\",\n",
    "                   spark_round((col(\"temperature\") - 32) * 5 / 9, 2))\n",
    "        .withColumn(\"temperature_zone\",\n",
    "                   when(col(\"temperature\") > 85, \"Critical\")\n",
    "                   .when(col(\"temperature\") > 75, \"Warning\")\n",
    "                   .otherwise(\"Normal\"))\n",
    "        .withColumn(\"risk_score\",\n",
    "                   spark_round(\n",
    "                       (col(\"temperature\") / 100 * 0.4) +\n",
    "                       (col(\"rotation_speed\") / 1000 * 0.3) +\n",
    "                       (col(\"density\") / 5 * 0.3),\n",
    "                       2\n",
    "                   ))\n",
    "    )\n",
    "\n",
    "print(\"Stream-snapshot join: Dimensions are snapshotted at stream start time.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Multiple Flows to Write to a Single Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example: Multiple sources writing to one table\n",
    "\"\"\"\n",
    "\n",
    "# Create the target streaming table first\n",
    "dp.create_streaming_table(\n",
    "    name=\"sensor_all_regions\",\n",
    "    comment=\"Combined sensor data from all regions\"\n",
    ")\n",
    "\n",
    "# Flow 1: US West data\n",
    "@dp.append_flow(target=\"sensor_all_regions\")\n",
    "def append_us_west():\n",
    "    return (\n",
    "        dp.read_stream(\"sensor_silver\")\n",
    "        .filter(col(\"region\") == \"West\")\n",
    "    )\n",
    "\n",
    "# Flow 2: US East data\n",
    "@dp.append_flow(target=\"sensor_all_regions\")\n",
    "def append_us_east():\n",
    "    return (\n",
    "        dp.read_stream(\"sensor_silver\")\n",
    "        .filter(col(\"region\") == \"East\")\n",
    "    )\n",
    "\n",
    "# Flow 3: International data\n",
    "@dp.append_flow(target=\"sensor_all_regions\")\n",
    "def append_international():\n",
    "    return (\n",
    "        dp.read_stream(\"sensor_silver\")\n",
    "        .filter(~col(\"region\").isin([\"West\", \"East\"]))\n",
    "    )\n",
    "\n",
    "print(\"\"\"\n",
    "✅ Multiple Flows Pattern:\n",
    "\n",
    "Benefits:\n",
    "- Add new sources without full refresh\n",
    "- Process regions independently\n",
    "- Better parallelization\n",
    "- Easier to maintain\n",
    "\n",
    "Each flow has its own checkpoint - isolated failure handling!\n",
    "\"\"\")\n",
    "\n",
    "# Reference: https://docs.databricks.com/aws/en/ldp/flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building the Gold Layer <a id=\"gold\"></a>\n",
    "\n",
    "### Gold Layer: Materialized Views for Aggregations\n",
    "\n",
    "Use **materialized views** for aggregations that need to update (not just append).\n",
    "\n",
    "### Materialized View: Factory KPIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save as gold/factory_kpis.py\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import avg, max, count, countDistinct, round as spark_round\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"factory_kpis_gold\",\n",
    "    comment=\"Factory-level KPIs and performance metrics\"\n",
    ")\n",
    "def factory_kpis_gold():\n",
    "    \"\"\"\n",
    "    Aggregate factory-level metrics.\n",
    "    Uses materialized view so aggregations update when new data arrives.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"sensor_enriched\")  # Read from silver\n",
    "        .groupBy(\"factory_id\", \"factory_name\", \"region\", \"city\")\n",
    "        .agg(\n",
    "            countDistinct(\"device_id\").alias(\"total_devices\"),\n",
    "            count(\"*\").alias(\"total_readings\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "            spark_round(max(\"temperature\"), 2).alias(\"max_temperature\"),\n",
    "            spark_round(avg(\"risk_score\"), 2).alias(\"avg_risk_score\"),\n",
    "            count(when(col(\"temperature_zone\") == \"Critical\", 1)).alias(\"critical_readings\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# SQL Equivalent:\n",
    "sql_mv = \"\"\"\n",
    "CREATE OR REFRESH MATERIALIZED VIEW factory_kpis_gold\n",
    "COMMENT 'Factory-level KPIs and performance metrics'\n",
    "AS SELECT\n",
    "  factory_id,\n",
    "  factory_name,\n",
    "  region,\n",
    "  city,\n",
    "  COUNT(DISTINCT device_id) as total_devices,\n",
    "  COUNT(*) as total_readings,\n",
    "  ROUND(AVG(temperature), 2) as avg_temperature,\n",
    "  MAX(temperature) as max_temperature,\n",
    "  ROUND(AVG(risk_score), 2) as avg_risk_score,\n",
    "  COUNT(CASE WHEN temperature_zone = 'Critical' THEN 1 END) as critical_readings\n",
    "FROM sensor_enriched\n",
    "GROUP BY factory_id, factory_name, region, city;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Materialized views automatically recompute when source data changes!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materialized View: Device Health Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save as gold/device_health.py\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import avg, count, stddev, round as spark_round\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"device_health_gold\",\n",
    "    comment=\"Device-level health metrics and status\"\n",
    ")\n",
    "def device_health_gold():\n",
    "    \"\"\"\n",
    "    Calculate per-device health metrics.\n",
    "    Updates automatically as new sensor data arrives.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.table(\"sensor_enriched\")\n",
    "        .groupBy(\"device_id\", \"factory_name\", \"model_name\", \"model_category\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"reading_count\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "            spark_round(stddev(\"temperature\"), 2).alias(\"temp_stddev\"),\n",
    "            spark_round(avg(\"risk_score\"), 2).alias(\"avg_risk_score\"),\n",
    "            count(when(col(\"temperature_zone\") == \"Critical\", 1)).alias(\"critical_count\")\n",
    "        )\n",
    "        .withColumn(\"health_status\",\n",
    "                   when(col(\"avg_risk_score\") > 0.7, \"Poor\")\n",
    "                   .when(col(\"avg_risk_score\") > 0.5, \"Fair\")\n",
    "                   .otherwise(\"Good\"))\n",
    "    )\n",
    "\n",
    "print(\"Materialized view handles incremental aggregation updates automatically!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Expectations <a id=\"expectations\"></a>\n",
    "\n",
    "### Three Types of Expectations\n",
    "\n",
    "**1. `expect` (Track Violations):**\n",
    "- Logs failed records but doesn't drop them\n",
    "- Good for monitoring non-critical issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dp.table(\n",
    "    name=\"sensor_with_tracking\",\n",
    "    expect={\n",
    "        \"valid_temperature\": \"temperature IS NOT NULL\"  # Track but don't drop\n",
    "    }\n",
    ")\n",
    "def sensor_with_tracking():\n",
    "    return dp.read_stream(\"sensor_bronze\")\n",
    "\n",
    "print(\"expect: Violations are logged in metrics but rows pass through\")\n",
    "\n",
    "# ---\n",
    "\n",
    "print(\"\\n2. expect_or_drop (Drop Invalid Rows):\")\n",
    "print(\"- Drops rows that fail validation\")\n",
    "print(\"- Good for critical data quality rules\")\n",
    "\n",
    "sensor_strict = \"\"\"\n",
    "@dp.table(\n",
    "    expect_or_drop={\n",
    "        \"required_fields\": \"device_id IS NOT NULL AND timestamp IS NOT NULL\",\n",
    "        \"valid_values\": \"air_pressure > 0 AND temperature > -50\"\n",
    "    }\n",
    ")\n",
    "def sensor_strict():\n",
    "    return dp.read_stream(\"sensor_bronze\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nexpect_or_drop: Invalid rows are dropped, violations tracked\")\n",
    "\n",
    "# ---\n",
    "\n",
    "print(\"\\n3. expect_or_fail (Stop Pipeline on Violations):\")\n",
    "print(\"- Pipeline fails if expectations not met\")\n",
    "print(\"- Good for critical data pipelines\")\n",
    "\n",
    "sensor_critical = \"\"\"\n",
    "@dp.table(\n",
    "    expect_or_fail={\n",
    "        \"no_nulls\": \"device_id IS NOT NULL AND timestamp IS NOT NULL\"\n",
    "    }\n",
    ")\n",
    "def sensor_critical():\n",
    "    return dp.read_stream(\"sensor_bronze\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nexpect_or_fail: Pipeline stops if violations occur\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Example with Expectations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive example with multiple expectation types\n",
    "\"\"\"\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_silver_validated\",\n",
    "    comment=\"Fully validated sensor data with comprehensive quality checks\",\n",
    "    # Track these issues (don't drop)\n",
    "    expect={\n",
    "        \"valid_device\": \"device_id IS NOT NULL\",\n",
    "        \"has_timestamp\": \"timestamp IS NOT NULL\",\n",
    "        \"reasonable_temp\": \"temperature BETWEEN -50 AND 150\"\n",
    "    },\n",
    "    # Drop rows with critical failures\n",
    "    expect_or_drop={\n",
    "        \"positive_pressure\": \"air_pressure > 0\",\n",
    "        \"positive_rotation\": \"rotation_speed >= 0\"\n",
    "    },\n",
    "    # Fail pipeline if too many critical issues\n",
    "    expect_or_fail={\n",
    "        \"sufficient_data_quality\": \"air_pressure > 0\"\n",
    "    }\n",
    ")\n",
    "def sensor_silver_validated():\n",
    "    \"\"\"Silver table with comprehensive data quality checks.\"\"\"\n",
    "    return (\n",
    "        dp.read_stream(\"sensor_bronze\")\n",
    "        .withColumn(\"air_pressure\",\n",
    "                   when(col(\"air_pressure\") < 0, abs_func(col(\"air_pressure\")))\n",
    "                   .otherwise(col(\"air_pressure\")))\n",
    "    )\n",
    "\n",
    "print(\"\"\"\n",
    "✅ Multi-level Data Quality:\n",
    "\n",
    "expect: Track but don't block (monitoring)\n",
    "expect_or_drop: Drop bad rows (cleaning)\n",
    "expect_or_fail: Stop pipeline (critical issues)\n",
    "\n",
    "All expectations are tracked in pipeline metrics!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Pipeline Example <a id=\"complete-example\"></a>\n",
    "\n",
    "### Full IoT Pipeline with All Layers\n",
    "\n",
    "Here's a complete pipeline definition you can use as a template. Save this as a pipeline in the multi-file editor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete pipeline: iot_pipeline.py\n",
    "Run this in the Lakeflow Pipelines Editor\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import col, when, abs as abs_func, round as spark_round, current_timestamp\n",
    "\n",
    "# ========================================\n",
    "# BRONZE LAYER - Data Ingestion\n",
    "# ========================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_bronze\",\n",
    "    comment=\"Raw sensor data ingested from volumes\"\n",
    ")\n",
    "def sensor_bronze():\n",
    "    \"\"\"Ingest sensor CSV files incrementally with Auto Loader.\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/default/db_crash_course/sensor_data/\")\n",
    "    )\n",
    "\n",
    "@dp.table(\n",
    "    name=\"inspection_bronze\",\n",
    "    comment=\"Raw inspection data from volumes\"\n",
    ")\n",
    "def inspection_bronze():\n",
    "    \"\"\"Ingest inspection CSV files incrementally.\"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/Volumes/default/db_crash_course/inspection_data/\")\n",
    "    )\n",
    "\n",
    "# ========================================\n",
    "# SILVER LAYER - Cleaning & Validation\n",
    "# ========================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_silver\",\n",
    "    comment=\"Cleaned and validated sensor data\",\n",
    "    expect={\n",
    "        \"valid_device_id\": \"device_id IS NOT NULL\",\n",
    "        \"valid_timestamp\": \"timestamp IS NOT NULL\"\n",
    "    },\n",
    "    expect_or_drop={\n",
    "        \"valid_temperature\": \"temperature BETWEEN -50 AND 150\",\n",
    "        \"positive_pressure\": \"air_pressure > 0\"\n",
    "    }\n",
    ")\n",
    "def sensor_silver():\n",
    "    \"\"\"Clean sensor data and apply quality checks.\"\"\"\n",
    "    return (\n",
    "        dp.read_stream(\"sensor_bronze\")\n",
    "        .withColumn(\"air_pressure\",\n",
    "                   when(col(\"air_pressure\") < 0, abs_func(col(\"air_pressure\")))\n",
    "                   .otherwise(col(\"air_pressure\")))\n",
    "        .withColumn(\"processed_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "print(\"Complete pipeline structure defined declaratively!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue: Silver Layer Enrichment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SILVER LAYER - Enrichment\n",
    "# ========================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_enriched\",\n",
    "    comment=\"Sensor data enriched with dimensions and business metrics\"\n",
    ")\n",
    "def sensor_enriched():\n",
    "    \"\"\"\n",
    "    Enrich with dimensions and add calculated fields.\n",
    "    Uses stream-snapshot joins.\n",
    "    \"\"\"\n",
    "    # Stream from silver\n",
    "    sensors = dp.read_stream(\"sensor_silver\")\n",
    "    \n",
    "    # Snapshot dimensions\n",
    "    factories = spark.read.table(\"default.db_crash_course.dim_factories\")\n",
    "    models = spark.read.table(\"default.db_crash_course.dim_models\")\n",
    "    devices = spark.read.table(\"default.db_crash_course.dim_devices\")\n",
    "    \n",
    "    # Join and calculate\n",
    "    enriched = (\n",
    "        sensors\n",
    "        .join(devices, \"device_id\", \"left\")\n",
    "        .join(factories, \"factory_id\", \"left\")\n",
    "        .join(models, \"model_id\", \"left\")\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        enriched\n",
    "        .withColumn(\"temperature_celsius\",\n",
    "                   spark_round((col(\"temperature\") - 32) * 5 / 9, 2))\n",
    "        .withColumn(\"temperature_zone\",\n",
    "                   when(col(\"temperature\") > 85, \"Critical\")\n",
    "                   .when(col(\"temperature\") > 75, \"Warning\")\n",
    "                   .otherwise(\"Normal\"))\n",
    "        .withColumn(\"risk_score\",\n",
    "                   spark_round(\n",
    "                       (col(\"temperature\") / 100 * 0.4) +\n",
    "                       (col(\"rotation_speed\") / 1000 * 0.3) +\n",
    "                       (col(\"density\") / 5 * 0.3),\n",
    "                       2\n",
    "                   ))\n",
    "    )\n",
    "\n",
    "print(\"Silver layer: Cleaned, validated, and enriched!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue: Gold Layer Materialized Views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# GOLD LAYER - Business Aggregations\n",
    "# ========================================\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"factory_kpis_gold\",\n",
    "    comment=\"Factory-level KPIs for dashboards\"\n",
    ")\n",
    "def factory_kpis_gold():\n",
    "    \"\"\"Aggregate metrics by factory.\"\"\"\n",
    "    from pyspark.sql.functions import avg, count, countDistinct\n",
    "    \n",
    "    return (\n",
    "        spark.read.table(\"sensor_enriched\")\n",
    "        .groupBy(\"factory_id\", \"factory_name\", \"region\")\n",
    "        .agg(\n",
    "            countDistinct(\"device_id\").alias(\"total_devices\"),\n",
    "            count(\"*\").alias(\"total_readings\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "            spark_round(avg(\"risk_score\"), 2).alias(\"avg_risk_score\"),\n",
    "            count(when(col(\"temperature_zone\") == \"Critical\", 1)).alias(\"critical_readings\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "@dp.materialized_view(\n",
    "    name=\"device_health_gold\",\n",
    "    comment=\"Device health metrics and status\"\n",
    ")\n",
    "def device_health_gold():\n",
    "    \"\"\"Per-device health aggregations.\"\"\"\n",
    "    from pyspark.sql.functions import avg, count, stddev\n",
    "    \n",
    "    return (\n",
    "        spark.read.table(\"sensor_enriched\")\n",
    "        .groupBy(\"device_id\", \"factory_name\", \"model_name\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"reading_count\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
    "            spark_round(avg(\"risk_score\"), 2).alias(\"avg_risk_score\")\n",
    "        )\n",
    "        .withColumn(\"health_status\",\n",
    "                   when(col(\"avg_risk_score\") > 0.7, \"Poor\")\n",
    "                   .when(col(\"avg_risk_score\") > 0.5, \"Fair\")\n",
    "                   .otherwise(\"Good\"))\n",
    "    )\n",
    "\n",
    "print(\"Gold layer: Materialized views for business metrics!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Deploy This Pipeline\n",
    "\n",
    "**Step 1: Create Pipeline in UI**\n",
    "\n",
    "1. Click **New** → **ETL Pipeline**\n",
    "2. Name: `iot_sensor_pipeline`\n",
    "3. Set catalog: `default`\n",
    "4. Set schema: `db_crash_course`\n",
    "\n",
    "**Step 2: Add the Code**\n",
    "\n",
    "Option A: Single File\n",
    "- Paste the complete code above into the main pipeline file\n",
    "\n",
    "Option B: Multi-File (Recommended)\n",
    "- Create `bronze/sensor_ingest.py` - Bronze tables\n",
    "- Create `silver/sensor_clean.py` - Silver tables\n",
    "- Create `gold/aggregations.py` - Gold views\n",
    "- Import functions as needed\n",
    "\n",
    "**Step 3: Configure Pipeline Settings**\n",
    "\n",
    "- **Target**: Catalog and schema where tables are created\n",
    "- **Cluster**: Compute resources (auto-scaling recommended)\n",
    "- **Continuous vs Triggered**: \n",
    "  - Continuous: Always running, low latency\n",
    "  - Triggered: Run on schedule or manual trigger\n",
    "\n",
    "**Step 4: Start the Pipeline**\n",
    "\n",
    "1. Click **Start** in the pipeline editor\n",
    "2. Monitor progress in the pipeline graph\n",
    "3. View data quality metrics\n",
    "4. Inspect data at each stage\n",
    "\n",
    "**Step 5: Monitor and Maintain**\n",
    "\n",
    "- View expectations dashboard for data quality\n",
    "- Check pipeline event log for errors\n",
    "- Review lineage graph\n",
    "- Set up alerts for failures\n",
    "\n",
    "**Reference:** [Multi-File Editor](https://docs.databricks.com/aws/en/ldp/multi-file-editor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SILVER LAYER - Enrichment (continued)\n",
    "# ========================================\n",
    "\n",
    "@dp.table(\n",
    "    name=\"sensor_enriched\",\n",
    "    comment=\"Sensor data enriched with dimensions\"\n",
    ")\n",
    "def sensor_enriched():\n",
    "    \"\"\"Enrich with dimensional data.\"\"\"\n",
    "    sensors = dp.read_stream(\"sensor_silver\")\n",
    "    \n",
    "    # Read dimensions as snapshots\n",
    "    factories = spark.read.table(\"default.db_crash_course.dim_factories\")\n",
    "    models = spark.read.table(\"default.db_crash_course.dim_models\")\n",
    "    \n",
    "    enriched = (\n",
    "        sensors\n",
    "        .join(factories.select(\"factory_id\", \"factory_name\", \"region\"), \"factory_id\", \"left\")\n",
    "        .join(models.select(\"model_id\", \"model_name\", \"model_category\"), \"model_id\", \"left\")\n",
    "    )\n",
    "    \n",
    "    return enriched.withColumn(\n",
    "        \"risk_score\",\n",
    "        spark_round((col(\"temperature\") / 100 * 0.4) + (col(\"rotation_speed\") / 1000 * 0.6), 2)\n",
    "    )\n",
    "\n",
    "# ========================================\n",
    "# GOLD LAYER - Materialized Views\n",
    "# ========================================\n",
    "\n",
    "@dp.materialized_view(name=\"factory_kpis_gold\")\n",
    "def factory_kpis_gold():\n",
    "    \"\"\"Factory-level aggregations.\"\"\"\n",
    "    from pyspark.sql.functions import avg, count, countDistinct\n",
    "    \n",
    "    return (\n",
    "        spark.read.table(\"sensor_enriched\")\n",
    "        .groupBy(\"factory_name\", \"region\")\n",
    "        .agg(\n",
    "            countDistinct(\"device_id\").alias(\"device_count\"),\n",
    "            spark_round(avg(\"temperature\"), 2).alias(\"avg_temp\"),\n",
    "            spark_round(avg(\"risk_score\"), 2).alias(\"avg_risk\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"✅ Complete pipeline definition - ready to deploy!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Version: Complete Pipeline\n",
    "\n",
    "You can also define the entire pipeline in SQL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Complete SQL pipeline\n",
    "-- Save as pipeline.sql\n",
    "\n",
    "-- BRONZE: Ingest sensor data\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_bronze\n",
    "AS SELECT * FROM STREAM read_files(\n",
    "  '/Volumes/default/db_crash_course/sensor_data/',\n",
    "  format => 'csv',\n",
    "  header => 'true'\n",
    ");\n",
    "\n",
    "-- SILVER: Clean and validate\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_silver (\n",
    "  CONSTRAINT valid_device EXPECT (device_id IS NOT NULL),\n",
    "  CONSTRAINT valid_temp EXPECT (temperature BETWEEN -50 AND 150),\n",
    "  CONSTRAINT positive_pressure EXPECT OR DROP (air_pressure > 0)\n",
    ")\n",
    "AS SELECT\n",
    "  device_id,\n",
    "  factory_id,\n",
    "  model_id,\n",
    "  timestamp,\n",
    "  CASE WHEN air_pressure < 0 THEN ABS(air_pressure) ELSE air_pressure END as air_pressure,\n",
    "  temperature,\n",
    "  rotation_speed,\n",
    "  density,\n",
    "  current_timestamp() as processed_at\n",
    "FROM STREAM(sensor_bronze);\n",
    "\n",
    "-- SILVER: Enrich\n",
    "CREATE OR REFRESH STREAMING TABLE sensor_enriched\n",
    "AS SELECT\n",
    "  s.*,\n",
    "  f.factory_name,\n",
    "  f.region,\n",
    "  m.model_name,\n",
    "  m.model_category,\n",
    "  ROUND((s.temperature - 32) * 5 / 9, 2) as temperature_celsius,\n",
    "  CASE\n",
    "    WHEN s.temperature > 85 THEN 'Critical'\n",
    "    WHEN s.temperature > 75 THEN 'Warning'\n",
    "    ELSE 'Normal'\n",
    "  END as temperature_zone\n",
    "FROM STREAM(sensor_silver) s\n",
    "LEFT JOIN default.db_crash_course.dim_factories f ON s.factory_id = f.factory_id\n",
    "LEFT JOIN default.db_crash_course.dim_models m ON s.model_id = m.model_id;\n",
    "\n",
    "-- GOLD: Factory aggregations\n",
    "CREATE OR REFRESH MATERIALIZED VIEW factory_kpis_gold\n",
    "AS SELECT\n",
    "  factory_name,\n",
    "  region,\n",
    "  COUNT(DISTINCT device_id) as device_count,\n",
    "  ROUND(AVG(temperature), 2) as avg_temperature,\n",
    "  COUNT(CASE WHEN temperature_zone = 'Critical' THEN 1 END) as critical_readings\n",
    "FROM sensor_enriched\n",
    "GROUP BY factory_name, region;\n",
    "\n",
    "sql_pipeline = \"\"\"SQL pipeline: Concise, declarative, production-ready!\"\"\"\n",
    "print(sql_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Patterns\n",
    "\n",
    "### Pattern 1: Backfilling Historical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ONCE to insert historical data one time\n",
    "@dp.append_flow(\n",
    "    target=\"sensor_silver\",\n",
    "    once=True  # Runs only once unless full refresh\n",
    ")\n",
    "def backfill_historical():\n",
    "    \"\"\"Backfill historical sensor data from archive.\"\"\"\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .load(\"/Volumes/default/db_crash_course/archive/historical_sensors/\")\n",
    "    )\n",
    "\n",
    "print(\"\"\"\n",
    "✅ Backfill Pattern:\n",
    "\n",
    "- ONCE flag: Runs one time only\n",
    "- Won't reprocess on incremental updates\n",
    "- Perfect for loading historical data\n",
    "- Combines with streaming for complete dataset\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Change Data Capture (CDC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle CDC data automatically\n",
    "@dp.table(\n",
    "    name=\"devices_current\"\n",
    ")\n",
    "def devices_current():\n",
    "    \"\"\"\n",
    "    Apply CDC changes to maintain current state.\n",
    "    Auto CDC handles INSERT, UPDATE, DELETE operations.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dp.read_stream(\"devices_cdc\")\n",
    "        .apply_changes(\n",
    "            keys=[\"device_id\"],\n",
    "            sequence_by=\"update_timestamp\",\n",
    "            stored_as_scd_type=\"1\"  # Type 1: Keep current state only\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Or with streaming CDC source:\n",
    "devices_scd2 = \"\"\"\n",
    "@dp.table\n",
    "def devices_historical():\n",
    "    return (\n",
    "        dp.read_stream(\"devices_cdc\")\n",
    "        .apply_changes(\n",
    "            keys=[\"device_id\"],\n",
    "            sequence_by=\"update_timestamp\",\n",
    "            stored_as_scd_type=\"2\"  # Type 2: Keep full history\n",
    "        )\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "✅ CDC Pattern:\n",
    "\n",
    "SCD Type 1: Keep current state (overwrites)\n",
    "SCD Type 2: Keep full history (versioning)\n",
    "\n",
    "Framework handles:\n",
    "- Deduplication\n",
    "- Ordering by sequence\n",
    "- Merge operations\n",
    "- History tracking (Type 2)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Watermarks for Stateful Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use watermarks for time-windowed aggregations\n",
    "@dp.table(name=\"sensor_hourly_aggregates\")\n",
    "def sensor_hourly_aggregates():\n",
    "    \"\"\"\n",
    "    Hourly aggregations with watermark for handling late data.\n",
    "    Watermark: Wait up to 2 hours for late-arriving data.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import window, avg, count\n",
    "    \n",
    "    return (\n",
    "        dp.read_stream(\"sensor_silver\")\n",
    "        .withWatermark(\"timestamp\", \"2 hours\")  # Handle late data up to 2 hours\n",
    "        .groupBy(\n",
    "            window(col(\"timestamp\"), \"1 hour\"),  # 1-hour windows\n",
    "            \"factory_id\"\n",
    "        )\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"reading_count\"),\n",
    "            avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "            avg(\"rotation_speed\").alias(\"avg_rotation_speed\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\"\"\n",
    "✅ Watermark Pattern:\n",
    "\n",
    "- Handles late-arriving data gracefully\n",
    "- Prevents unbounded state growth\n",
    "- Required for windowed aggregations\n",
    "- Balances latency vs completeness\n",
    "\n",
    "Example: withWatermark(\"timestamp\", \"2 hours\")\n",
    "Waits up to 2 hours for late data before finalizing windows.\n",
    "\"\"\")\n",
    "\n",
    "# Reference: https://docs.databricks.com/aws/en/ldp/streaming-tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ **Declarative Pipelines** - Define what, not how  \n",
    "✅ **Multi-file editor** - Organize code into modules  \n",
    "✅ **Streaming tables** - Incremental, append-only processing  \n",
    "✅ **Materialized views** - Auto-updating aggregations  \n",
    "✅ **Flows** - Multiple sources to one target  \n",
    "✅ **Expectations** - Declarative data quality  \n",
    "✅ **Advanced patterns** - Backfilling, CDC, watermarks  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Declarative > Imperative**: Focus on transformations, not orchestration\n",
    "2. **Streaming tables** for bronze/silver (append-only)\n",
    "3. **Materialized views** for gold (aggregations that update)\n",
    "4. **Expectations** provide automatic data quality tracking\n",
    "5. **Multi-file editor** enables better code organization\n",
    "6. **Flows** allow multiple sources to write to one target\n",
    "7. **Framework handles**: Checkpointing, recovery, lineage, monitoring\n",
    "\n",
    "### Streaming Tables vs Materialized Views\n",
    "\n",
    "| Use Case | Use This |\n",
    "|----------|----------|\n",
    "| Ingest files | Streaming table |\n",
    "| Clean/filter | Streaming table |\n",
    "| Append-only transforms | Streaming table |\n",
    "| Aggregations | Materialized view |\n",
    "| Joins with updates | Materialized view |\n",
    "| Always-correct views | Materialized view |\n",
    "\n",
    "### Production Benefits:\n",
    "\n",
    "**Automatic:**\n",
    "- Incremental processing\n",
    "- Checkpoint management  \n",
    "- Schema evolution\n",
    "- Data quality tracking\n",
    "- Lineage visualization\n",
    "- Error recovery\n",
    "\n",
    "**Built-in:**\n",
    "- Monitoring dashboard\n",
    "- Expectations metrics\n",
    "- Event logs\n",
    "- Performance insights\n",
    "\n",
    "### Creating Your First Pipeline:\n",
    "\n",
    "1. **UI: New → ETL Pipeline**\n",
    "2. **Choose Python or SQL**\n",
    "3. **Define tables/views** with decorators\n",
    "4. **Add expectations** for quality\n",
    "5. **Start pipeline** and monitor\n",
    "6. **Iterate** based on metrics\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "**Organization:**\n",
    "- Use multi-file editor for large pipelines\n",
    "- Separate bronze/silver/gold into different files\n",
    "- Create reusable helper functions\n",
    "- Document expectations and business logic\n",
    "\n",
    "**Performance:**\n",
    "- Use streaming tables for append-only data\n",
    "- Use materialized views for aggregations\n",
    "- Add watermarks for windowed operations\n",
    "- Partition appropriately (framework optimizes automatically)\n",
    "\n",
    "**Quality:**\n",
    "- Start with `expect` to understand data\n",
    "- Add `expect_or_drop` for known issues\n",
    "- Use `expect_or_fail` for critical rules\n",
    "- Monitor expectations dashboard\n",
    "\n",
    "**Operations:**\n",
    "- Use continuous mode for real-time\n",
    "- Use triggered mode for batch\n",
    "- Set up alerts for failures\n",
    "- Review event logs regularly\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Create your first declarative pipeline for the IoT dataset\n",
    "- Explore the pipeline graph visualization\n",
    "- Review expectations dashboard after running\n",
    "- Try different expectation strategies\n",
    "- Add CDC for slowly changing dimensions\n",
    "- Implement windowed aggregations with watermarks\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Lakeflow Pipelines Documentation](https://docs.databricks.com/aws/en/ldp/)\n",
    "- [Multi-File Editor](https://docs.databricks.com/aws/en/ldp/multi-file-editor)\n",
    "- [Streaming Tables](https://docs.databricks.com/aws/en/ldp/streaming-tables)\n",
    "- [Materialized Views](https://docs.databricks.com/aws/en/ldp/materialized-views)\n",
    "- [Flows](https://docs.databricks.com/aws/en/ldp/flows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42c02305-b387-4dd2-8662-9e148e06b162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "6 Data Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
