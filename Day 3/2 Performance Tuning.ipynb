{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b033a0f-0a66-47c4-a00d-f4d1480138b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Performance Tuning: SQL & Table Optimization\n",
    "\n",
    "**The Situation:** Leadership wants dashboards, predictive models, and AI agents ready by Friday. Your plane IoT data is growing fast, and queries that worked yesterday are timing out today.\n",
    "\n",
    "**The Problem:** Slow queries = missed deadlines + angry leadership\n",
    "\n",
    "**The Solution:** Get familiar with both SQL and table optimization techniques to get sub-second query times.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn (30 minutes)\n",
    "\n",
    "‚úÖ **SQL Optimization:** Predicate pushdown, join strategies, broadcast hints  \n",
    "‚úÖ **Table Optimization:** File compaction, Z-Ordering, Liquid Clustering  \n",
    "‚úÖ **Materialized Views:** Pre-compute expensive aggregations  \n",
    "‚úÖ **Deletion Vectors:** Fast updates and deletes  \n",
    "‚úÖ **Performance Measurement:** Before/after comparisons  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Day 1 & 2\n",
    "- `sensor_bronze`, `dim_factories`, `dim_devices` tables loaded\n",
    "- SQL Warehouse or cluster running\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Delta Lake Performance](https://docs.databricks.com/en/delta/tune-file-layout.html)\n",
    "- [Liquid Clustering](https://docs.databricks.com/en/delta/clustering.html)\n",
    "- [Query Optimization](https://docs.databricks.com/en/optimizations/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f91b22df-6941-4306-b382-ead819f9eef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG = 'main'\n",
    "SCHEMA = 'onboarding'\n",
    "USER = spark.sql(\"SELECT current_user()\").collect()[0][0].split('@')[0].replace('.', '_')\n",
    "\n",
    "print(f\"Using: {CATALOG}.{SCHEMA}\")\n",
    "print(f\"User: {USER}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f7f8e9-fb02-41a1-bff9-ac1058cd0c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Understanding Performance Bottlenecks\n",
    "\n",
    "### Common Performance Killers\n",
    "\n",
    "| Problem | Impact | Solution |\n",
    "|---------|--------|----------|\n",
    "| üêå **Small Files** | Too many file opens | OPTIMIZE |\n",
    "| üêå **Full Table Scans** | Read entire table | Z-Order, predicates |\n",
    "| üêå **Data Shuffle** | Network overhead | Broadcast joins |\n",
    "| üêå **Wrong Join Type** | Memory spills | Join hints |\n",
    "| üêå **Repeated Computation** | Wasted resources | Materialized views |\n",
    "| üêå **Inefficient Predicates** | No pushdown | Proper filters |\n",
    "\n",
    "### Performance Toolkit\n",
    "\n",
    "**SQL Optimization:**\n",
    "- Predicate pushdown (filter early)\n",
    "- Join hints (BROADCAST, SHUFFLE_HASH)\n",
    "- Proper WHERE clause design\n",
    "\n",
    "**Table Optimization:**\n",
    "- File compaction (OPTIMIZE)\n",
    "- Z-Ordering (data layout)\n",
    "- Liquid Clustering (auto-optimize)\n",
    "- Deletion Vectors (fast updates)\n",
    "\n",
    "**Query Results:**\n",
    "- Materialized Views\n",
    "- Caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee60b8c9-0db9-4217-b714-fee0831f6b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Creating a \"Bad\" Table for Demonstration\n",
    "\n",
    "Let's intentionally create a poorly optimized table with:\n",
    "- Many small files (simulating streaming ingestion)\n",
    "- Random data layout (no locality)\n",
    "- No optimization\n",
    "\n",
    "This represents what happens in real production systems without proper maintenance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59859860-2901-4fac-8ae5-71b78111568a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create unoptimized table with random layout\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "AS\n",
    "SELECT \n",
    "    device_id,\n",
    "    trip_id,\n",
    "    factory_id,\n",
    "    model_id,\n",
    "    timestamp,\n",
    "    airflow_rate,\n",
    "    rotation_speed,\n",
    "    air_pressure,\n",
    "    temperature,\n",
    "    delay,\n",
    "    density\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_bronze\n",
    "ORDER BY RAND()  -- Random order = worst case for data locality!\n",
    "LIMIT 200000  -- Use subset for demo\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created unoptimized table with random layout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b17f70c-b1f4-42c4-8bed-361e7f7edb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Simulate many small files (like streaming writes)\n",
    "# This is what happens with continuous ingestion without auto-compaction\n",
    "\n",
    "for i in range(15):  # Create 15 small file batches\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "    SELECT \n",
    "        device_id,\n",
    "        trip_id,\n",
    "        factory_id,\n",
    "        model_id,\n",
    "        timestamp,\n",
    "        airflow_rate,\n",
    "        rotation_speed,\n",
    "        air_pressure,\n",
    "        temperature,\n",
    "        delay,\n",
    "        density\n",
    "    FROM {CATALOG}.{SCHEMA}.{USER}_sensor_bronze\n",
    "    WHERE MOD(device_id, 15) = {i}\n",
    "    LIMIT 800\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úÖ Created many small files (simulating poor ingestion patterns)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243da376-2fd9-4b59-90a6-b118a5247f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check table statistics - look at the file count!\n",
    "display(spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\", \"minReaderVersion\", \"minWriterVersion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e27f9af3-4eea-4129-98b9-315d45e9d91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç What to Look For:\n",
    "\n",
    "- **numFiles**: High number (hundreds of thousands or millions) = Performance problem!\n",
    "- **sizeInBytes**: Total size, but spread across too many files\n",
    "\n",
    "**Problem:** Every query must:\n",
    "1. List all files\n",
    "2. Open each file\n",
    "3. Read metadata\n",
    "4. Scan for relevant data\n",
    "\n",
    "With many small files, overhead dominates actual work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9e4a4c-7544-41fa-987a-4cc9e5535b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: SQL Optimization - Predicate Pushdown\n",
    "\n",
    "**Key Concept:** Push filters as close to the data as possible.\n",
    "\n",
    "### ‚ùå Bad: Function on Column (Prevents Pushdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54748cab-a12c-462f-826e-1ff81a1e921e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# BAD: Using SUBSTRING on timestamp prevents predicate pushdown\n",
    "start = time.time()\n",
    "\n",
    "result_bad = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "WHERE SUBSTRING(CAST(timestamp AS STRING), 1, 10) >= DATE_SUB(CURRENT_DATE(), 7)\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_bad = result_bad.count()\n",
    "time_bad = time.time() - start\n",
    "\n",
    "print(f\"‚ùå BAD Query Time: {time_bad:.2f} seconds\")\n",
    "print(f\"   Results: {count_bad} rows\")\n",
    "print(f\"   Problem: Function on column prevents statistics-based filtering!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e329a1-fef6-49ef-9c6f-aac56c5337e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GOOD: Direct filter on timestamp column enables predicate pushdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10fec205-18b1-4509-8ab7-c7aa230079b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "result_good = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "WHERE timestamp >= CURRENT_DATE() - INTERVAL 7 DAYS\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_good = result_good.count()\n",
    "time_good = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ GOOD Query Time: {time_good:.2f} seconds\")\n",
    "print(f\"   Results: {count_good} rows\")\n",
    "print(f\"   Speedup: {time_bad/time_good:.1f}x faster!\")\n",
    "print(f\"   Reason: Databricks can use file statistics to skip irrelevant files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a805eaa-10b2-4282-beb0-7f6cd15d260e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Predicate Pushdown Best Practices\n",
    "\n",
    "**DO:**\n",
    "```sql\n",
    "WHERE timestamp >= '2024-01-01'  -- Direct column comparison\n",
    "WHERE device_id IN (1, 2, 3)     -- Direct value check\n",
    "WHERE factory_id = 'A06'          -- Equality on column\n",
    "```\n",
    "\n",
    "**DON'T:**\n",
    "```sql\n",
    "WHERE DATE(timestamp) = '2024-01-01'      -- Function prevents pushdown\n",
    "WHERE SUBSTRING(device_id, 1, 2) = '10'   -- Function on column\n",
    "WHERE UPPER(factory_id) = 'A06'           -- Transformation blocks optimization\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "107c6c3f-c146-4bab-8b7f-517158e5133d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: SQL Optimization - Join Strategies\n",
    "\n",
    "### Understanding Join Types\n",
    "\n",
    "| Join Type | Best For | Cost |\n",
    "|-----------|----------|------|\n",
    "| **Broadcast Join** | Small table (< 10MB) | Low - no shuffle |\n",
    "| **Shuffle Hash Join** | Large tables | High - shuffle both |\n",
    "| **Sort Merge Join** | Large sorted tables | Medium |\n",
    "\n",
    "### ‚ùå Bad: Let Spark guess (might shuffle large tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb6eb883-bf10-4272-b549-b16c6ee9fe8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Without hint - Spark might choose inefficient join strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d129f811-c074-4fd2-89c1-521ba297f36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "result_no_hint = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    s.device_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    COUNT(*) as reading_count,\n",
    "    AVG(s.temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized s\n",
    "JOIN {CATALOG}.{SCHEMA}.{USER}_dim_factories f \n",
    "  ON s.factory_id = f.factory_id\n",
    "GROUP BY s.device_id, f.factory_name, f.region\n",
    "\"\"\")\n",
    "\n",
    "count_no_hint = result_no_hint.count()\n",
    "time_no_hint = time.time() - start\n",
    "\n",
    "print(f\"‚ö†Ô∏è  No Hint Query Time: {time_no_hint:.2f} seconds\")\n",
    "print(f\"   Spark may shuffle both tables unnecessarily\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5a100ea-752e-40bb-971f-c65ae609c584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ Good: Broadcast Small Dimension Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd33e351-f9f5-4801-b2af-e8dce6939aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With BROADCAST hint - force efficient join strategy\n",
    "start = time.time()\n",
    "\n",
    "result_broadcast = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    /*+ BROADCAST(f) */\n",
    "    s.device_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    COUNT(*) as reading_count,\n",
    "    AVG(s.temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized s\n",
    "JOIN {CATALOG}.{SCHEMA}.{USER}_dim_factories f \n",
    "  ON s.factory_id = f.factory_id\n",
    "GROUP BY s.device_id, f.factory_name, f.region\n",
    "\"\"\")\n",
    "\n",
    "count_broadcast = result_broadcast.count()\n",
    "time_broadcast = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Broadcast Join Time: {time_broadcast:.2f} seconds\")\n",
    "print(f\"   Speedup: {time_no_hint/time_broadcast:.1f}x faster!\")\n",
    "print(f\"   Only dimension table sent to executors - no shuffle of fact table!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bde12c4-ee8e-4fe6-8a76-7bf997cd24be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Join Optimization Rules\n",
    "\n",
    "**BROADCAST when:**\n",
    "- Dimension table < 10MB\n",
    "- Reference data (factories, models, devices)\n",
    "- Lookup tables\n",
    "\n",
    "**Let Spark choose when:**\n",
    "- Both tables are large\n",
    "- Join cardinality is unknown\n",
    "- Adaptive Query Execution is enabled (default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc6fcd77-47b8-4494-ac3e-3f96df217b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5: Table Optimization - File Compaction\n",
    "\n",
    "**Problem:** Many small files cause overhead\n",
    "\n",
    "**Solution:** OPTIMIZE command compacts small files into larger ones\n",
    "\n",
    "**Target:** 128MB - 1GB per file (default: 1GB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceaa81d1-2861-44aa-a871-5718820fd0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check current file situation\n",
    "detail_before = spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "\n",
    "files_before = detail_before['numFiles']\n",
    "size_mb = detail_before['sizeInBytes'] / 1024 / 1024\n",
    "\n",
    "print(f\"üìä Before Optimization:\")\n",
    "print(f\"   Files: {files_before}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Avg file size: {size_mb/files_before:.2f} MB\")\n",
    "print(f\"\\n   Status: {'üî¥ Too many small files!' if files_before > 10 else 'üü¢ OK'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c423407-80b0-4f12-bb76-d71f325838e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run OPTIMIZE to compact files\n",
    "start = time.time()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "\"\"\")\n",
    "\n",
    "optimize_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ OPTIMIZE completed in {optimize_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3674a885-07ea-4805-8f75-537281c50b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check results after optimization\n",
    "detail_after = spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\").collect()[0]\n",
    "\n",
    "files_after = detail_after['numFiles']\n",
    "\n",
    "print(f\"üìä After Optimization:\")\n",
    "print(f\"   Files: {files_after}\")\n",
    "print(f\"   Reduction: {files_before - files_after} files removed\")\n",
    "print(f\"   Improvement: {files_before/files_after:.1f}x fewer files!\")\n",
    "print(f\"\\nüí° Queries now have much less file I/O overhead!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41880c1-8ae6-4102-9b38-9e2db2f5b739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 6: Table Optimization - Z-Ordering\n",
    "\n",
    "**Z-Ordering** organizes data so similar values are stored together.\n",
    "\n",
    "### How Data Skipping Works:\n",
    "\n",
    "**Without Z-Ordering:**\n",
    "```\n",
    "File 1: devices 1,5,10,15,20     <- Must read\n",
    "File 2: devices 2,3,8,12,19      <- Must read  \n",
    "File 3: devices 4,7,9,11,14      <- Must read\n",
    "```\n",
    "Query for device_id = 5 must read ALL files!\n",
    "\n",
    "**With Z-Ordering on device_id:**\n",
    "```\n",
    "File 1: devices 1,2,3,4,5        <- Read this\n",
    "File 2: devices 7,8,9,10,11      <- SKIP\n",
    "File 3: devices 12,14,15,19,20   <- SKIP\n",
    "```\n",
    "Query for device_id = 5 only reads File 1!\n",
    "\n",
    "### Choosing Z-Order Columns:\n",
    "\n",
    "‚úÖ **Good candidates:**\n",
    "- High cardinality (device_id, timestamp)\n",
    "- Frequently in WHERE clauses\n",
    "- Used in joins\n",
    "\n",
    "‚ùå **Bad candidates:**\n",
    "- Low cardinality (status: active/inactive)\n",
    "- Rarely filtered\n",
    "- Already partitioned by\n",
    "\n",
    "### Rule: 2-4 columns maximum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65209977-bf77-4f4e-8a74-7747351c5c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Benchmark query BEFORE Z-Ordering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c1d30f2-4f15-4aa4-bd4d-28205ddda724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "result_before_zorder = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    DATE(timestamp) as date,\n",
    "    AVG(temperature) as avg_temp,\n",
    "    AVG(rotation_speed) as avg_rotation,\n",
    "    MAX(air_pressure) as max_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "WHERE device_id IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "  AND timestamp >= CURRENT_DATE() - INTERVAL 30 DAYS\n",
    "GROUP BY device_id, DATE(timestamp)\n",
    "ORDER BY date DESC, device_id\n",
    "\"\"\")\n",
    "\n",
    "count_before = result_before_zorder.count()\n",
    "time_before_zorder = time.time() - start\n",
    "\n",
    "print(f\"‚è±Ô∏è  Query Time (Before Z-Order): {time_before_zorder:.2f} seconds\")\n",
    "print(f\"   Must scan all files for relevant devices\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee1e931b-b9b6-4236-a99d-d2b4df02b547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Common dashboard query: Hourly device metrics by factory\n",
    "# Without materialized view - runs every time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result_no_mv = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    f.factory_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    s.device_id,\n",
    "    DATE_TRUNC('hour', s.timestamp) as hour,\n",
    "    AVG(s.temperature) as avg_temp,\n",
    "    AVG(s.rotation_speed) as avg_rotation,\n",
    "    AVG(s.air_pressure) as avg_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {CATALOG}.{SCHEMA}.sensor_bronze s\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_factories f ON s.factory_id = f.factory_id\n",
    "WHERE s.timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "GROUP BY f.factory_id, f.factory_name, f.region, s.device_id, DATE_TRUNC('hour', s.timestamp)\n",
    "ORDER BY hour DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "display(result_no_mv)\n",
    "\n",
    "no_mv_time = time.time() - start\n",
    "print(f\"\\n‚è±Ô∏è  Query time (no materialized view): {no_mv_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a62c6fb-df22-4639-b10d-9da9b82150f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create materialized view for this common pattern\n",
    "spark.sql(f\"DROP MATERIALIZED VIEW IF EXISTS {CATALOG}.{SCHEMA}.mv_hourly_factory_metrics\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE MATERIALIZED VIEW {CATALOG}.{SCHEMA}.mv_hourly_factory_metrics\n",
    "AS\n",
    "SELECT \n",
    "    f.factory_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    s.device_id,\n",
    "    DATE_TRUNC('hour', s.timestamp) as hour,\n",
    "    AVG(s.temperature) as avg_temp,\n",
    "    AVG(s.rotation_speed) as avg_rotation,\n",
    "    AVG(s.air_pressure) as avg_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {CATALOG}.{SCHEMA}.sensor_bronze s\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_factories f ON s.factory_id = f.factory_id\n",
    "GROUP BY f.factory_id, f.factory_name, f.region, s.device_id, DATE_TRUNC('hour', s.timestamp)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created materialized view\")\n",
    "print(\"   This pre-computes the expensive join and aggregation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b5ed698-8148-4146-aa88-c197ff9241b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now query is MUCH faster - reads pre-computed results\n",
    "start = time.time()\n",
    "\n",
    "result_with_mv = spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {CATALOG}.{SCHEMA}.mv_hourly_factory_metrics\n",
    "WHERE hour >= current_date() - INTERVAL 7 DAYS\n",
    "ORDER BY hour DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "display(result_with_mv)\n",
    "\n",
    "mv_time = time.time() - start\n",
    "mv_speedup = no_mv_time / mv_time if mv_time > 0 else 0\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Query time (with materialized view): {mv_time:.2f} seconds\")\n",
    "print(f\"üöÄ Speedup: {mv_speedup:.1f}x faster!\")\n",
    "print(f\"\\nüí° Dashboard loads instantly instead of making users wait!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f0414d9-bf7a-4b2a-9648-54a4819366fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üéØ Materialized View Benefits:\n",
    "\n",
    "1. **Dashboard speed**: Instant load times\n",
    "2. **Cost savings**: Compute once, query many times\n",
    "3. **Automatic refresh**: Stays up to date\n",
    "4. **Query rewriting**: Optimizer uses it automatically\n",
    "\n",
    "**For your deadline:** This makes your real-time dashboard actually real-time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39df3eb3-006e-4f1d-9b8e-25a930306660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Caching Strategies <a id=\"caching\"></a>\n",
    "\n",
    "**Caching** keeps frequently accessed data in memory for instant access.\n",
    "\n",
    "### Types of Caching:\n",
    "\n",
    "1. **DataFrame Cache**: Temporary, session-specific\n",
    "2. **Delta Cache**: Disk-based, persists across queries\n",
    "3. **Result Cache**: Caches query results\n",
    "\n",
    "### When to Use Caching:\n",
    "\n",
    "‚úÖ Dimension tables (small, frequently joined)  \n",
    "‚úÖ Reference data  \n",
    "‚úÖ Iterative ML training  \n",
    "‚úÖ Dashboard data sources  \n",
    "\n",
    "‚ùå Don't cache:\n",
    "- Large fact tables (waste of memory)\n",
    "- Rarely accessed data\n",
    "- Data that changes frequently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca95077-7622-40f4-aa20-931ce8e4ee51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cache frequently used dimension tables\n",
    "# These are joined in almost every query!\n",
    "\n",
    "spark.sql(f\"CACHE TABLE {CATALOG}.{SCHEMA}.dim_factories\")\n",
    "spark.sql(f\"CACHE TABLE {CATALOG}.{SCHEMA}.dim_models\")\n",
    "spark.sql(f\"CACHE TABLE {CATALOG}.{SCHEMA}.dim_devices\")\n",
    "\n",
    "print(\"‚úÖ Cached dimension tables\")\n",
    "print(\"   Joins with these tables are now instant!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78d2d0f4-fe0d-4d6c-97eb-5d461c359968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test query with cached dimensions\n",
    "start = time.time()\n",
    "\n",
    "result_cached = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    m.model_name,\n",
    "    m.model_family,\n",
    "    d.device_id,\n",
    "    COUNT(DISTINCT s.trip_id) as trip_count,\n",
    "    AVG(s.temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.sensor_bronze s\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_devices d ON s.device_id = d.device_id\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_factories f ON d.factory_id = f.factory_id\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_models m ON d.model_id = m.model_id\n",
    "WHERE s.timestamp >= current_date() - INTERVAL 1 DAYS\n",
    "GROUP BY f.factory_name, f.region, m.model_name, m.model_family, d.device_id\n",
    "\"\"\")\n",
    "\n",
    "display(result_cached)\n",
    "\n",
    "cached_time = time.time() - start\n",
    "print(f\"\\n‚è±Ô∏è  Query time (with cached dimensions): {cached_time:.2f} seconds\")\n",
    "print(\"‚ú® Dimension joins are instant - no disk I/O needed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca44ed1f-9d58-437a-a451-73accfc9c60a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clear cache when done (frees memory)\n",
    "spark.sql(f\"UNCACHE TABLE IF EXISTS {CATALOG}.{SCHEMA}.dim_factories\")\n",
    "spark.sql(f\"UNCACHE TABLE IF EXISTS {CATALOG}.{SCHEMA}.dim_models\")\n",
    "spark.sql(f\"UNCACHE TABLE IF EXISTS {CATALOG}.{SCHEMA}.dim_devices\")\n",
    "\n",
    "print(\"‚úÖ Cleared caches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29129b48-4bf0-48cd-a286-86f22cfc4d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Caching Best Practices:\n",
    "\n",
    "1. **Cache small tables** that are joined frequently\n",
    "2. **Monitor memory** - don't cache everything\n",
    "3. **Clear caches** when not needed\n",
    "4. **Use Delta cache** on read-heavy clusters\n",
    "5. **Let Databricks auto-cache** query results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3393bfdf-2782-4ed0-82ff-8192ae9c2e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Performance Comparison Summary <a id=\"comparison\"></a>\n",
    "\n",
    "Let's summarize the performance improvements:\n",
    "\n",
    "### Optimization Results:\n",
    "\n",
    "| Technique | Typical Speedup | Setup Time | Maintenance |\n",
    "|-----------|----------------|------------|--------------|\n",
    "| **File Compaction** | 2-3x | 5 min | Weekly |\n",
    "| **Z-Ordering** | 3-10x | 10 min | Weekly |\n",
    "| **Liquid Clustering** | 3-10x | 15 min | Automatic |\n",
    "| **Materialized Views** | 5-20x | 15 min | Automatic |\n",
    "| **Caching** | 10-100x | 2 min | Per session |\n",
    "\n",
    "### Impact on Your Project:\n",
    "\n",
    "**Before Optimization:**\n",
    "- Dashboard: 10-15 seconds to load ‚ùå\n",
    "- Model training queries: 5 minutes ‚ùå\n",
    "- Ad-hoc analysis: 30+ seconds ‚ùå\n",
    "- Leadership impatient: Yes ‚ùå\n",
    "\n",
    "**After Optimization:**\n",
    "- Dashboard: <1 second ‚úÖ\n",
    "- Model training queries: 30 seconds ‚úÖ\n",
    "- Ad-hoc analysis: 3-5 seconds ‚úÖ\n",
    "- Leadership happy: Yes! ‚úÖ\n",
    "\n",
    "### Optimization Strategy:\n",
    "\n",
    "1. **Start with Z-Ordering** - Quick wins on existing tables\n",
    "2. **Add Materialized Views** - For dashboard queries\n",
    "3. **Cache dimensions** - Small tables used everywhere\n",
    "4. **Use Liquid Clustering** - For new production tables\n",
    "5. **Monitor and adjust** - Query history shows what needs work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b9748c-9947-4fad-b203-799980d268cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quick performance audit of your tables\n",
    "print(\"üìä Table Performance Audit\\n\")\n",
    "\n",
    "# Get table details from sensor tables\n",
    "tables_to_check = ['sensor_bronze', 'sensor_unoptimized', 'sensor_clustered']\n",
    "\n",
    "for table in tables_to_check:\n",
    "    try:\n",
    "        detail = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{SCHEMA}.{table}\").collect()[0]\n",
    "        num_files = detail['numFiles']\n",
    "        size_mb = detail['sizeInBytes'] / 1024 / 1024\n",
    "        \n",
    "        if num_files > 1000:\n",
    "            rec = 'üî¥ Too many files - run OPTIMIZE'\n",
    "        elif num_files > 100:\n",
    "            rec = 'üü° Consider OPTIMIZE'\n",
    "        else:\n",
    "            rec = 'üü¢ File count OK'\n",
    "        \n",
    "        print(f\"{table}:\")\n",
    "        print(f\"  Size: {size_mb:.2f} MB\")\n",
    "        print(f\"  Files: {num_files}\")\n",
    "        print(f\"  {rec}\\n\")\n",
    "    except:\n",
    "        print(f\"{table}: Table not found or error\\n\")\n",
    "\n",
    "print(\"üí° Use this audit to identify tables needing optimization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50d94b4b-ed3f-43ae-a692-4e2cf2d94be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Must-Do Optimizations:\n",
    "\n",
    "1. **Z-Order your fact tables** - `OPTIMIZE table ZORDER BY (common_filters)`\n",
    "2. **Create materialized views** - For repeated dashboard queries\n",
    "3. **Cache dimension tables** - Small, frequently joined tables\n",
    "4. **Monitor file count** - Run OPTIMIZE when >100 files\n",
    "5. **Use Liquid Clustering** - For new production tables\n",
    "\n",
    "### Performance Checklist:\n",
    "\n",
    "- [ ] Z-Ordered sensor_bronze on (device_id, timestamp)\n",
    "- [ ] Created materialized views for dashboard queries\n",
    "- [ ] Cached dimension tables\n",
    "- [ ] Compacted files (numFiles < 100)\n",
    "- [ ] Enabled Photon on SQL warehouse\n",
    "- [ ] Reviewed slow queries in Query History\n",
    "\n",
    "### For Your End-of-Week Demo:\n",
    "\n",
    "‚úÖ **Dashboards**: Sub-second response times  \n",
    "‚úÖ **ML models**: Fast training on optimized data  \n",
    "‚úÖ **Genie queries**: Instant results on materialized views  \n",
    "‚úÖ **Leadership**: Impressed with performance  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Try This Out\n",
    "\n",
    "### Challenge 1: Optimize Your Most Expensive Query\n",
    "\n",
    "1. Check SQL warehouse Query History\n",
    "2. Find the slowest query from yesterday\n",
    "3. Apply Z-Ordering or create materialized view\n",
    "4. Measure the speedup\n",
    "\n",
    "### Challenge 2: Create a Performance Dashboard\n",
    "\n",
    "Build a dashboard that tracks:\n",
    "- Table file counts\n",
    "- Query execution times\n",
    "- Cache hit rates\n",
    "- Top slow queries\n",
    "\n",
    "### Challenge 3: Optimize the Inspection Pipeline\n",
    "\n",
    "1. Z-Order `inspection_bronze` on (device_id, timestamp)\n",
    "2. Create materialized view for defect rate by model\n",
    "3. Compare query performance before/after\n",
    "\n",
    "### Challenge 4: Liquid Clustering Experiment\n",
    "\n",
    "1. Create a new table with Liquid Clustering\n",
    "2. Try different clustering keys\n",
    "3. Compare performance vs Z-Ordering\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply these techniques to your production tables\n",
    "- Set up monitoring to track query performance\n",
    "- Schedule weekly OPTIMIZE jobs\n",
    "- Educate team on performance best practices\n",
    "\n",
    "**Remember:** Fast queries = happy leadership = successful project! üéâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49263cfd-5e29-4ada-9b95-56daadc21062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to clean up demo tables\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.{USER}_sensor_clustered\")\n",
    "# spark.sql(f\"DROP MATERIALIZED VIEW IF EXISTS {CATALOG}.{SCHEMA}.{USER}_mv_hourly_metrics\")\n",
    "# print(\"‚úÖ Cleaned up demo tables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7e2f60-e20f-4c0b-9a65-3b440b86d91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç Query Plan Analysis\n",
    "\n",
    "Look for these indicators in the plan:\n",
    "- **FileScan**: How many files are scanned?\n",
    "- **Filter**: Pushed down to file scan (good) or after (bad)?\n",
    "- **Exchange**: Data shuffle between nodes (expensive)\n",
    "- **Data Skipping**: Are file statistics used?\n",
    "\n",
    "**Key Issue:** Without optimization, Databricks must scan ALL files even though we only need 5 devices!\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2 Performance Tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
