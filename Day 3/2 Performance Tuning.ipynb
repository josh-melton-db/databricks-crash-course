{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b033a0f-0a66-47c4-a00d-f4d1480138b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Performance Tuning: SQL & Table Optimization\n",
    "\n",
    "**The Situation:** Leadership wants dashboards, predictive models, and AI agents ready by Friday. Your plane IoT data is growing fast, and queries that worked yesterday are timing out today.\n",
    "\n",
    "**The Problem:** Slow queries = missed deadlines + angry leadership\n",
    "\n",
    "**The Solution:** Get familiar with both SQL and table optimization techniques to get sub-second query times.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn (30 minutes)\n",
    "\n",
    "‚úÖ **SQL Optimization:** Predicate pushdown, join strategies, broadcast hints  \n",
    "‚úÖ **Liquid Clustering:** Automatic data layout optimization  \n",
    "‚úÖ **Materialized Views:** Pre-compute expensive aggregations  \n",
    "‚úÖ **Deletion Vectors:** Fast updates and deletes  \n",
    "‚úÖ **Query Profile:** Analyze query execution on SQL Warehouses  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Day 1 & 2\n",
    "- `sensor_bronze`, `dim_factories`, `dim_devices` tables loaded\n",
    "- SQL Warehouse or cluster running\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Delta Lake Performance](https://docs.databricks.com/en/delta/tune-file-layout.html)\n",
    "- [Liquid Clustering](https://docs.databricks.com/en/delta/clustering.html)\n",
    "- [Query Optimization](https://docs.databricks.com/en/optimizations/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f91b22df-6941-4306-b382-ead819f9eef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THESE VALUES!\n",
    "CATALOG = \"your_catalog\"    # Update: Change to your catalog name\n",
    "SCHEMA = \"your_username\"    # Update: Use your username (without special characters)\n",
    "\n",
    "# Example: If your email is john.doe@company.com, use:\n",
    "# CATALOG = 'main' \n",
    "# SCHEMA = 'john_doe'\n",
    "\n",
    "print(f\"‚úÖ Using catalog: {CATALOG}\")\n",
    "print(f\"‚úÖ Using schema: {SCHEMA}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f7f8e9-fb02-41a1-bff9-ac1058cd0c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Understanding Performance Bottlenecks\n",
    "\n",
    "### Common Performance Killers\n",
    "\n",
    "| Problem | Impact | Solution |\n",
    "|---------|--------|----------|\n",
    "| üêå **Small Files** | Too many file opens | OPTIMIZE |\n",
    "| üêå **Full Table Scans** | Read entire table | Liquid Clustering, predicates |\n",
    "| üêå **Data Shuffle** | Network overhead | Broadcast joins |\n",
    "| üêå **Wrong Join Type** | Memory spills | Join hints |\n",
    "| üêå **Repeated Computation** | Wasted resources | Materialized views |\n",
    "| üêå **Inefficient Predicates** | No pushdown | Proper filters |\n",
    "\n",
    "### Performance Toolkit\n",
    "\n",
    "**SQL Optimization:**\n",
    "- Predicate pushdown (filter early)\n",
    "- Join hints (BROADCAST, SHUFFLE_HASH)\n",
    "- Proper WHERE clause design\n",
    "- Query Profile analysis\n",
    "\n",
    "**Table Optimization:**\n",
    "- File compaction (OPTIMIZE)\n",
    "- Liquid Clustering (automatic data layout optimization)\n",
    "- Deletion Vectors (fast updates)\n",
    "\n",
    "**Query Results:**\n",
    "- Materialized Views\n",
    "- Caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee60b8c9-0db9-4217-b714-fee0831f6b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Creating a \"Bad\" Table for Demonstration\n",
    "\n",
    "Let's intentionally create a poorly optimized table with:\n",
    "- Many small files (simulating streaming ingestion)\n",
    "- Random data layout (no locality)\n",
    "- No optimization\n",
    "\n",
    "This represents what happens in real production systems without proper maintenance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59859860-2901-4fac-8ae5-71b78111568a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create unoptimized table with random layout\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "AS\n",
    "SELECT \n",
    "    device_id,\n",
    "    trip_id,\n",
    "    factory_id,\n",
    "    model_id,\n",
    "    timestamp,\n",
    "    airflow_rate,\n",
    "    rotation_speed,\n",
    "    air_pressure,\n",
    "    temperature,\n",
    "    delay,\n",
    "    density\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_bronze\n",
    "ORDER BY RAND()  -- Random order = worst case for data locality!\n",
    "LIMIT 200000  -- Use subset for demo\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created unoptimized table with random layout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b17f70c-b1f4-42c4-8bed-361e7f7edb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Simulate many small files (like streaming writes)\n",
    "# This is what happens with continuous ingestion without auto-compaction\n",
    "\n",
    "for i in range(15):  # Create 15 small file batches\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "    SELECT \n",
    "        device_id,\n",
    "        trip_id,\n",
    "        factory_id,\n",
    "        model_id,\n",
    "        timestamp,\n",
    "        airflow_rate,\n",
    "        rotation_speed,\n",
    "        air_pressure,\n",
    "        temperature,\n",
    "        delay,\n",
    "        density\n",
    "    FROM {CATALOG}.{SCHEMA}.{USER}_sensor_bronze\n",
    "    WHERE MOD(device_id, 15) = {i}\n",
    "    LIMIT 800\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úÖ Created many small files (simulating poor ingestion patterns)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243da376-2fd9-4b59-90a6-b118a5247f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check table statistics - look at the file count!\n",
    "display(spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\", \"minReaderVersion\", \"minWriterVersion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e27f9af3-4eea-4129-98b9-315d45e9d91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç What to Look For:\n",
    "\n",
    "- **numFiles**: High number (hundreds of thousands or millions) = Performance problem!\n",
    "- **sizeInBytes**: Total size, but spread across too many files\n",
    "\n",
    "**Problem:** Every query must:\n",
    "1. List all files\n",
    "2. Open each file\n",
    "3. Read metadata\n",
    "4. Scan for relevant data\n",
    "\n",
    "With many small files, overhead dominates actual work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9e4a4c-7544-41fa-987a-4cc9e5535b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: SQL Optimization - Predicate Pushdown\n",
    "\n",
    "**Key Concept:** Push filters as close to the data as possible.\n",
    "\n",
    "### ‚ùå Bad: Function on Column (Prevents Pushdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54748cab-a12c-462f-826e-1ff81a1e921e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# BAD: Using SUBSTRING on timestamp prevents predicate pushdown\n",
    "start = time.time()\n",
    "\n",
    "result_bad = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "WHERE SUBSTRING(CAST(timestamp AS STRING), 1, 10) >= DATE_SUB(CURRENT_DATE(), 7)\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_bad = result_bad.count()\n",
    "time_bad = time.time() - start\n",
    "\n",
    "print(f\"‚ùå BAD Query Time: {time_bad:.2f} seconds\")\n",
    "print(f\"   Results: {count_bad} rows\")\n",
    "print(f\"   Problem: Function on column prevents statistics-based filtering!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e329a1-fef6-49ef-9c6f-aac56c5337e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# GOOD: Direct filter on timestamp column enables predicate pushdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10fec205-18b1-4509-8ab7-c7aa230079b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "result_good = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "WHERE timestamp >= CURRENT_DATE() - INTERVAL 7 DAYS\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_good = result_good.count()\n",
    "time_good = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ GOOD Query Time: {time_good:.2f} seconds\")\n",
    "print(f\"   Results: {count_good} rows\")\n",
    "print(f\"   Speedup: {time_bad/time_good:.1f}x faster!\")\n",
    "print(f\"   Reason: Databricks can use file statistics to skip irrelevant files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a805eaa-10b2-4282-beb0-7f6cd15d260e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Predicate Pushdown Best Practices\n",
    "\n",
    "**DO:**\n",
    "```sql\n",
    "WHERE timestamp >= '2024-01-01'  -- Direct column comparison\n",
    "WHERE device_id IN (1, 2, 3)     -- Direct value check\n",
    "WHERE factory_id = 'A06'          -- Equality on column\n",
    "```\n",
    "\n",
    "**DON'T:**\n",
    "```sql\n",
    "WHERE DATE(timestamp) = '2024-01-01'      -- Function prevents pushdown\n",
    "WHERE SUBSTRING(device_id, 1, 2) = '10'   -- Function on column\n",
    "WHERE UPPER(factory_id) = 'A06'           -- Transformation blocks optimization\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "107c6c3f-c146-4bab-8b7f-517158e5133d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: SQL Optimization - Join Strategies\n",
    "\n",
    "### Understanding Join Types\n",
    "\n",
    "| Join Type | Best For | Cost |\n",
    "|-----------|----------|------|\n",
    "| **Broadcast Join** | Small table (< 10MB) | Low - no shuffle |\n",
    "| **Shuffle Hash Join** | Large tables | High - shuffle both |\n",
    "| **Sort Merge Join** | Large sorted tables | Medium |\n",
    "\n",
    "### ‚ùå Bad: Let Spark guess (might shuffle large tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb6eb883-bf10-4272-b549-b16c6ee9fe8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Without hint - Spark might choose inefficient join strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d129f811-c074-4fd2-89c1-521ba297f36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "result_no_hint = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    s.device_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    COUNT(*) as reading_count,\n",
    "    AVG(s.temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized s\n",
    "JOIN {CATALOG}.{SCHEMA}.{USER}_dim_factories f \n",
    "  ON s.factory_id = f.factory_id\n",
    "GROUP BY s.device_id, f.factory_name, f.region\n",
    "\"\"\")\n",
    "\n",
    "count_no_hint = result_no_hint.count()\n",
    "time_no_hint = time.time() - start\n",
    "\n",
    "print(f\"‚ö†Ô∏è  No Hint Query Time: {time_no_hint:.2f} seconds\")\n",
    "print(f\"   Spark may shuffle both tables unnecessarily\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5a100ea-752e-40bb-971f-c65ae609c584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ Good: Broadcast Small Dimension Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd33e351-f9f5-4801-b2af-e8dce6939aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With BROADCAST hint - force efficient join strategy\n",
    "start = time.time()\n",
    "\n",
    "result_broadcast = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    /*+ BROADCAST(f) */\n",
    "    s.device_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    COUNT(*) as reading_count,\n",
    "    AVG(s.temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized s\n",
    "JOIN {CATALOG}.{SCHEMA}.{USER}_dim_factories f \n",
    "  ON s.factory_id = f.factory_id\n",
    "GROUP BY s.device_id, f.factory_name, f.region\n",
    "\"\"\")\n",
    "\n",
    "count_broadcast = result_broadcast.count()\n",
    "time_broadcast = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Broadcast Join Time: {time_broadcast:.2f} seconds\")\n",
    "print(f\"   Speedup: {time_no_hint/time_broadcast:.1f}x faster!\")\n",
    "print(f\"   Only dimension table sent to executors - no shuffle of fact table!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bde12c4-ee8e-4fe6-8a76-7bf997cd24be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Join Optimization Rules\n",
    "\n",
    "**BROADCAST when:**\n",
    "- Dimension table < 10MB\n",
    "- Reference data (factories, models, devices)\n",
    "- Lookup tables\n",
    "\n",
    "**Let Spark choose when:**\n",
    "- Both tables are large\n",
    "- Join cardinality is unknown\n",
    "- Adaptive Query Execution is enabled (default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc6fcd77-47b8-4494-ac3e-3f96df217b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5: Table Optimization - File Compaction\n",
    "\n",
    "**Problem:** Many small files cause overhead\n",
    "\n",
    "**Solution:** OPTIMIZE command compacts small files into larger ones\n",
    "\n",
    "**Target:** 128MB - 1GB per file (default: 1GB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceaa81d1-2861-44aa-a871-5718820fd0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check current file situation\n",
    "detail_before = spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "\n",
    "files_before = detail_before['numFiles']\n",
    "size_mb = detail_before['sizeInBytes'] / 1024 / 1024\n",
    "\n",
    "print(f\"üìä Before Optimization:\")\n",
    "print(f\"   Files: {files_before}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Avg file size: {size_mb/files_before:.2f} MB\")\n",
    "print(f\"\\n   Status: {'üî¥ Too many small files!' if files_before > 10 else 'üü¢ OK'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c423407-80b0-4f12-bb76-d71f325838e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run OPTIMIZE to compact files\n",
    "start = time.time()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "\"\"\")\n",
    "\n",
    "optimize_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ OPTIMIZE completed in {optimize_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3674a885-07ea-4805-8f75-537281c50b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check results after optimization\n",
    "detail_after = spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\").collect()[0]\n",
    "\n",
    "files_after = detail_after['numFiles']\n",
    "\n",
    "print(f\"üìä After Optimization:\")\n",
    "print(f\"   Files: {files_after}\")\n",
    "print(f\"   Reduction: {files_before - files_after} files removed\")\n",
    "print(f\"   Improvement: {files_before/files_after:.1f}x fewer files!\")\n",
    "print(f\"\\nüí° Queries now have much less file I/O overhead!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41880c1-8ae6-4102-9b38-9e2db2f5b739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 6: Table Optimization - Liquid Clustering\n",
    "\n",
    "**Liquid Clustering** is Delta Lake's automatic data layout optimization - the successor to Z-Ordering and partitioning.\n",
    "\n",
    "### Why Liquid Clustering?\n",
    "\n",
    "**Old approach (Z-Ordering):**\n",
    "- Manual OPTIMIZE commands required\n",
    "- Must choose columns upfront\n",
    "- Re-optimize needed when access patterns change\n",
    "- Separate from file compaction\n",
    "\n",
    "**New approach (Liquid Clustering):**\n",
    "- ‚úÖ Automatic optimization during writes\n",
    "- ‚úÖ Adapts to changing access patterns\n",
    "- ‚úÖ No manual maintenance required\n",
    "- ‚úÖ Combines compaction + data layout\n",
    "\n",
    "### How Data Skipping Works:\n",
    "\n",
    "**Without Clustering:**\n",
    "```\n",
    "File 1: devices 1,5,10,15,20     <- Must read\n",
    "File 2: devices 2,3,8,12,19      <- Must read  \n",
    "File 3: devices 4,7,9,11,14      <- Must read\n",
    "```\n",
    "Query for device_id = 5 must read ALL files!\n",
    "\n",
    "**With Liquid Clustering on device_id:**\n",
    "```\n",
    "File 1: devices 1,2,3,4,5        <- Read this (automatically organized!)\n",
    "File 2: devices 7,8,9,10,11      <- SKIP\n",
    "File 3: devices 12,14,15,19,20   <- SKIP\n",
    "```\n",
    "Query for device_id = 5 only reads File 1!\n",
    "\n",
    "### Choosing Clustering Columns:\n",
    "\n",
    "‚úÖ **Good candidates:**\n",
    "- High cardinality (device_id, timestamp)\n",
    "- Frequently in WHERE clauses\n",
    "- Used in joins\n",
    "- Common GROUP BY columns\n",
    "\n",
    "‚ùå **Bad candidates:**\n",
    "- Low cardinality (status: active/inactive)\n",
    "- Rarely filtered\n",
    "\n",
    "### Rule: 2-4 columns maximum, order matters (most selective first)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65209977-bf77-4f4e-8a74-7747351c5c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create a table WITH Liquid Clustering\n",
    "\n",
    "# First, let's create a clustered version of the unoptimized table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{SCHEMA}.sensor_clustered\n",
    "CLUSTER BY (device_id, timestamp)\n",
    "AS SELECT * FROM {CATALOG}.{SCHEMA}.sensor_bronze\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created table with Liquid Clustering on (device_id, timestamp)\")\n",
    "print(\"   - Automatically organizes data as it's written\")\n",
    "print(\"   - No manual OPTIMIZE needed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c1d30f2-4f15-4aa4-bd4d-28205ddda724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare query performance: Unclustered vs Clustered\n",
    "\n",
    "import time\n",
    "\n",
    "# Benchmark query on UNCLUSTERED table\n",
    "print(\"üîç Testing UNCLUSTERED table...\")\n",
    "start = time.time()\n",
    "\n",
    "result_unclustered = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    DATE(timestamp) as date,\n",
    "    AVG(temperature) as avg_temp,\n",
    "    AVG(rotation_speed) as avg_rotation,\n",
    "    MAX(air_pressure) as max_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {CATALOG}.{SCHEMA}.sensor_bronze\n",
    "WHERE device_id IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "  AND timestamp >= CURRENT_DATE() - INTERVAL 7 DAYS\n",
    "GROUP BY device_id, DATE(timestamp)\n",
    "ORDER BY date DESC, device_id\n",
    "\"\"\")\n",
    "\n",
    "count_unclustered = result_unclustered.count()\n",
    "time_unclustered = time.time() - start\n",
    "\n",
    "print(f\"‚è±Ô∏è  Query Time (Unclustered): {time_unclustered:.2f} seconds\")\n",
    "print(f\"   Must scan many files to find relevant devices\\n\")\n",
    "\n",
    "# Benchmark same query on CLUSTERED table\n",
    "print(\"üîç Testing CLUSTERED table...\")\n",
    "start = time.time()\n",
    "\n",
    "result_clustered = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    DATE(timestamp) as date,\n",
    "    AVG(temperature) as avg_temp,\n",
    "    AVG(rotation_speed) as avg_rotation,\n",
    "    MAX(air_pressure) as max_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {CATALOG}.{SCHEMA}.sensor_clustered\n",
    "WHERE device_id IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "  AND timestamp >= CURRENT_DATE() - INTERVAL 7 DAYS\n",
    "GROUP BY device_id, DATE(timestamp)\n",
    "ORDER BY date DESC, device_id\n",
    "\"\"\")\n",
    "\n",
    "count_clustered = result_clustered.count()\n",
    "time_clustered = time.time() - start\n",
    "\n",
    "print(f\"‚è±Ô∏è  Query Time (Clustered): {time_clustered:.2f} seconds\")\n",
    "print(f\"   Data skipping means fewer files to read\")\n",
    "\n",
    "# Calculate speedup\n",
    "if time_clustered > 0:\n",
    "    speedup = time_unclustered / time_clustered\n",
    "    print(f\"\\nüöÄ Performance Improvement: {speedup:.1f}x faster with Liquid Clustering!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee1e931b-b9b6-4236-a99d-d2b4df02b547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Common dashboard query: Hourly device metrics by factory\n",
    "# Without materialized view - runs every time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result_no_mv = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    f.factory_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    s.device_id,\n",
    "    DATE_TRUNC('hour', s.timestamp) as hour,\n",
    "    AVG(s.temperature) as avg_temp,\n",
    "    AVG(s.rotation_speed) as avg_rotation,\n",
    "    AVG(s.air_pressure) as avg_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {CATALOG}.{SCHEMA}.sensor_bronze s\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_factories f ON s.factory_id = f.factory_id\n",
    "WHERE s.timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "GROUP BY f.factory_id, f.factory_name, f.region, s.device_id, DATE_TRUNC('hour', s.timestamp)\n",
    "ORDER BY hour DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "display(result_no_mv)\n",
    "\n",
    "no_mv_time = time.time() - start\n",
    "print(f\"\\n‚è±Ô∏è  Query time (no materialized view): {no_mv_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a62c6fb-df22-4639-b10d-9da9b82150f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create materialized view for this common pattern\n",
    "spark.sql(f\"DROP MATERIALIZED VIEW IF EXISTS {CATALOG}.{SCHEMA}.mv_hourly_factory_metrics\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE MATERIALIZED VIEW {CATALOG}.{SCHEMA}.mv_hourly_factory_metrics\n",
    "AS\n",
    "SELECT \n",
    "    f.factory_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    s.device_id,\n",
    "    DATE_TRUNC('hour', s.timestamp) as hour,\n",
    "    AVG(s.temperature) as avg_temp,\n",
    "    AVG(s.rotation_speed) as avg_rotation,\n",
    "    AVG(s.air_pressure) as avg_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {CATALOG}.{SCHEMA}.sensor_bronze s\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_factories f ON s.factory_id = f.factory_id\n",
    "GROUP BY f.factory_id, f.factory_name, f.region, s.device_id, DATE_TRUNC('hour', s.timestamp)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created materialized view\")\n",
    "print(\"   This pre-computes the expensive join and aggregation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b5ed698-8148-4146-aa88-c197ff9241b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now query is MUCH faster - reads pre-computed results\n",
    "start = time.time()\n",
    "\n",
    "result_with_mv = spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {CATALOG}.{SCHEMA}.mv_hourly_factory_metrics\n",
    "WHERE hour >= current_date() - INTERVAL 7 DAYS\n",
    "ORDER BY hour DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "display(result_with_mv)\n",
    "\n",
    "mv_time = time.time() - start\n",
    "mv_speedup = no_mv_time / mv_time if mv_time > 0 else 0\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Query time (with materialized view): {mv_time:.2f} seconds\")\n",
    "print(f\"üöÄ Speedup: {mv_speedup:.1f}x faster!\")\n",
    "print(f\"\\nüí° Dashboard loads instantly instead of making users wait!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f0414d9-bf7a-4b2a-9648-54a4819366fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üéØ Materialized View Benefits:\n",
    "\n",
    "1. **Dashboard speed**: Instant load times\n",
    "2. **Cost savings**: Compute once, query many times\n",
    "3. **Automatic refresh**: Stays up to date\n",
    "4. **Query rewriting**: Optimizer uses it automatically\n",
    "\n",
    "**For your deadline:** This makes your real-time dashboard actually real-time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39df3eb3-006e-4f1d-9b8e-25a930306660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Caching Strategies <a id=\"caching\"></a>\n",
    "\n",
    "**Caching** keeps frequently accessed data in memory for instant access.\n",
    "\n",
    "### Types of Caching:\n",
    "\n",
    "1. **DataFrame Cache**: Temporary, session-specific\n",
    "2. **Delta Cache**: Disk-based, persists across queries\n",
    "3. **Result Cache**: Caches query results\n",
    "\n",
    "### When to Use Caching:\n",
    "\n",
    "‚úÖ Dimension tables (small, frequently joined)  \n",
    "‚úÖ Reference data  \n",
    "‚úÖ Iterative ML training  \n",
    "‚úÖ Dashboard data sources  \n",
    "\n",
    "‚ùå Don't cache:\n",
    "- Large fact tables (waste of memory)\n",
    "- Rarely accessed data\n",
    "- Data that changes frequently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca95077-7622-40f4-aa20-931ce8e4ee51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cache frequently used dimension tables\n",
    "# These are joined in almost every query!\n",
    "\n",
    "spark.sql(f\"CACHE TABLE {CATALOG}.{SCHEMA}.dim_factories\")\n",
    "spark.sql(f\"CACHE TABLE {CATALOG}.{SCHEMA}.dim_models\")\n",
    "spark.sql(f\"CACHE TABLE {CATALOG}.{SCHEMA}.dim_devices\")\n",
    "\n",
    "print(\"‚úÖ Cached dimension tables\")\n",
    "print(\"   Joins with these tables are now instant!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78d2d0f4-fe0d-4d6c-97eb-5d461c359968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test query with cached dimensions\n",
    "start = time.time()\n",
    "\n",
    "result_cached = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    m.model_name,\n",
    "    m.model_family,\n",
    "    d.device_id,\n",
    "    COUNT(DISTINCT s.trip_id) as trip_count,\n",
    "    AVG(s.temperature) as avg_temp\n",
    "FROM {CATALOG}.{SCHEMA}.sensor_bronze s\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_devices d ON s.device_id = d.device_id\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_factories f ON d.factory_id = f.factory_id\n",
    "JOIN {CATALOG}.{SCHEMA}.dim_models m ON d.model_id = m.model_id\n",
    "WHERE s.timestamp >= current_date() - INTERVAL 1 DAYS\n",
    "GROUP BY f.factory_name, f.region, m.model_name, m.model_family, d.device_id\n",
    "\"\"\")\n",
    "\n",
    "display(result_cached)\n",
    "\n",
    "cached_time = time.time() - start\n",
    "print(f\"\\n‚è±Ô∏è  Query time (with cached dimensions): {cached_time:.2f} seconds\")\n",
    "print(\"‚ú® Dimension joins are instant - no disk I/O needed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca44ed1f-9d58-437a-a451-73accfc9c60a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clear cache when done (frees memory)\n",
    "spark.sql(f\"UNCACHE TABLE IF EXISTS {CATALOG}.{SCHEMA}.dim_factories\")\n",
    "spark.sql(f\"UNCACHE TABLE IF EXISTS {CATALOG}.{SCHEMA}.dim_models\")\n",
    "spark.sql(f\"UNCACHE TABLE IF EXISTS {CATALOG}.{SCHEMA}.dim_devices\")\n",
    "\n",
    "print(\"‚úÖ Cleared caches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29129b48-4bf0-48cd-a286-86f22cfc4d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Caching Best Practices:\n",
    "\n",
    "1. **Cache small tables** that are joined frequently\n",
    "2. **Monitor memory** - don't cache everything\n",
    "3. **Clear caches** when not needed\n",
    "4. **Use Delta cache** on read-heavy clusters\n",
    "5. **Let Databricks auto-cache** query results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3393bfdf-2782-4ed0-82ff-8192ae9c2e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Using Query Profile on SQL Warehouses\n",
    "\n",
    "**Query Profile** is your best friend for diagnosing slow queries on SQL Warehouses.\n",
    "\n",
    "### What is Query Profile?\n",
    "\n",
    "Query Profile shows you **exactly** what your query is doing:\n",
    "- Which operations took the longest\n",
    "- How much data was read\n",
    "- Where shuffles happened\n",
    "- Memory spills\n",
    "\n",
    "### How to Access Query Profile:\n",
    "\n",
    "1. Run a query on a **SQL Warehouse** (not a cluster)\n",
    "2. After the query completes, click the **\"Query Profile\"** tab\n",
    "3. Explore the visual execution plan\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "| Problem in Profile | Meaning | Solution |\n",
    "|-------------------|---------|----------|\n",
    "| üî¥ **Large Scan** | Reading too much data | Add Liquid Clustering, better filters |\n",
    "| üî¥ **Shuffle** | Data moving between nodes | Use broadcast joins for small tables |\n",
    "| üî¥ **Spill to Disk** | Out of memory | Increase warehouse size or optimize query |\n",
    "| üî¥ **Many Tasks** | Too many small files | Run OPTIMIZE |\n",
    "\n",
    "### Example Workflow:\n",
    "\n",
    "```\n",
    "1. Query is slow (10+ seconds) ‚ùå\n",
    "2. Check Query Profile ‚Üí See \"Large Scan\"\n",
    "3. Add Liquid Clustering to table\n",
    "4. Re-run query ‚Üí 2 seconds ‚úÖ\n",
    "```\n",
    "\n",
    "**Learn more:** [Query Profile Documentation](https://docs.databricks.com/aws/en/sql/user/queries/query-profile)\n",
    "\n",
    "**üí° Pro Tip:** Query Profile only works on SQL Warehouses, not compute clusters. If you're running notebooks on a cluster, switch to a SQL Warehouse to use this feature.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Performance Comparison Summary <a id=\"comparison\"></a>\n",
    "\n",
    "Let's summarize the performance improvements:\n",
    "\n",
    "### Optimization Results:\n",
    "\n",
    "| Technique | Typical Speedup | Setup Time | Maintenance |\n",
    "|-----------|----------------|------------|--------------|\n",
    "| **File Compaction** | 2-3x | 5 min | As needed |\n",
    "| **Liquid Clustering** | 3-10x | 10 min | Automatic |\n",
    "| **Materialized Views** | 5-20x | 15 min | Automatic |\n",
    "| **Caching** | 10-100x | 2 min | Per session |\n",
    "\n",
    "### Impact on Your Project:\n",
    "\n",
    "**Before Optimization:**\n",
    "- Dashboard: 10-15 seconds to load ‚ùå\n",
    "- Model training queries: 5 minutes ‚ùå\n",
    "- Ad-hoc analysis: 30+ seconds ‚ùå\n",
    "- Leadership impatient: Yes ‚ùå\n",
    "\n",
    "**After Optimization:**\n",
    "- Dashboard: <1 second ‚úÖ\n",
    "- Model training queries: 30 seconds ‚úÖ\n",
    "- Ad-hoc analysis: 3-5 seconds ‚úÖ\n",
    "- Leadership happy: Yes! ‚úÖ\n",
    "\n",
    "### Optimization Strategy:\n",
    "\n",
    "1. **Use Liquid Clustering** - For all production tables\n",
    "2. **Add Materialized Views** - For repeated dashboard queries\n",
    "3. **Cache dimension tables** - Small tables used everywhere\n",
    "4. **Run OPTIMIZE** - When you have many small files\n",
    "5. **Use Query Profile** - Identify bottlenecks in slow queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b9748c-9947-4fad-b203-799980d268cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quick performance audit of your tables\n",
    "print(\"üìä Table Performance Audit\\n\")\n",
    "\n",
    "# Get table details from sensor tables\n",
    "tables_to_check = ['sensor_bronze', 'sensor_unoptimized', 'sensor_clustered']\n",
    "\n",
    "for table in tables_to_check:\n",
    "    try:\n",
    "        detail = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{SCHEMA}.{table}\").collect()[0]\n",
    "        num_files = detail['numFiles']\n",
    "        size_mb = detail['sizeInBytes'] / 1024 / 1024\n",
    "        \n",
    "        if num_files > 1000:\n",
    "            rec = 'üî¥ Too many files - run OPTIMIZE'\n",
    "        elif num_files > 100:\n",
    "            rec = 'üü° Consider OPTIMIZE'\n",
    "        else:\n",
    "            rec = 'üü¢ File count OK'\n",
    "        \n",
    "        print(f\"{table}:\")\n",
    "        print(f\"  Size: {size_mb:.2f} MB\")\n",
    "        print(f\"  Files: {num_files}\")\n",
    "        print(f\"  {rec}\\n\")\n",
    "    except:\n",
    "        print(f\"{table}: Table not found or error\\n\")\n",
    "\n",
    "print(\"üí° Use this audit to identify tables needing optimization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50d94b4b-ed3f-43ae-a692-4e2cf2d94be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Must-Do Optimizations:\n",
    "\n",
    "1. **Use Liquid Clustering** - `CREATE TABLE ... CLUSTER BY (col1, col2)`\n",
    "2. **Create materialized views** - For repeated dashboard queries\n",
    "3. **Cache dimension tables** - Small, frequently joined tables\n",
    "4. **Monitor file count** - Run OPTIMIZE when >100 files\n",
    "5. **Use Query Profile** - Analyze slow queries on SQL Warehouses\n",
    "\n",
    "### Performance Checklist:\n",
    "\n",
    "- [ ] Created tables with Liquid Clustering on (device_id, timestamp)\n",
    "- [ ] Created materialized views for dashboard queries\n",
    "- [ ] Cached dimension tables\n",
    "- [ ] Compacted files (numFiles < 100)\n",
    "- [ ] Enabled Photon on SQL warehouse\n",
    "- [ ] Used Query Profile to analyze slow queries\n",
    "\n",
    "### For Your End-of-Week Demo:\n",
    "\n",
    "‚úÖ **Dashboards**: Sub-second response times  \n",
    "‚úÖ **ML models**: Fast training on optimized data  \n",
    "‚úÖ **Genie queries**: Instant results on materialized views  \n",
    "‚úÖ **Leadership**: Impressed with performance  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Try This Out\n",
    "\n",
    "### Challenge 1: Optimize Your Most Expensive Query\n",
    "\n",
    "1. Check SQL warehouse Query History\n",
    "2. Find the slowest query from yesterday\n",
    "3. Use Query Profile to identify bottlenecks\n",
    "4. Apply Liquid Clustering or create materialized view\n",
    "5. Measure the speedup\n",
    "\n",
    "### Challenge 2: Create Clustered Tables\n",
    "\n",
    "Convert your existing tables to use Liquid Clustering:\n",
    "\n",
    "```sql\n",
    "-- Sensor data - cluster by device and time\n",
    "ALTER TABLE sensor_bronze CLUSTER BY (device_id, timestamp);\n",
    "\n",
    "-- Inspection data - cluster by device and time  \n",
    "ALTER TABLE inspection_bronze CLUSTER BY (device_id, timestamp);\n",
    "\n",
    "-- Run OPTIMIZE to apply clustering\n",
    "OPTIMIZE sensor_bronze;\n",
    "OPTIMIZE inspection_bronze;\n",
    "```\n",
    "\n",
    "### Challenge 3: Use Query Profile\n",
    "\n",
    "On a SQL Warehouse:\n",
    "1. Run a complex query\n",
    "2. Click the \"Query Profile\" tab\n",
    "3. Identify the slowest operation\n",
    "4. Look for:\n",
    "   - Full table scans ‚Üí add clustering\n",
    "   - Large shuffles ‚Üí add join hints\n",
    "   - Spills to disk ‚Üí increase warehouse size\n",
    "\n",
    "**Learn more:** [Query Profile on SQL Warehouses](https://docs.databricks.com/aws/en/sql/user/queries/query-profile)\n",
    "\n",
    "### Challenge 4: Optimize the Inspection Pipeline\n",
    "\n",
    "1. Add Liquid Clustering to `inspection_bronze` on (device_id, timestamp)\n",
    "2. Create materialized view for defect rate by model\n",
    "3. Compare query performance before/after\n",
    "\n",
    "### Challenge 5: Experiment with Different Clustering Keys\n",
    "\n",
    "1. Create test tables with different clustering strategies:\n",
    "   - `CLUSTER BY (device_id, timestamp)`\n",
    "   - `CLUSTER BY (factory_id, timestamp)`\n",
    "   - `CLUSTER BY (device_id, factory_id)`\n",
    "2. Run the same query on each\n",
    "3. Measure which performs best for your use case\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply these techniques to your production tables\n",
    "- Set up monitoring to track query performance\n",
    "- Schedule weekly OPTIMIZE jobs\n",
    "- Educate team on performance best practices\n",
    "\n",
    "**Remember:** Fast queries = happy leadership = successful project! üéâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49263cfd-5e29-4ada-9b95-56daadc21062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to clean up demo tables\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.{USER}_sensor_unoptimized\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.{USER}_sensor_clustered\")\n",
    "# spark.sql(f\"DROP MATERIALIZED VIEW IF EXISTS {CATALOG}.{SCHEMA}.{USER}_mv_hourly_metrics\")\n",
    "# print(\"‚úÖ Cleaned up demo tables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7e2f60-e20f-4c0b-9a65-3b440b86d91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç Query Plan Analysis\n",
    "\n",
    "Look for these indicators in the plan:\n",
    "- **FileScan**: How many files are scanned?\n",
    "- **Filter**: Pushed down to file scan (good) or after (bad)?\n",
    "- **Exchange**: Data shuffle between nodes (expensive)\n",
    "- **Data Skipping**: Are file statistics used?\n",
    "\n",
    "**Key Issue:** Without optimization, Databricks must scan ALL files even though we only need 5 devices!\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2 Performance Tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
