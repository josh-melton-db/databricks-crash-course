{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b033a0f-0a66-47c4-a00d-f4d1480138b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Performance Tuning: SQL & Table Optimization\n",
    "\n",
    "**The Situation:** Leadership wants dashboards, predictive models, and AI agents ready by Friday. Your plane IoT data is growing fast, and queries that worked yesterday are timing out today.\n",
    "\n",
    "**The Problem:** Slow queries = missed deadlines + angry leadership\n",
    "\n",
    "**The Solution:** Get familiar with both SQL and table optimization techniques to get sub-second query times.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "‚úÖ **SQL Optimization:** Predicate pushdown, join strategies, broadcast hints  \n",
    "‚úÖ **Liquid Clustering:** Automatic data layout optimization  \n",
    "‚úÖ **Materialized Views:** Pre-compute expensive aggregations  \n",
    "‚úÖ **Deletion Vectors:** Fast updates and deletes  \n",
    "‚úÖ **Query Profile:** Analyze query execution on SQL Warehouses  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Day 1 & 2\n",
    "- `sensor_bronze`, `dim_factories`, `dim_devices` tables loaded\n",
    "- SQL Warehouse or cluster running\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Delta Lake Performance](https://docs.databricks.com/en/delta/tune-file-layout.html)\n",
    "- [Liquid Clustering](https://docs.databricks.com/en/delta/clustering.html)\n",
    "- [Query Optimization](https://docs.databricks.com/en/optimizations/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91b22df-6941-4306-b382-ead819f9eef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import re\n",
    "\n",
    "catalog = \"dwx_airops_insights_platform_dev_working\"\n",
    "source_schema = \"db_crash_course\"  # Shared schema to read from\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username_base = username.split('@')[0]  # Extract username before @ symbol\n",
    "target_schema = re.sub(r'[^a-zA-Z0-9_]', '_', username_base)  # Replace special chars with _\n",
    "\n",
    "# Create target schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{target_schema}\")\n",
    "\n",
    "print(f\"‚úÖ Using catalog: {catalog}\")\n",
    "print(f\"üìñ Reading from schema: {source_schema} (shared)\")\n",
    "print(f\"‚úçÔ∏è  Writing to schema: {target_schema} (your personal schema)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f7f8e9-fb02-41a1-bff9-ac1058cd0c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Understanding Performance Bottlenecks\n",
    "\n",
    "### Common Performance Killers\n",
    "\n",
    "| Problem | Impact | Solution |\n",
    "|---------|--------|----------|\n",
    "| üêå **Small Files** | Too many file opens | OPTIMIZE |\n",
    "| üêå **Full Table Scans** | Read entire table | Liquid Clustering, predicates |\n",
    "| üêå **Data Shuffle** | Network overhead | Broadcast joins |\n",
    "| üêå **Wrong Join Type** | Memory spills | Join hints |\n",
    "| üêå **Repeated Computation** | Wasted resources | Materialized views |\n",
    "| üêå **Inefficient Predicates** | No pushdown | Proper filters |\n",
    "\n",
    "### Performance Toolkit\n",
    "\n",
    "**SQL Optimization:**\n",
    "- Predicate pushdown (filter early)\n",
    "- Join hints (BROADCAST, SHUFFLE_HASH)\n",
    "- Proper WHERE clause design\n",
    "- Query Profile analysis\n",
    "\n",
    "**Table Optimization:**\n",
    "- File compaction (OPTIMIZE)\n",
    "- Liquid Clustering (automatic data layout optimization)\n",
    "- Deletion Vectors (fast updates)\n",
    "\n",
    "**Query Results:**\n",
    "- Materialized Views\n",
    "- Caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee60b8c9-0db9-4217-b714-fee0831f6b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Creating a \"Bad\" Table for Demonstration\n",
    "\n",
    "Let's intentionally create a poorly optimized table with:\n",
    "- Many small files (simulating streaming ingestion)\n",
    "- Random data layout (no locality)\n",
    "- No optimization\n",
    "\n",
    "This represents what happens in real production systems without proper maintenance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59859860-2901-4fac-8ae5-71b78111568a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create unoptimized table with random layout in your target schema\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{target_schema}.sensor_unoptimized\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {catalog}.{target_schema}.sensor_unoptimized\n",
    "AS\n",
    "SELECT \n",
    "    device_id,\n",
    "    trip_id,\n",
    "    factory_id,\n",
    "    model_id,\n",
    "    timestamp,\n",
    "    airflow_rate,\n",
    "    rotation_speed,\n",
    "    air_pressure,\n",
    "    temperature,\n",
    "    delay,\n",
    "    density\n",
    "FROM {catalog}.{source_schema}.sensor_bronze\n",
    "ORDER BY RAND()  -- Random order = worst case for data locality!\n",
    "LIMIT 200000  -- Use subset for demo\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created unoptimized table with random layout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b17f70c-b1f4-42c4-8bed-361e7f7edb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Simulate many small files (like streaming writes)\n",
    "# This is what happens with continuous ingestion without auto-compaction\n",
    "\n",
    "for i in range(45):  # Create 15 small file batches\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {catalog}.{target_schema}.sensor_unoptimized\n",
    "    SELECT \n",
    "        device_id,\n",
    "        trip_id,\n",
    "        factory_id,\n",
    "        model_id,\n",
    "        timestamp,\n",
    "        airflow_rate,\n",
    "        rotation_speed,\n",
    "        air_pressure,\n",
    "        temperature,\n",
    "        delay,\n",
    "        density\n",
    "    FROM {catalog}.{source_schema}.sensor_bronze\n",
    "    WHERE MOD(device_id, 15) = {i}\n",
    "    LIMIT 800\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úÖ Created many small files (simulating poor ingestion patterns)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243da376-2fd9-4b59-90a6-b118a5247f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check table statistics - look at the file count!\n",
    "display(spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {catalog}.{target_schema}.sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\", \"minReaderVersion\", \"minWriterVersion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27f9af3-4eea-4129-98b9-315d45e9d91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç What to Look For:\n",
    "\n",
    "- **numFiles**: High number (hundreds of thousands or millions) = Performance problem!\n",
    "- **sizeInBytes**: Total size, but spread across too many files\n",
    "\n",
    "**Problem:** Every query must:\n",
    "1. List all files\n",
    "2. Open each file\n",
    "3. Read metadata\n",
    "4. Scan for relevant data\n",
    "\n",
    "With many small files, overhead dominates actual work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9e4a4c-7544-41fa-987a-4cc9e5535b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## Part 3: SQL Optimization - Predicate Pushdown\n\n**Key Concept:** Push filters as close to the data as possible.\n\n### Comparing Query Performance\n\nLet's compare two approaches to filtering - one that prevents optimization and one that enables it."
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54748cab-a12c-462f-826e-1ff81a1e921e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# BAD: Using SUBSTRING on timestamp prevents predicate pushdown\n",
    "start = time.time()\n",
    "\n",
    "result_bad = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {catalog}.{target_schema}.sensor_unoptimized\n",
    "WHERE SUBSTRING(CAST(timestamp AS STRING), 1, 10) >= DATE_SUB(CURRENT_DATE(), 7)\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_bad = result_bad.count()\n",
    "time_bad = time.time() - start\n",
    "\n",
    "print(f\"‚ùå BAD Query Time: {time_bad:.2f} seconds\")\n",
    "print(f\"   Results: {count_bad} rows\")\n",
    "print(f\"   Problem: Function on column prevents statistics-based filtering!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "#### ‚ùå Without Predicate Pushdown (Bad)\n\nUsing a function on the column prevents statistics-based filtering:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e329a1-fef6-49ef-9c6f-aac56c5337e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "#### ‚úÖ With Predicate Pushdown (Good)\n\nDirect filter on timestamp column enables predicate pushdown:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fec205-18b1-4509-8ab7-c7aa230079b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "### Best Practices for Predicate Pushdown\n\n**DO:**\n```sql\nWHERE timestamp >= '2024-01-01'  -- Direct column comparison\nWHERE device_id IN (1, 2, 3)     -- Direct value check\nWHERE factory_id = 'A06'          -- Equality on column\n```\n\n**DON'T:**\n```sql\nWHERE DATE(timestamp) = '2024-01-01'      -- Function prevents pushdown\nWHERE SUBSTRING(device_id, 1, 2) = '10'   -- Function on column\nWHERE UPPER(factory_id) = 'A06'           -- Transformation blocks optimization\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a805eaa-10b2-4282-beb0-7f6cd15d260e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## Part 4: SQL Optimization - Join Strategies\n\n### Understanding Join Types\n\n| Join Type | Best For | Cost |\n|-----------|----------|------|\n| **Broadcast Join** | Small table (< 10MB) | Low - no shuffle |\n| **Shuffle Hash Join** | Large tables | High - shuffle both |\n| **Sort Merge Join** | Large sorted tables | Medium |\n\n### Comparing Join Strategies\n\nLet's compare queries with and without join hints to see the performance impact."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107c6c3f-c146-4bab-8b7f-517158e5133d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "#### ‚ö†Ô∏è Without Hint - Spark Auto-Selects Join Strategy\n\nLet Spark choose the join strategy automatically:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d129f811-c074-4fd2-89c1-521ba297f36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "#### ‚úÖ With Broadcast Hint - Force Efficient Join\n\nUse the BROADCAST hint to force an efficient join strategy:"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a100ea-752e-40bb-971f-c65ae609c584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ Good: Broadcast Small Dimension Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd33e351-f9f5-4801-b2af-e8dce6939aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "### Best Practices for Join Optimization\n\n**BROADCAST when:**\n- Dimension table < 10MB\n- Reference data (factories, models, devices)\n- Lookup tables\n\n**Let Spark choose when:**\n- Both tables are large\n- Join cardinality is unknown\n- Adaptive Query Execution is enabled (default)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bde12c4-ee8e-4fe6-8a76-7bf997cd24be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Join Optimization Rules\n",
    "\n",
    "**BROADCAST when:**\n",
    "- Dimension table < 10MB\n",
    "- Reference data (factories, models, devices)\n",
    "- Lookup tables\n",
    "\n",
    "**Let Spark choose when:**\n",
    "- Both tables are large\n",
    "- Join cardinality is unknown\n",
    "- Adaptive Query Execution is enabled (default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6fcd77-47b8-4494-ac3e-3f96df217b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5: Table Optimization - File Compaction\n",
    "\n",
    "**Problem:** Many small files cause overhead\n",
    "\n",
    "**Solution:** OPTIMIZE command compacts small files into larger ones\n",
    "\n",
    "**Target:** 128MB - 1GB per file (default: 1GB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceaa81d1-2861-44aa-a871-5718820fd0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check current file situation\n",
    "detail_before = spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {catalog}.{target_schema}.sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "\n",
    "files_before = detail_before['numFiles']\n",
    "size_mb = detail_before['sizeInBytes'] / 1024 / 1024\n",
    "\n",
    "print(f\"üìä Before Optimization:\")\n",
    "print(f\"   Files: {files_before}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Avg file size: {size_mb/files_before:.2f} MB\")\n",
    "print(f\"\\n   Status: {'üî¥ Too many small files!' if files_before > 10 else 'üü¢ OK'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c423407-80b0-4f12-bb76-d71f325838e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run OPTIMIZE to compact files\n",
    "start = time.time()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {catalog}.{target_schema}.sensor_unoptimized\n",
    "\"\"\")\n",
    "\n",
    "optimize_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ OPTIMIZE completed in {optimize_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3674a885-07ea-4805-8f75-537281c50b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "## Part 6: Table Optimization - Liquid Clustering\n\n**Liquid Clustering** is Delta Lake's automatic data layout optimization - the successor to Z-Ordering and partitioning.\n\n### Why Liquid Clustering?\n\n**Old approach (Z-Ordering):**\n- Manual OPTIMIZE commands required\n- Must choose columns upfront\n- Re-optimize needed when access patterns change\n- Separate from file compaction\n\n**New approach (Liquid Clustering):**\n- ‚úÖ Automatic optimization during writes\n- ‚úÖ Adapts to changing access patterns\n- ‚úÖ No manual maintenance required\n- ‚úÖ Combines compaction + data layout\n\n### How Data Skipping Works\n\n**Without Clustering:**\n```\nFile 1: devices 1,5,10,15,20     <- Must read\nFile 2: devices 2,3,8,12,19      <- Must read  \nFile 3: devices 4,7,9,11,14      <- Must read\n```\nQuery for device_id = 5 must read ALL files!\n\n**With Liquid Clustering on device_id:**\n```\nFile 1: devices 1,2,3,4,5        <- Read this (automatically organized!)\nFile 2: devices 7,8,9,10,11      <- SKIP\nFile 3: devices 12,14,15,19,20   <- SKIP\n```\nQuery for device_id = 5 only reads File 1!\n\n### Choosing Clustering Columns\n\n‚úÖ **Good candidates:**\n- High cardinality (device_id, timestamp)\n- Frequently in WHERE clauses\n- Used in joins\n- Common GROUP BY columns\n\n‚ùå **Bad candidates:**\n- Low cardinality (status: active/inactive)\n- Rarely filtered\n\n**Rule:** 2-4 columns maximum, order matters (most selective first)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b41880c1-8ae6-4102-9b38-9e2db2f5b739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "### Creating a Clustered Table\n\nCreate a table with Liquid Clustering enabled:"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65209977-bf77-4f4e-8a74-7747351c5c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create a table WITH Liquid Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad163738-cdd3-4313-8f7b-15bbf43603fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{target_schema}\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbb9982-ba6d-4a54-8343-cde8f8e61fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, let's create a clustered version of the unoptimized table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{target_schema}.sensor_clustered\n",
    "CLUSTER BY (device_id, timestamp)\n",
    "AS SELECT * FROM {catalog}.{source_schema}.sensor_bronze\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created table with Liquid Clustering on (device_id, timestamp)\")\n",
    "print(\"   - Automatically organizes data as it's written\")\n",
    "print(\"   - No manual OPTIMIZE needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1d30f2-4f15-4aa4-bd4d-28205ddda724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare query performance: Unclustered vs Clustered\n",
    "\n",
    "import time\n",
    "\n",
    "# Benchmark query on UNCLUSTERED table\n",
    "print(\"üîç Testing UNCLUSTERED table...\")\n",
    "start = time.time()\n",
    "\n",
    "result_unclustered = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    DATE(timestamp) as date,\n",
    "    AVG(temperature) as avg_temp,\n",
    "    AVG(rotation_speed) as avg_rotation,\n",
    "    MAX(air_pressure) as max_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {catalog}.{source_schema}.sensor_bronze\n",
    "WHERE device_id IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "  AND timestamp >= CURRENT_DATE() - INTERVAL 7 DAYS\n",
    "GROUP BY device_id, DATE(timestamp)\n",
    "ORDER BY date DESC, device_id\n",
    "\"\"\")\n",
    "\n",
    "count_unclustered = result_unclustered.count()\n",
    "time_unclustered = time.time() - start\n",
    "\n",
    "print(f\"‚è±Ô∏è  Query Time (Unclustered): {time_unclustered:.2f} seconds\")\n",
    "print(f\"   Must scan many files to find relevant devices\\n\")\n",
    "\n",
    "# Benchmark same query on CLUSTERED table\n",
    "print(\"üîç Testing CLUSTERED table...\")\n",
    "start = time.time()\n",
    "\n",
    "result_clustered = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    DATE(timestamp) as date,\n",
    "    AVG(temperature) as avg_temp,\n",
    "    AVG(rotation_speed) as avg_rotation,\n",
    "    MAX(air_pressure) as max_pressure,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {catalog}.{target_schema}.sensor_clustered\n",
    "WHERE device_id IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "  AND timestamp >= CURRENT_DATE() - INTERVAL 7 DAYS\n",
    "GROUP BY device_id, DATE(timestamp)\n",
    "ORDER BY date DESC, device_id\n",
    "\"\"\")\n",
    "\n",
    "count_clustered = result_clustered.count()\n",
    "time_clustered = time.time() - start\n",
    "\n",
    "print(f\"‚è±Ô∏è  Query Time (Clustered): {time_clustered:.2f} seconds\")\n",
    "print(f\"   Data skipping means fewer files to read\")\n",
    "\n",
    "# Calculate speedup\n",
    "if time_clustered > 0:\n",
    "    speedup = time_unclustered / time_clustered\n",
    "    print(f\"\\nüöÄ Performance Improvement: {speedup:.1f}x faster with Liquid Clustering!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0414d9-bf7a-4b2a-9648-54a4819366fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## Part 7: Caching Strategies\n\n**Caching** keeps frequently accessed data in memory for instant access.\n\n### Types of Caching\n\n1. **DataFrame Cache**: Temporary, session-specific\n2. **Delta Cache**: Disk-based, persists across queries\n3. **Result Cache**: Caches query results\n\n### When to Use Caching\n\n‚úÖ **Cache these:**\n- Dimension tables (small, frequently joined)  \n- Reference data  \n- Iterative ML training  \n- Dashboard data sources  \n\n‚ùå **Don't cache:**\n- Large fact tables (waste of memory)\n- Rarely accessed data\n- Data that changes frequently"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39df3eb3-006e-4f1d-9b8e-25a930306660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Caching Strategies <a id=\"caching\"></a>\n",
    "\n",
    "**Caching** keeps frequently accessed data in memory for instant access.\n",
    "\n",
    "### Types of Caching:\n",
    "\n",
    "1. **DataFrame Cache**: Temporary, session-specific\n",
    "2. **Delta Cache**: Disk-based, persists across queries\n",
    "3. **Result Cache**: Caches query results\n",
    "\n",
    "### When to Use Caching:\n",
    "\n",
    "‚úÖ Dimension tables (small, frequently joined)  \n",
    "‚úÖ Reference data  \n",
    "‚úÖ Iterative ML training  \n",
    "‚úÖ Dashboard data sources  \n",
    "\n",
    "‚ùå Don't cache:\n",
    "- Large fact tables (waste of memory)\n",
    "- Rarely accessed data\n",
    "- Data that changes frequently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca95077-7622-40f4-aa20-931ce8e4ee51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cache frequently used dimension tables\n",
    "# These are joined in almost every query!\n",
    "\n",
    "spark.sql(f\"CACHE TABLE {catalog}.{source_schema}.dim_factories\")\n",
    "spark.sql(f\"CACHE TABLE {catalog}.{source_schema}.dim_models\")\n",
    "spark.sql(f\"CACHE TABLE {catalog}.{source_schema}.dim_devices\")\n",
    "\n",
    "print(\"‚úÖ Cached dimension tables\")\n",
    "print(\"   Joins with these tables are now instant!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d2d0f4-fe0d-4d6c-97eb-5d461c359968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test query with cached dimensions\n",
    "start = time.time()\n",
    "\n",
    "result_cached = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    m.model_name,\n",
    "    m.model_family,\n",
    "    d.device_id,\n",
    "    COUNT(DISTINCT s.trip_id) as trip_count,\n",
    "    AVG(s.temperature) as avg_temp\n",
    "FROM {catalog}.{source_schema}.sensor_bronze s\n",
    "JOIN {catalog}.{source_schema}.dim_devices d ON s.device_id = d.device_id\n",
    "JOIN {catalog}.{source_schema}.dim_factories f ON d.factory_id = f.factory_id\n",
    "JOIN {catalog}.{source_schema}.dim_models m ON d.model_id = m.model_id\n",
    "WHERE s.timestamp >= current_date() - INTERVAL 1 DAYS\n",
    "GROUP BY f.factory_name, f.region, m.model_name, m.model_family, d.device_id\n",
    "\"\"\")\n",
    "\n",
    "display(result_cached)\n",
    "\n",
    "cached_time = time.time() - start\n",
    "print(f\"\\n‚è±Ô∏è  Query time (with cached dimensions): {cached_time:.2f} seconds\")\n",
    "print(\"‚ú® Dimension joins are instant - no disk I/O needed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca44ed1f-9d58-437a-a451-73accfc9c60a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "### Best Practices for Caching\n\n1. **Cache small tables** that are joined frequently\n2. **Monitor memory** - don't cache everything\n3. **Clear caches** when not needed\n4. **Use Delta cache** on read-heavy clusters\n5. **Let Databricks auto-cache** query results"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29129b48-4bf0-48cd-a286-86f22cfc4d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "## Part 8: Using Query Profile on SQL Warehouses\n\n**Query Profile** is your best friend for diagnosing slow queries on SQL Warehouses.\n\n### What is Query Profile?\n\nQuery Profile shows you **exactly** what your query is doing:\n- Which operations took the longest\n- How much data was read\n- Where shuffles happened\n- Memory spills\n\n### How to Access Query Profile\n\n1. Run a query on a **SQL Warehouse** (not a cluster)\n2. After the query completes, click the **\"Query Profile\"** tab\n3. Explore the visual execution plan\n\n### What to Look For\n\n| Problem in Profile | Meaning | Solution |\n|-------------------|---------|----------|\n| üî¥ **Large Scan** | Reading too much data | Add Liquid Clustering, better filters |\n| üî¥ **Shuffle** | Data moving between nodes | Use broadcast joins for small tables |\n| üî¥ **Spill to Disk** | Out of memory | Increase warehouse size or optimize query |\n| üî¥ **Many Tasks** | Too many small files | Run OPTIMIZE |\n\n### Example Workflow\n\n```\n1. Query is slow (10+ seconds) ‚ùå\n2. Check Query Profile ‚Üí See \"Large Scan\"\n3. Add Liquid Clustering to table\n4. Re-run query ‚Üí 2 seconds ‚úÖ\n```\n\n**Learn more:** [Query Profile Documentation](https://docs.databricks.com/aws/en/sql/user/queries/query-profile)\n\n**üí° Pro Tip:** Query Profile only works on SQL Warehouses, not compute clusters. If you're running notebooks on a cluster, switch to a SQL Warehouse to use this feature."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3393bfdf-2782-4ed0-82ff-8192ae9c2e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Using Query Profile on SQL Warehouses\n",
    "\n",
    "**Query Profile** is your best friend for diagnosing slow queries on SQL Warehouses.\n",
    "\n",
    "### What is Query Profile?\n",
    "\n",
    "Query Profile shows you **exactly** what your query is doing:\n",
    "- Which operations took the longest\n",
    "- How much data was read\n",
    "- Where shuffles happened\n",
    "- Memory spills\n",
    "\n",
    "### How to Access Query Profile:\n",
    "\n",
    "1. Run a query on a **SQL Warehouse** (not a cluster)\n",
    "2. After the query completes, click the **\"Query Profile\"** tab\n",
    "3. Explore the visual execution plan\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "| Problem in Profile | Meaning | Solution |\n",
    "|-------------------|---------|----------|\n",
    "| üî¥ **Large Scan** | Reading too much data | Add Liquid Clustering, better filters |\n",
    "| üî¥ **Shuffle** | Data moving between nodes | Use broadcast joins for small tables |\n",
    "| üî¥ **Spill to Disk** | Out of memory | Increase warehouse size or optimize query |\n",
    "| üî¥ **Many Tasks** | Too many small files | Run OPTIMIZE |\n",
    "\n",
    "### Example Workflow:\n",
    "\n",
    "```\n",
    "1. Query is slow (10+ seconds) ‚ùå\n",
    "2. Check Query Profile ‚Üí See \"Large Scan\"\n",
    "3. Add Liquid Clustering to table\n",
    "4. Re-run query ‚Üí 2 seconds ‚úÖ\n",
    "```\n",
    "\n",
    "**Learn more:** [Query Profile Documentation](https://docs.databricks.com/aws/en/sql/user/queries/query-profile)\n",
    "\n",
    "**üí° Pro Tip:** Query Profile only works on SQL Warehouses, not compute clusters. If you're running notebooks on a cluster, switch to a SQL Warehouse to use this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7e2f60-e20f-4c0b-9a65-3b440b86d91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç Query Plan Analysis\n",
    "\n",
    "Look for these indicators in the plan:\n",
    "- **FileScan**: How many files are scanned?\n",
    "- **Filter**: Pushed down to file scan (good) or after (bad)?\n",
    "- **Exchange**: Data shuffle between nodes (expensive)\n",
    "- **Data Skipping**: Are file statistics used?\n",
    "\n",
    "**Key Issue:** Without optimization, Databricks must scan ALL files even though we only need 5 devices!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d94b4b-ed3f-43ae-a692-4e2cf2d94be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Must-Do Optimizations:\n",
    "\n",
    "1. **Use Liquid Clustering** - `CREATE TABLE ... CLUSTER BY (col1, col2)`\n",
    "2. **Create materialized views** - For repeated dashboard queries\n",
    "3. **Cache dimension tables** - Small, frequently joined tables\n",
    "4. **Monitor file count** - Run OPTIMIZE when >100 files\n",
    "5. **Use Query Profile** - Analyze slow queries on SQL Warehouses\n",
    "\n",
    "### Performance Checklist:\n",
    "\n",
    "- [ ] Created tables with Liquid Clustering on (device_id, timestamp)\n",
    "- [ ] Created materialized views for dashboard queries\n",
    "- [ ] Cached dimension tables\n",
    "- [ ] Compacted files (numFiles < 100)\n",
    "- [ ] Enabled Photon on SQL warehouse\n",
    "- [ ] Used Query Profile to analyze slow queries\n",
    "\n",
    "### For Your End-of-Week Demo:\n",
    "\n",
    "‚úÖ **Dashboards**: Sub-second response times  \n",
    "‚úÖ **ML models**: Fast training on optimized data  \n",
    "‚úÖ **Genie queries**: Instant results on materialized views  \n",
    "‚úÖ **Leadership**: Impressed with performance  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Try This Out\n",
    "\n",
    "### Challenge 1: Optimize Your Most Expensive Query\n",
    "\n",
    "1. Check SQL warehouse Query History\n",
    "2. Find the slowest query from yesterday\n",
    "3. Use Query Profile to identify bottlenecks\n",
    "4. Apply Liquid Clustering or create materialized view\n",
    "5. Measure the speedup\n",
    "\n",
    "### Challenge 2: Create Clustered Tables\n",
    "\n",
    "Convert your existing tables to use Liquid Clustering:\n",
    "\n",
    "```sql\n",
    "-- Sensor data - cluster by device and time\n",
    "ALTER TABLE sensor_bronze CLUSTER BY (device_id, timestamp);\n",
    "\n",
    "-- Inspection data - cluster by device and time  \n",
    "ALTER TABLE inspection_bronze CLUSTER BY (device_id, timestamp);\n",
    "\n",
    "-- Run OPTIMIZE to apply clustering\n",
    "OPTIMIZE sensor_bronze;\n",
    "OPTIMIZE inspection_bronze;\n",
    "```\n",
    "\n",
    "### Challenge 3: Use Query Profile\n",
    "\n",
    "On a SQL Warehouse:\n",
    "1. Run a complex query\n",
    "2. Click the \"Query Profile\" tab\n",
    "3. Identify the slowest operation\n",
    "4. Look for:\n",
    "   - Full table scans ‚Üí add clustering\n",
    "   - Large shuffles ‚Üí add join hints\n",
    "   - Spills to disk ‚Üí increase warehouse size\n",
    "\n",
    "**Learn more:** [Query Profile on SQL Warehouses](https://docs.databricks.com/aws/en/sql/user/queries/query-profile)\n",
    "\n",
    "### Challenge 4: Optimize the Inspection Pipeline\n",
    "\n",
    "1. Add Liquid Clustering to `inspection_bronze` on (device_id, timestamp)\n",
    "2. Create materialized view for defect rate by model\n",
    "3. Compare query performance before/after\n",
    "\n",
    "### Challenge 5: Experiment with Different Clustering Keys\n",
    "\n",
    "1. Create test tables with different clustering strategies:\n",
    "   - `CLUSTER BY (device_id, timestamp)`\n",
    "   - `CLUSTER BY (factory_id, timestamp)`\n",
    "   - `CLUSTER BY (device_id, factory_id)`\n",
    "2. Run the same query on each\n",
    "3. Measure which performs best for your use case\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply these techniques to your production tables\n",
    "- Set up monitoring to track query performance\n",
    "- Schedule weekly OPTIMIZE jobs\n",
    "- Educate team on performance best practices\n",
    "\n",
    "**Remember:** Fast queries = happy leadership = successful project! üéâ\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2 Performance Tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}