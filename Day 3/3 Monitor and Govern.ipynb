{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55d67db6-c460-482c-924f-1c2bc1e6320e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monitor and Govern Databricks Workspaces\n",
    "\n",
    "Use system tables to monitor usage, costs, and implement governance with Unity Catalog.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "âœ… Query system tables for observability  \n",
    "âœ… Analyze billing and cost allocation  \n",
    "âœ… Monitor workspace usage and performance  \n",
    "âœ… Implement Unity Catalog security  \n",
    "âœ… Create governance dashboards  \n",
    "\n",
    "**Note**: Since students won't have access to actual system tables, we'll use synthetic data that matches the schema.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [System Tables](https://docs.databricks.com/aws/en/admin/system-tables/)\n",
    "- [Billing Tables](https://docs.databricks.com/aws/en/admin/system-tables/billing)\n",
    "- [Unity Catalog Governance](https://docs.databricks.com/aws/en/data-governance/unity-catalog/)\n",
    "- [Observability Dashboards](https://github.com/CodyAustinDavis/dbsql_sme/tree/main/Observability%20Dashboards%20and%20DBA%20Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9876ae4c-1969-4504-9449-555b8819a822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. System Tables Overview\n",
    "\n",
    "### What are System Tables?\n",
    "\n",
    "**System Tables** provide observability into:\n",
    "- Billing and usage\n",
    "- Query execution\n",
    "- Warehouse performance\n",
    "- Audit logs\n",
    "- Lineage information\n",
    "\n",
    "### Available Schemas\n",
    "\n",
    "```\n",
    "system.billing.*        - Cost and usage data\n",
    "system.compute.*        - Cluster and warehouse metrics\n",
    "system.query.*          - Query execution logs\n",
    "system.audit.*          - Audit logs\n",
    "system.lineage.*        - Data lineage\n",
    "```\n",
    "\n",
    "### Access Requirements\n",
    "\n",
    "**In Production:**\n",
    "- Account admin privileges\n",
    "- Unity Catalog enabled\n",
    "- System tables schema access\n",
    "\n",
    "**In This Training:**\n",
    "- We'll use synthetic data with matching schemas\n",
    "- Demonstrates real-world queries and patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Billing and Cost Analysis\n",
    "\n",
    "### Synthetic Billing Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118a7867-e5f7-4a90-8fe7-1e14d627d747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create synthetic system tables for training\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# First, ensure the training schema exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS training\")\n",
    "\n",
    "# ========================================\n",
    "# 1. BILLING DATA\n",
    "# ========================================\n",
    "\n",
    "# Generate sample billing records\n",
    "dates = [(datetime.now() - timedelta(days=x)).strftime('%Y-%m-%d') for x in range(30)]\n",
    "workspaces = ['prod-workspace', 'dev-workspace', 'staging-workspace']\n",
    "sku_names = ['JOBS_COMPUTE', 'ALL_PURPOSE_COMPUTE', 'SQL_COMPUTE', 'DELTA_LIVE_TABLES']\n",
    "users = ['user1@company.com', 'user2@company.com', 'user3@company.com', 'system']\n",
    "\n",
    "billing_data = []\n",
    "for date in dates:\n",
    "    for _ in range(50):\n",
    "        billing_data.append({\n",
    "            'usage_date': date,\n",
    "            'workspace_id': random.choice(workspaces),\n",
    "            'sku_name': random.choice(sku_names),\n",
    "            'usage_quantity': round(random.uniform(0.1, 10.0), 2),\n",
    "            'usage_unit': 'DBU',\n",
    "            'list_price': round(random.uniform(0.1, 2.0), 2),\n",
    "            'usage_metadata': {\n",
    "                'job_id': f'job_{random.randint(1, 100)}',\n",
    "                'cluster_id': f'cluster_{random.randint(1, 20)}',\n",
    "                'user': random.choice(users)\n",
    "            }\n",
    "        })\n",
    "\n",
    "billing_df = spark.createDataFrame(billing_data)\n",
    "billing_df.write.mode('overwrite').saveAsTable('training.system_billing')\n",
    "\n",
    "print('âœ… Synthetic billing data created')\n",
    "\n",
    "# ========================================\n",
    "# 2. QUERY HISTORY DATA\n",
    "# ========================================\n",
    "\n",
    "# Sample query texts related to our IoT dataset\n",
    "query_texts = [\n",
    "    'SELECT * FROM sensor_bronze WHERE factory_id = \"A06\"',\n",
    "    'SELECT device_id, AVG(temperature) FROM sensor_bronze GROUP BY device_id',\n",
    "    'SELECT * FROM inspection_gold ORDER BY timestamp DESC',\n",
    "    'SELECT COUNT(*) FROM anomaly_detected',\n",
    "    'INSERT INTO sensor_bronze SELECT * FROM sensor_data',\n",
    "    'OPTIMIZE sensor_bronze ZORDER BY (device_id)',\n",
    "    'SELECT f.factory_name, COUNT(*) FROM sensor_bronze s JOIN dim_factories f',\n",
    "    'CREATE TABLE sensor_aggregates AS SELECT device_id, AVG(temp)',\n",
    "    'SELECT * FROM dim_devices WHERE status = \"Active\"',\n",
    "    'UPDATE dim_devices SET status = \"Maintenance\" WHERE device_id = 123',\n",
    "]\n",
    "\n",
    "warehouses = ['iot-analytics-warehouse', 'prod-sql-warehouse', 'dev-warehouse']\n",
    "query_types = ['SELECT', 'INSERT', 'UPDATE', 'CREATE', 'OPTIMIZE']\n",
    "\n",
    "query_history_data = []\n",
    "for i in range(500):\n",
    "    query_start_time = datetime.now() - timedelta(days=random.randint(0, 7), \n",
    "                                                    hours=random.randint(0, 23),\n",
    "                                                    minutes=random.randint(0, 59))\n",
    "    execution_time_ms = random.randint(100, 30000)\n",
    "    \n",
    "    query_history_data.append({\n",
    "        'query_id': f'query_{i}',\n",
    "        'query_text': random.choice(query_texts),\n",
    "        'query_start_time': query_start_time,\n",
    "        'execution_time_ms': execution_time_ms,\n",
    "        'rows_produced': random.randint(0, 100000),\n",
    "        'bytes_scanned': random.randint(1000, 10000000),\n",
    "        'compute_cost': round(random.uniform(0.01, 5.0), 3),\n",
    "        'warehouse_id': random.choice(warehouses),\n",
    "        'user_email': random.choice(users),\n",
    "        'query_type': random.choice(query_types),\n",
    "        'status': random.choice(['FINISHED', 'FINISHED', 'FINISHED', 'FAILED'])\n",
    "    })\n",
    "\n",
    "query_history_df = spark.createDataFrame(query_history_data)\n",
    "query_history_df.write.mode('overwrite').saveAsTable('training.query_history')\n",
    "\n",
    "print('âœ… Synthetic query history created')\n",
    "\n",
    "# ========================================\n",
    "# 3. USER PERMISSIONS DATA\n",
    "# ========================================\n",
    "\n",
    "# Create user permissions table for row-level security example\n",
    "user_permissions_data = [\n",
    "    {'user_email': 'user1@company.com', 'region': 'North Region', 'role': 'analyst'},\n",
    "    {'user_email': 'user2@company.com', 'region': 'East Region', 'role': 'analyst'},\n",
    "    {'user_email': 'user3@company.com', 'region': 'South Region', 'role': 'analyst'},\n",
    "    {'user_email': 'admin@company.com', 'region': 'All', 'role': 'admin'},\n",
    "    {'user_email': 'system', 'region': 'All', 'role': 'system'},\n",
    "]\n",
    "\n",
    "user_permissions_df = spark.createDataFrame(user_permissions_data)\n",
    "user_permissions_df.write.mode('overwrite').saveAsTable('training.user_permissions')\n",
    "\n",
    "print('âœ… Synthetic user permissions created')\n",
    "\n",
    "# ========================================\n",
    "# 4. AUDIT LOG DATA\n",
    "# ========================================\n",
    "\n",
    "# Sample audit logs\n",
    "table_names = ['sensor_bronze', 'inspection_bronze', 'anomaly_detected', \n",
    "               'inspection_gold', 'dim_devices', 'dim_factories']\n",
    "action_names = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE_TABLE', 'DROP_TABLE']\n",
    "\n",
    "audit_data = []\n",
    "for i in range(300):\n",
    "    event_time = datetime.now() - timedelta(days=random.randint(0, 7),\n",
    "                                            hours=random.randint(0, 23),\n",
    "                                            minutes=random.randint(0, 59))\n",
    "    \n",
    "    audit_data.append({\n",
    "        'event_id': f'event_{i}',\n",
    "        'event_time': event_time,\n",
    "        'event_date': event_time.date(),\n",
    "        'user_email': random.choice(users),\n",
    "        'action_name': random.choice(action_names),\n",
    "        'table_full_name': f'default.training.{random.choice(table_names)}',\n",
    "        'workspace_id': random.choice(workspaces),\n",
    "        'request_id': f'req_{i}',\n",
    "        'source_ip': f'10.0.{random.randint(1, 255)}.{random.randint(1, 255)}',\n",
    "    })\n",
    "\n",
    "audit_df = spark.createDataFrame(audit_data)\n",
    "audit_df.write.mode('overwrite').saveAsTable('training.audit_logs')\n",
    "\n",
    "print('âœ… Synthetic audit logs created')\n",
    "\n",
    "# ========================================\n",
    "# SUMMARY\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('ðŸ“Š SYNTHETIC SYSTEM TABLES CREATED')\n",
    "print('='*60)\n",
    "print(f'âœ… training.system_billing     - {billing_df.count()} records')\n",
    "print(f'âœ… training.query_history      - {query_history_df.count()} records')\n",
    "print(f'âœ… training.user_permissions   - {user_permissions_df.count()} records')\n",
    "print(f'âœ… training.audit_logs         - {audit_df.count()} records')\n",
    "print('='*60)\n",
    "\n",
    "# Display sample data\n",
    "print('\\nðŸ“‹ Sample Billing Data:')\n",
    "billing_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca1c650-f16e-4ca3-ac76-bf22a82432a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Cost Analysis Queries\n",
    "\n",
    "Now let's analyze our synthetic billing data to understand cost patterns. In a real production environment, you would use `system.billing.*` tables.\n",
    "\n",
    "### Total Cost by Day\n",
    "\n",
    "2. **Cost by Workspace** (Bar Chart)\n",
    "```sql\n",
    "SELECT workspace_id, SUM(usage_quantity * list_price) as cost\n",
    "FROM training.system_billing\n",
    "WHERE usage_date >= CURRENT_DATE - 30\n",
    "GROUP BY workspace_id;\n",
    "```\n",
    "\n",
    "3. **Top Cost Drivers** (Table)\n",
    "```sql\n",
    "SELECT \n",
    "  usage_metadata.job_id,\n",
    "  sku_name,\n",
    "  SUM(usage_quantity * list_price) as total_cost\n",
    "FROM training.system_billing\n",
    "WHERE usage_date >= CURRENT_DATE - 7\n",
    "GROUP BY usage_metadata.job_id, sku_name\n",
    "ORDER BY total_cost DESC\n",
    "LIMIT 20;\n",
    "```\n",
    "\n",
    "4. **User Cost Allocation** (Pie Chart)\n",
    "```sql\n",
    "SELECT \n",
    "  usage_metadata.user,\n",
    "  SUM(usage_quantity * list_price) as cost\n",
    "FROM training.system_billing\n",
    "WHERE usage_date >= CURRENT_DATE - 30\n",
    "GROUP BY usage_metadata.user;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "âœ… **System tables** - Observability into usage and costs  \n",
    "âœ… **Billing analysis** - Track and allocate costs  \n",
    "âœ… **Usage monitoring** - Query patterns and performance  \n",
    "âœ… **Unity Catalog security** - Row filters and column masking  \n",
    "âœ… **Governance dashboards** - Visual cost tracking  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Monitor costs regularly** - Daily/weekly review\n",
    "2. **Implement cost allocation** - Tag and track by team/project\n",
    "3. **Use row-level security** - Protect sensitive data\n",
    "4. **Audit access** - Track who accesses what\n",
    "5. **Create dashboards** - Visualize key metrics\n",
    "\n",
    "### Cost Optimization Tips:\n",
    "\n",
    "- Use job clusters instead of all-purpose\n",
    "- Enable auto-termination\n",
    "- Right-size clusters\n",
    "- Use spot instances where possible\n",
    "- Schedule non-urgent jobs for off-peak hours\n",
    "- Archive old data to cheaper storage\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [System Tables Guide](https://docs.databricks.com/aws/en/admin/system-tables/)\n",
    "- [Unity Catalog Security](https://docs.databricks.com/aws/en/data-governance/unity-catalog/access-control)\n",
    "- [Observability Examples](https://github.com/CodyAustinDavis/dbsql_sme/tree/main/Observability%20Dashboards%20and%20DBA%20Resources)\n",
    "- [Cost Management](https://docs.databricks.com/aws/en/admin/account-settings/usage-detail-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Total Cost by Day\n",
    "SELECT \n",
    "  usage_date,\n",
    "  SUM(usage_quantity * list_price) as total_cost,\n",
    "  COUNT(*) as num_operations\n",
    "FROM training.system_billing\n",
    "GROUP BY usage_date\n",
    "ORDER BY usage_date DESC\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost by Workspace\n",
    "\n",
    "Identify which workspaces are consuming the most resources:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Cost by Workspace\n",
    "SELECT \n",
    "  workspace_id,\n",
    "  SUM(usage_quantity * list_price) as total_cost,\n",
    "  COUNT(*) as num_operations,\n",
    "  AVG(usage_quantity * list_price) as avg_cost_per_operation\n",
    "FROM training.system_billing\n",
    "WHERE usage_date >= CURRENT_DATE - 30\n",
    "GROUP BY workspace_id\n",
    "ORDER BY total_cost DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost by SKU Type\n",
    "\n",
    "Understand which compute types are driving costs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Cost by SKU Type\n",
    "SELECT \n",
    "  sku_name,\n",
    "  SUM(usage_quantity) as total_dbus,\n",
    "  SUM(usage_quantity * list_price) as total_cost,\n",
    "  AVG(usage_quantity * list_price) as avg_cost_per_operation,\n",
    "  COUNT(*) as operations\n",
    "FROM training.system_billing\n",
    "WHERE usage_date >= CURRENT_DATE - 30\n",
    "GROUP BY sku_name\n",
    "ORDER BY total_cost DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost by User\n",
    "\n",
    "Track cost allocation to individual users and teams:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Cost by User\n",
    "SELECT \n",
    "  usage_metadata.user,\n",
    "  COUNT(*) as operations,\n",
    "  SUM(usage_quantity * list_price) as total_cost,\n",
    "  AVG(usage_quantity * list_price) as avg_cost_per_operation\n",
    "FROM training.system_billing\n",
    "WHERE usage_date >= CURRENT_DATE - 30\n",
    "GROUP BY usage_metadata.user\n",
    "ORDER BY total_cost DESC\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Usage Monitoring\n",
    "\n",
    "Monitor query execution patterns and warehouse performance using query history.\n",
    "\n",
    "### Most Expensive Queries\n",
    "\n",
    "Identify queries that are consuming the most resources:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Most expensive queries in the last 7 days\n",
    "SELECT \n",
    "  query_id,\n",
    "  LEFT(query_text, 80) as query_text_preview,\n",
    "  execution_time_ms,\n",
    "  rows_produced,\n",
    "  bytes_scanned,\n",
    "  compute_cost,\n",
    "  user_email,\n",
    "  warehouse_id,\n",
    "  query_start_time\n",
    "FROM training.query_history\n",
    "WHERE query_start_time >= CURRENT_DATE - 7\n",
    "ORDER BY compute_cost DESC\n",
    "LIMIT 20;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Performance by Type\n",
    "\n",
    "Analyze performance patterns by query type:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Query performance by type\n",
    "SELECT \n",
    "  query_type,\n",
    "  COUNT(*) as query_count,\n",
    "  AVG(execution_time_ms) as avg_duration_ms,\n",
    "  MAX(execution_time_ms) as max_duration_ms,\n",
    "  AVG(compute_cost) as avg_cost,\n",
    "  SUM(compute_cost) as total_cost\n",
    "FROM training.query_history\n",
    "WHERE query_start_time >= CURRENT_DATE - 7\n",
    "GROUP BY query_type\n",
    "ORDER BY query_count DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Activity Analysis\n",
    "\n",
    "Track which users are running the most queries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- User activity patterns\n",
    "SELECT \n",
    "  user_email,\n",
    "  COUNT(*) as total_queries,\n",
    "  SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed_queries,\n",
    "  AVG(execution_time_ms) as avg_execution_ms,\n",
    "  SUM(compute_cost) as total_cost\n",
    "FROM training.query_history\n",
    "WHERE query_start_time >= CURRENT_DATE - 7\n",
    "GROUP BY user_email\n",
    "ORDER BY total_queries DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Unity Catalog Security\n",
    "\n",
    "Implement data governance using Unity Catalog's security features.\n",
    "\n",
    "### View User Permissions\n",
    "\n",
    "First, let's see what permissions we've defined:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- View user permissions\n",
    "SELECT * FROM training.user_permissions;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grant Privileges (Examples)\n",
    "\n",
    "**Note:** These are example commands. In production, you would grant privileges to actual user groups.\n",
    "\n",
    "**Grant SELECT on schema:**\n",
    "```sql\n",
    "GRANT SELECT ON SCHEMA training TO `data-analysts`;\n",
    "```\n",
    "\n",
    "**Grant table access:**\n",
    "```sql\n",
    "GRANT SELECT ON TABLE training.system_billing TO `data-analysts`;\n",
    "```\n",
    "\n",
    "**Grant usage on catalog:**\n",
    "```sql\n",
    "GRANT USAGE ON CATALOG <your_catalog> TO `data-analysts`;\n",
    "```\n",
    "\n",
    "### Row-Level Security (Conceptual Example)\n",
    "\n",
    "Unity Catalog supports row filters to restrict data based on user permissions. Here's how it works:\n",
    "\n",
    "**Step 1: Create a filter function**\n",
    "```sql\n",
    "CREATE FUNCTION training.filter_by_region(region STRING)\n",
    "RETURN region IN (\n",
    "  SELECT region FROM training.user_permissions \n",
    "  WHERE user_email = current_user()\n",
    ");\n",
    "```\n",
    "\n",
    "**Step 2: Apply the filter to a table**\n",
    "```sql\n",
    "ALTER TABLE <your_table>\n",
    "SET ROW FILTER training.filter_by_region(region) ON (region);\n",
    "```\n",
    "\n",
    "This ensures users only see data for their authorized regions.\n",
    "\n",
    "### Column Masking (Conceptual Example)\n",
    "\n",
    "Mask sensitive columns based on user roles:\n",
    "\n",
    "**Step 1: Create masking function**\n",
    "```sql\n",
    "CREATE FUNCTION training.mask_device_id(device_id STRING)\n",
    "RETURN CASE \n",
    "  WHEN is_member('admin') THEN device_id\n",
    "  ELSE CONCAT('***', RIGHT(device_id, 4))\n",
    "END;\n",
    "```\n",
    "\n",
    "**Step 2: Apply mask to column**\n",
    "```sql\n",
    "ALTER TABLE <your_table>\n",
    "ALTER COLUMN device_id\n",
    "SET MASK training.mask_device_id;\n",
    "```\n",
    "\n",
    "Non-admin users will only see masked device IDs (e.g., \"***1234\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Audit Logging\n",
    "\n",
    "Track data access and changes using audit logs.\n",
    "\n",
    "### Recent Data Access Events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Track data access events\n",
    "SELECT \n",
    "  event_time,\n",
    "  user_email,\n",
    "  action_name,\n",
    "  table_full_name,\n",
    "  workspace_id,\n",
    "  source_ip\n",
    "FROM training.audit_logs\n",
    "WHERE action_name IN ('SELECT', 'UPDATE', 'DELETE', 'INSERT')\n",
    "  AND event_date >= CURRENT_DATE - 7\n",
    "ORDER BY event_time DESC\n",
    "LIMIT 50;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Access by User\n",
    "\n",
    "Who is accessing which tables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Table access patterns by user\n",
    "SELECT \n",
    "  user_email,\n",
    "  table_full_name,\n",
    "  COUNT(*) as access_count,\n",
    "  COUNT(DISTINCT DATE(event_time)) as days_accessed,\n",
    "  MAX(event_time) as last_accessed\n",
    "FROM training.audit_logs\n",
    "WHERE event_date >= CURRENT_DATE - 7\n",
    "  AND action_name = 'SELECT'\n",
    "GROUP BY user_email, table_full_name\n",
    "ORDER BY access_count DESC\n",
    "LIMIT 20;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Changes\n",
    "\n",
    "Track DDL operations (CREATE, DROP, ALTER):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Track schema changes (CREATE, DROP, ALTER)\n",
    "SELECT \n",
    "  event_time,\n",
    "  user_email,\n",
    "  action_name,\n",
    "  table_full_name,\n",
    "  workspace_id\n",
    "FROM training.audit_logs\n",
    "WHERE action_name IN ('CREATE_TABLE', 'DROP_TABLE', 'ALTER_TABLE')\n",
    "  AND event_date >= CURRENT_DATE - 7\n",
    "ORDER BY event_time DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Create Observability Dashboards\n",
    "\n",
    "Use the queries above to build governance dashboards. Here are key visualizations to create:\n",
    "\n",
    "### Dashboard Structure\n",
    "\n",
    "**Page 1: Cost Overview**\n",
    "- Daily Cost Trend (Line Chart) - Use cell 4 query\n",
    "- Cost by Workspace (Bar Chart) - Use cell 6 query\n",
    "- Cost by SKU Type (Pie Chart) - Use cell 8 query\n",
    "- Top Cost Drivers by User (Table) - Use cell 10 query\n",
    "\n",
    "**Page 2: Usage Monitoring**\n",
    "- Most Expensive Queries (Table) - Use cell 12 query\n",
    "- Query Performance by Type (Bar Chart) - Use cell 14 query\n",
    "- User Activity (Table) - Use cell 16 query\n",
    "\n",
    "**Page 3: Security & Audit**\n",
    "- Recent Access Events (Table) - Use cell 21 query\n",
    "- Table Access by User (Heatmap) - Use cell 23 query\n",
    "- Schema Changes (Timeline) - Use cell 25 query\n",
    "\n",
    "### How to Create the Dashboard\n",
    "\n",
    "1. **Open Dashboards** from the left sidebar\n",
    "2. **Create New Dashboard**\n",
    "3. **Add Visualizations** for each query above\n",
    "4. **Set Refresh Schedule** (e.g., every 6 hours)\n",
    "5. **Share with Team** - Grant view access to stakeholders\n",
    "\n",
    "### Example: Daily Cost Trend Query\n",
    "\n",
    "This query works well for a line chart visualization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Daily cost trend (optimized for line chart)\n",
    "SELECT \n",
    "  usage_date,\n",
    "  SUM(usage_quantity * list_price) as total_cost,\n",
    "  SUM(CASE WHEN sku_name = 'JOBS_COMPUTE' THEN usage_quantity * list_price ELSE 0 END) as jobs_cost,\n",
    "  SUM(CASE WHEN sku_name = 'SQL_COMPUTE' THEN usage_quantity * list_price ELSE 0 END) as sql_cost,\n",
    "  SUM(CASE WHEN sku_name = 'ALL_PURPOSE_COMPUTE' THEN usage_quantity * list_price ELSE 0 END) as all_purpose_cost\n",
    "FROM training.system_billing\n",
    "GROUP BY usage_date\n",
    "ORDER BY usage_date;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've learned how to monitor and govern your Databricks workspace using system tables and Unity Catalog.\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "âœ… **Created synthetic system tables** - Billing, query history, audit logs  \n",
    "âœ… **Analyzed costs** - By workspace, user, SKU type, and time  \n",
    "âœ… **Monitored usage** - Query patterns, performance, and user activity  \n",
    "âœ… **Implemented security** - Row filters, column masking, access control  \n",
    "âœ… **Built audit trails** - Track data access and schema changes  \n",
    "âœ… **Designed dashboards** - Visual cost tracking and governance  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Monitor costs regularly** - Set up daily/weekly reviews\n",
    "2. **Implement cost allocation** - Tag and track by team/project\n",
    "3. **Use row-level security** - Protect sensitive data automatically\n",
    "4. **Audit access** - Track who accesses what and when\n",
    "5. **Create dashboards** - Visualize key metrics for stakeholders\n",
    "\n",
    "### Applying This to the IoT Project\n",
    "\n",
    "For your IoT manufacturing project, you should:\n",
    "\n",
    "**Cost Monitoring:**\n",
    "- Track costs of your daily sensor data pipelines\n",
    "- Monitor dashboard query costs from Genie spaces\n",
    "- Allocate costs to different teams (ops, analytics, ML)\n",
    "\n",
    "**Usage Governance:**\n",
    "- Ensure analysts only access authorized factory data\n",
    "- Mask device IDs for non-admin users\n",
    "- Monitor which ML models are consuming resources\n",
    "\n",
    "**Security & Compliance:**\n",
    "- Restrict regional data access based on user location\n",
    "- Audit access to sensitive inspection data\n",
    "- Track schema changes to production tables\n",
    "\n",
    "### Cost Optimization Tips\n",
    "\n",
    "- **Use job clusters** instead of all-purpose for scheduled workloads\n",
    "- **Enable auto-termination** on interactive clusters (15-30 minutes)\n",
    "- **Right-size clusters** - Don't over-provision compute\n",
    "- **Use spot instances** where possible for non-critical jobs\n",
    "- **Schedule non-urgent jobs** for off-peak hours\n",
    "- **Archive old data** to cheaper storage tiers\n",
    "- **Use Photon** for SQL queries (2-3x performance improvement)\n",
    "- **Enable Predictive Optimization** for automatic table maintenance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Create a governance dashboard** using the queries above\n",
    "2. **Set up alerts** for cost thresholds and unusual access patterns\n",
    "3. **Implement row-level security** on your IoT tables\n",
    "4. **Review audit logs** weekly to track usage patterns\n",
    "5. **Share cost reports** with leadership to demonstrate value\n",
    "\n",
    "---\n",
    "\n",
    "## Try This Out\n",
    "\n",
    "**Challenge:** Create a comprehensive governance dashboard for the IoT project with:\n",
    "1. Cost by factory (join billing data with job metadata)\n",
    "2. Most queried IoT tables (from query_history)\n",
    "3. User access patterns (from audit_logs)\n",
    "4. Failed query analysis (for troubleshooting)\n",
    "\n",
    "**Bonus:** Set up row-level security so analysts only see data from their assigned factories.\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Resources:**\n",
    "- [System Tables Guide](https://docs.databricks.com/aws/en/admin/system-tables/)\n",
    "- [Unity Catalog Security](https://docs.databricks.com/aws/en/data-governance/unity-catalog/access-control)\n",
    "- [Observability Examples](https://github.com/CodyAustinDavis/dbsql_sme/tree/main/Observability%20Dashboards%20and%20DBA%20Resources)\n",
    "- [Cost Management](https://docs.databricks.com/aws/en/admin/account-settings/usage-detail-tags-aws)\n",
    "- [Row Filters and Column Masks](https://docs.databricks.com/aws/en/data-governance/unity-catalog/row-and-column-filters)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ You've completed Day 3!** You now have the skills to build end-to-end data and ML pipelines, monitor costs, and govern your Databricks workspace effectively.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3 Monitor and Govern",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
