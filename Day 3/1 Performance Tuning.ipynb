{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b033a0f-0a66-47c4-a00d-f4d1480138b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance Tuning: SQL & Table Optimization\n",
    "\n",
    "**The Situation:** Leadership wants dashboards, predictive models, and AI agents ready by Friday. Your plane IoT data is growing fast, and queries that worked yesterday are timing out today.\n",
    "\n",
    "**The Problem:** Slow queries = higher cost, missed deadlines, and angry leadership\n",
    "\n",
    "**The Solution:** Get familiar with both SQL and table optimization techniques to get sub-second query times.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "‚úÖ **SQL Optimization:** Predicate pushdown, join strategies, broadcast hints  \n",
    "‚úÖ **Liquid Clustering:** Automatic data layout optimization  \n",
    "‚úÖ **Materialized Views:** Pre-compute expensive aggregations  \n",
    "‚úÖ **Query Profile:** Analyze query execution on SQL Warehouses  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Day 1 & 2\n",
    "- `sensor_bronze`, `dim_factories`, `dim_devices` tables loaded\n",
    "- SQL Warehouse or cluster running\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Delta Lake Performance](https://docs.databricks.com/en/delta/tune-file-layout.html)\n",
    "- [Liquid Clustering](https://docs.databricks.com/en/delta/clustering.html)\n",
    "- [Query Optimization](https://docs.databricks.com/en/optimizations/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91b22df-6941-4306-b382-ead819f9eef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import re\n",
    "\n",
    "catalog = \"dwx_airops_insights_platform_dev_working\"\n",
    "source_schema = \"db_crash_course\"  # Shared schema to read from\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username_base = username.split('@')[0]  # Extract username before @ symbol\n",
    "target_schema = re.sub(r'[^a-zA-Z0-9_]', '_', username_base)  # Replace special chars with _\n",
    "\n",
    "# Create target schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{target_schema}\")\n",
    "\n",
    "print(f\"‚úÖ Using catalog: {catalog}\")\n",
    "print(f\"üìñ Reading from schema: {source_schema} (shared)\")\n",
    "print(f\"‚úçÔ∏è  Writing to schema: {target_schema} (your personal schema)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f7f8e9-fb02-41a1-bff9-ac1058cd0c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Understanding Performance Bottlenecks\n",
    "\n",
    "### Common Performance Killers\n",
    "\n",
    "| Problem | Impact | Solution |\n",
    "|---------|--------|----------|\n",
    "| üêå **Small Files** | Too many file opens | OPTIMIZE |\n",
    "| üêå **Full Table Scans** | Read entire table | Liquid Clustering, predicates |\n",
    "| üêå **Data Shuffle** | Network overhead | Broadcast joins |\n",
    "| üêå **Wrong Join Type** | Memory spills | Join hints |\n",
    "| üêå **Repeated Computation** | Wasted resources | Materialized views |\n",
    "| üêå **Inefficient Predicates** | No pushdown | Proper filters |\n",
    "\n",
    "### Performance Toolkit\n",
    "\n",
    "**SQL Optimization:**\n",
    "- Predicate pushdown (filter early)\n",
    "- Join hints (BROADCAST, SHUFFLE_HASH)\n",
    "- Proper WHERE clause design\n",
    "- Query Profile analysis\n",
    "\n",
    "**Table Optimization:**\n",
    "- File compaction (OPTIMIZE)\n",
    "- Liquid Clustering (automatic data layout optimization)\n",
    "- Deletion Vectors (fast updates)\n",
    "\n",
    "**Query Results:**\n",
    "- Materialized Views\n",
    "- Caching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee60b8c9-0db9-4217-b714-fee0831f6b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Creating a \"Bad\" Table for Demonstration\n",
    "\n",
    "Let's intentionally create a poorly optimized table with:\n",
    "- Many small files (simulating streaming ingestion)\n",
    "- Random data layout (no locality)\n",
    "- No optimization\n",
    "\n",
    "This represents what happens in real production systems without proper maintenance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59859860-2901-4fac-8ae5-71b78111568a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create unoptimized table with random layout in your target schema\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{target_schema}.sensor_unoptimized\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {catalog}.{target_schema}.sensor_unoptimized\n",
    "AS\n",
    "SELECT \n",
    "    device_id,\n",
    "    trip_id,\n",
    "    factory_id,\n",
    "    model_id,\n",
    "    timestamp,\n",
    "    airflow_rate,\n",
    "    rotation_speed,\n",
    "    air_pressure,\n",
    "    temperature,\n",
    "    delay,\n",
    "    density\n",
    "FROM {catalog}.{source_schema}.sensor_bronze\n",
    "ORDER BY RAND()  -- Random order = worst case for data locality!\n",
    "LIMIT 200000  -- Use subset for demo\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created unoptimized table with random layout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b17f70c-b1f4-42c4-8bed-361e7f7edb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Simulate many small files (like streaming writes)\n",
    "# This is what happens with continuous ingestion without auto-compaction\n",
    "\n",
    "for i in range(15):  # Create 15 small file batches\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {catalog}.{target_schema}.sensor_unoptimized\n",
    "    SELECT \n",
    "        device_id,\n",
    "        trip_id,\n",
    "        factory_id,\n",
    "        model_id,\n",
    "        timestamp,\n",
    "        airflow_rate,\n",
    "        rotation_speed,\n",
    "        air_pressure,\n",
    "        temperature,\n",
    "        delay,\n",
    "        density\n",
    "    FROM {catalog}.{source_schema}.sensor_bronze\n",
    "    WHERE MOD(device_id, 15) = {i}\n",
    "    LIMIT 800\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úÖ Created many small files (simulating poor ingestion patterns)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243da376-2fd9-4b59-90a6-b118a5247f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check table statistics - look at the file count!\n",
    "display(spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {catalog}.{target_schema}.sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\", \"minReaderVersion\", \"minWriterVersion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27f9af3-4eea-4129-98b9-315d45e9d91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîç What to Look For:\n",
    "\n",
    "- **numFiles**: High number (hundreds of thousands or millions) = Performance problem!\n",
    "- **sizeInBytes**: Total size, but spread across too many files\n",
    "\n",
    "**Problem:** Every query must:\n",
    "1. List all files\n",
    "2. Open each file\n",
    "3. Read metadata\n",
    "4. Scan for relevant data\n",
    "\n",
    "With many small files, overhead dominates actual work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9e4a4c-7544-41fa-987a-4cc9e5535b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: SQL Optimization - Predicate Pushdown\n",
    "\n",
    "**Key Concept:** Push filters as close to the data as possible.\n",
    "\n",
    "### Comparing Query Performance\n",
    "\n",
    "Let's compare two approaches to filtering - one that prevents optimization and one that enables it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8976e3b-a83b-4d99-9880-69ff7f37f9a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚ùå Without Predicate Pushdown (Bad)\n",
    "\n",
    "Using a function on the column prevents statistics-based filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54748cab-a12c-462f-826e-1ff81a1e921e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# BAD: Using SUBSTRING on timestamp prevents predicate pushdown\n",
    "start = time.time()\n",
    "\n",
    "result_bad = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {catalog}.{target_schema}.sensor_unoptimized\n",
    "WHERE SUBSTRING(CAST(timestamp AS STRING), 1, 10) >= DATE_SUB(CURRENT_DATE(), 7)\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_bad = result_bad.count()\n",
    "time_bad = time.time() - start\n",
    "\n",
    "print(f\"‚ùå BAD Query Time: {time_bad:.2f} seconds\")\n",
    "print(f\"   Results: {count_bad} rows\")\n",
    "print(f\"   Problem: Function on column prevents statistics-based filtering!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e329a1-fef6-49ef-9c6f-aac56c5337e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚úÖ With Predicate Pushdown (Good)\n",
    "\n",
    "Direct filter on timestamp column enables predicate pushdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd95dfc-4d87-4953-b87c-60c9370a755a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GOOD: Direct filter on timestamp enables predicate pushdown\n",
    "start = time.time()\n",
    "\n",
    "result_good = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {catalog}.{target_schema}.sensor_unoptimized\n",
    "WHERE timestamp >= DATE_SUB(CURRENT_DATE(), 7)\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_good = result_good.count()\n",
    "time_good = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ GOOD Query Time: {time_good:.2f} seconds\")\n",
    "print(f\"   Results: {count_good} rows\")\n",
    "print(f\"   Benefit: Delta uses file statistics to skip irrelevant files!\")\n",
    "\n",
    "# Show improvement\n",
    "if time_good > 0 and time_bad > 0:\n",
    "    speedup = time_bad / time_good\n",
    "    print(f\"\\nüöÄ Speedup: {speedup:.1f}x faster with predicate pushdown!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fec205-18b1-4509-8ab7-c7aa230079b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Best Practices for Predicate Pushdown\n",
    "\n",
    "**DO:**\n",
    "```sql\n",
    "WHERE timestamp >= '2024-01-01'  -- Direct column comparison\n",
    "WHERE device_id IN (1, 2, 3)     -- Direct value check\n",
    "WHERE factory_id = 'A06'          -- Equality on column\n",
    "```\n",
    "\n",
    "**DON'T:**\n",
    "```sql\n",
    "WHERE DATE(timestamp) = '2024-01-01'      -- Function prevents pushdown\n",
    "WHERE SUBSTRING(device_id, 1, 2) = '10'   -- Function on column\n",
    "WHERE UPPER(factory_id) = 'A06'           -- Transformation blocks optimization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a805eaa-10b2-4282-beb0-7f6cd15d260e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: SQL Optimization - Join Strategies\n",
    "\n",
    "### Understanding Join Types\n",
    "\n",
    "| Join Type | Best For | Cost |\n",
    "|-----------|----------|------|\n",
    "| **Broadcast Join** | Small table (< 10MB) | Low - no shuffle |\n",
    "| **Shuffle Hash Join** | Large tables | High - shuffle both |\n",
    "| **Sort Merge Join** | Large sorted tables | Medium |\n",
    "\n",
    "### Comparing Join Strategies\n",
    "\n",
    "Let's compare queries with and without join hints to see the performance impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107c6c3f-c146-4bab-8b7f-517158e5133d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚ö†Ô∏è Without Hint - Spark Auto-Selects Join Strategy\n",
    "\n",
    "Without a hint we let Spark choose the join strategy automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d129f811-c074-4fd2-89c1-521ba297f36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚úÖ With Broadcast Hint - Force Efficient Join\n",
    "\n",
    "Use the BROADCAST hint to force an efficient join strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2324d702-5f21-498b-8125-abf18d7fd312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WITH BROADCAST hint - Force efficient join strategy\n",
    "start = time.time()\n",
    "\n",
    "result_broadcast = spark.sql(f\"\"\"\n",
    "SELECT /*+ BROADCAST(f) */\n",
    "    s.device_id,\n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    AVG(s.temperature) as avg_temp,\n",
    "    COUNT(*) as reading_count\n",
    "FROM {catalog}.{target_schema}.sensor_unoptimized s\n",
    "JOIN {catalog}.{source_schema}.dim_factories f\n",
    "  ON s.factory_id = f.factory_id\n",
    "WHERE s.timestamp >= CURRENT_DATE() - INTERVAL 7 DAYS\n",
    "GROUP BY s.device_id, f.factory_name, f.region\n",
    "\"\"\")\n",
    "\n",
    "count_broadcast = result_broadcast.count()\n",
    "time_broadcast = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Query Time (Broadcast): {time_broadcast:.2f} seconds\")\n",
    "print(f\"   Small dimension table broadcast to all nodes - no shuffle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd33e351-f9f5-4801-b2af-e8dce6939aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Best Practices for Join Optimization\n",
    "\n",
    "**BROADCAST when:**\n",
    "- Dimension table < 10MB\n",
    "- Reference data (factories, models, devices)\n",
    "- Lookup tables\n",
    "\n",
    "**Let Spark choose when:**\n",
    "- Both tables are large\n",
    "- Join cardinality is unknown\n",
    "- Adaptive Query Execution is enabled (default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bde12c4-ee8e-4fe6-8a76-7bf997cd24be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üí° Join Optimization Rules\n",
    "\n",
    "**BROADCAST when:**\n",
    "- Dimension table < 10MB\n",
    "- Reference data (factories, models, devices)\n",
    "- Lookup tables\n",
    "\n",
    "**Let Spark choose when:**\n",
    "- Both tables are large\n",
    "- Join cardinality is unknown\n",
    "- Adaptive Query Execution is enabled (default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceaa81d1-2861-44aa-a871-5718820fd0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check current file situation\n",
    "detail_before = spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {catalog}.{target_schema}.sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "\n",
    "files_before = detail_before['numFiles']\n",
    "size_mb = detail_before['sizeInBytes'] / 1024 / 1024\n",
    "\n",
    "print(f\"üìä Before Optimization:\")\n",
    "print(f\"   Files: {files_before}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Avg file size: {size_mb/files_before:.2f} MB\")\n",
    "print(f\"\\n   Status: {'üî¥ Too many small files!' if files_before > 10 else 'üü¢ OK'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c423407-80b0-4f12-bb76-d71f325838e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run OPTIMIZE to compact files\n",
    "start = time.time()\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "OPTIMIZE {catalog}.{target_schema}.sensor_unoptimized\n",
    "\"\"\")\n",
    "\n",
    "# Check the new file count\n",
    "detail_after = spark.sql(f\"\"\"\n",
    "DESCRIBE DETAIL {catalog}.{target_schema}.sensor_unoptimized\n",
    "\"\"\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "\n",
    "files_after = detail_after['numFiles']\n",
    "size_mb = detail_after['sizeInBytes'] / 1024 / 1024\n",
    "\n",
    "print(f\"üìä After Optimization:\")\n",
    "print(f\"   Files: {files_after}\")\n",
    "print(f\"   Size: {size_mb:.2f} MB\")\n",
    "print(f\"   Avg file size: {size_mb/files_after:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a93f19-4e2d-4d53-ab1b-76b4d2fd61ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5: Table Optimization - Liquid Clustering\n",
    "\n",
    "**Liquid Clustering** is Delta Lake's automatic data layout optimization. Unity Catalog can automatically cluster all of your tables, [described here](https://docs.databricks.com/aws/en/optimizations/predictive-optimization), or you can manually cluster them as shown below\n",
    "\n",
    "### Why Liquid Clustering?\n",
    "- ‚úÖ Automatic optimization during writes\n",
    "- ‚úÖ Adapts to changing access patterns\n",
    "- ‚úÖ No manual maintenance required\n",
    "- ‚úÖ Combines compaction + data layout\n",
    "\n",
    "### How Data Skipping Works\n",
    "\n",
    "**Without Clustering:**\n",
    "```\n",
    "File 1: devices 1,5,10,15,20     <- Must read\n",
    "File 2: devices 2,3,8,12,19      <- Must read  \n",
    "File 3: devices 4,7,9,11,14      <- Must read\n",
    "```\n",
    "Query for device_id = 5 must read ALL files!\n",
    "\n",
    "**With Liquid Clustering on device_id:**\n",
    "```\n",
    "File 1: devices 1,2,3,4,5        <- Read this (automatically organized!)\n",
    "File 2: devices 7,8,9,10,11      <- SKIP\n",
    "File 3: devices 12,14,15,19,20   <- SKIP\n",
    "```\n",
    "Query for device_id = 5 only reads File 1!\n",
    "\n",
    "### Choosing Clustering Columns\n",
    "\n",
    "‚úÖ **Good candidates:**\n",
    "- High cardinality (device_id, timestamp)\n",
    "- Frequently in WHERE clauses\n",
    "- Used in joins\n",
    "- Common GROUP BY columns\n",
    "\n",
    "‚ùå **Bad candidates:**\n",
    "- Low cardinality (status: active/inactive)\n",
    "- Rarely filtered\n",
    "\n",
    "**Rule:** 2-4 columns maximum, order matters (most selective first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbb9982-ba6d-4a54-8343-cde8f8e61fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, let's create a clustered version of the unoptimized table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {catalog}.{target_schema}.sensor_clustered\n",
    "CLUSTER BY (device_id, timestamp)\n",
    "AS SELECT * FROM {catalog}.{source_schema}.sensor_bronze\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created table with Liquid Clustering on (device_id, timestamp)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e4c5f0-d4ad-484a-a562-05608047553d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# BAD: Using SUBSTRING on timestamp prevents predicate pushdown\n",
    "start = time.time()\n",
    "\n",
    "result_bad = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {catalog}.{target_schema}.sensor_clustered\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_bad = result_bad.count()\n",
    "time_bad = time.time() - start\n",
    "\n",
    "print(f\"‚ùå BAD Query Time: {time_bad:.2f} seconds\")\n",
    "print(f\"   Results: {count_bad} rows\")\n",
    "print(f\"   Problem: No data skipping!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20bfa06e-9db6-4f34-88fc-3075c2943288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GOOD: Direct filter on timestamp enables predicate pushdown\n",
    "start = time.time()\n",
    "\n",
    "result_good = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    device_id,\n",
    "    factory_id,\n",
    "    AVG(temperature) as avg_temp\n",
    "FROM {catalog}.{target_schema}.sensor_clustered\n",
    "GROUP BY device_id, factory_id\n",
    "\"\"\")\n",
    "\n",
    "count_good = result_good.count()\n",
    "time_good = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ GOOD Query Time: {time_good:.2f} seconds\")\n",
    "print(f\"   Results: {count_good} rows\")\n",
    "print(f\"   Benefit: Delta uses file statistics to skip irrelevant files!\")\n",
    "\n",
    "# Show improvement\n",
    "if time_good > 0 and time_bad > 0:\n",
    "    speedup = time_bad / time_good\n",
    "    print(f\"\\nüöÄ Speedup: {speedup:.1f}x faster with predicate pushdown!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39df3eb3-006e-4f1d-9b8e-25a930306660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 6. Caching Strategies\n",
    "\n",
    "**Caching** keeps frequently accessed data in memory for instant access.\n",
    "\n",
    "### Types of Caching:\n",
    "\n",
    "1. **DataFrame Cache**: Temporary, session-specific\n",
    "2. **Delta Cache**: Disk-based, persists across queries\n",
    "3. **Result Cache**: Caches query results\n",
    "\n",
    "### When to Use Caching:\n",
    "\n",
    "‚úÖ Dimension tables (small, frequently joined)  \n",
    "‚úÖ Reference data  \n",
    "‚úÖ Iterative ML training  \n",
    "‚úÖ Dashboard data sources  \n",
    "\n",
    "‚ùå Don't cache:\n",
    "- Large fact tables (waste of memory)\n",
    "- Rarely accessed data\n",
    "- Data that changes frequently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca95077-7622-40f4-aa20-931ce8e4ee51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cache frequently used dimension tables\n",
    "# These are joined in almost every query!\n",
    "\n",
    "spark.sql(f\"CACHE TABLE {catalog}.{source_schema}.dim_factories\")\n",
    "spark.sql(f\"CACHE TABLE {catalog}.{source_schema}.dim_models\")\n",
    "spark.sql(f\"CACHE TABLE {catalog}.{source_schema}.dim_devices\")\n",
    "\n",
    "print(\"‚úÖ Cached dimension tables\")\n",
    "print(\"   Joins with these tables are now instant!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7e2f60-e20f-4c0b-9a65-3b440b86d91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test query with cached dimensions\n",
    "start = time.time()\n",
    "\n",
    "result_cached = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    f.factory_name,\n",
    "    f.region,\n",
    "    m.model_name,\n",
    "    m.model_family,\n",
    "    d.device_id,\n",
    "    COUNT(DISTINCT s.trip_id) as trip_count,\n",
    "    AVG(s.temperature) as avg_temp\n",
    "FROM {catalog}.{source_schema}.sensor_bronze s\n",
    "JOIN {catalog}.{source_schema}.dim_devices d ON s.device_id = d.device_id\n",
    "JOIN {catalog}.{source_schema}.dim_factories f ON d.factory_id = f.factory_id\n",
    "JOIN {catalog}.{source_schema}.dim_models m ON d.model_id = m.model_id\n",
    "GROUP BY f.factory_name, f.region, m.model_name, m.model_family, d.device_id\n",
    "\"\"\")\n",
    "\n",
    "display(result_cached)\n",
    "\n",
    "cached_time = time.time() - start\n",
    "print(f\"\\n‚è±Ô∏è  Query time (with cached dimensions): {cached_time:.2f} seconds\")\n",
    "print(\"‚ú® Dimension joins are instant - no disk I/O needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29129b48-4bf0-48cd-a286-86f22cfc4d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Try This: Using Query Profile on SQL Warehouses\n",
    "\n",
    "**Query Profile** is your best friend for diagnosing slow queries on SQL Warehouses.\n",
    "\n",
    "### What is Query Profile?\n",
    "\n",
    "Query Profile shows you **exactly** what your query is doing:\n",
    "- Which operations took the longest\n",
    "- How much data was read\n",
    "- Where shuffles happened\n",
    "- Memory spills\n",
    "\n",
    "### How to Access Query Profile\n",
    "\n",
    "1. Run a query on a **SQL Warehouse** (not a cluster)\n",
    "2. After the query completes, click the **\"Query Profile\"** tab\n",
    "3. Explore the visual execution plan\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "| Problem in Profile | Meaning | Solution |\n",
    "|-------------------|---------|----------|\n",
    "| üî¥ **Large Scan** | Reading too much data | Add Liquid Clustering, better filters |\n",
    "| üî¥ **Shuffle** | Data moving between nodes | Use broadcast joins for small tables |\n",
    "| üî¥ **Spill to Disk** | Out of memory | Increase warehouse size or optimize query |\n",
    "| üî¥ **Many Tasks** | Too many small files | Run OPTIMIZE |\n",
    "\n",
    "### Example Workflow\n",
    "\n",
    "```\n",
    "1. Query is slow (10+ seconds) ‚ùå\n",
    "2. Check Query Profile ‚Üí See \"Large Scan\"\n",
    "3. Add Liquid Clustering to table\n",
    "4. Re-run query ‚Üí 2 seconds ‚úÖ\n",
    "```\n",
    "\n",
    "**Learn more:** [Query Profile Documentation](https://docs.databricks.com/aws/en/sql/user/queries/query-profile)\n",
    "\n",
    "**üí° Pro Tip:** Query Profile only works on SQL Warehouses, not all-purpose or job clusters. If you're running notebooks on a cluster, switch to a SQL Warehouse to use this feature."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1 Performance Tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
