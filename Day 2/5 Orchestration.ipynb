{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Orchestration with Databricks Jobs\n\nAutomate and schedule data pipelines, notebooks, and workflows with Databricks Jobs.\n\n## What You'll Learn\n\n\u2705 Create and configure Databricks Jobs  \n\u2705 Schedule workflows with cron expressions  \n\u2705 Implement file arrival triggers  \n\u2705 Build multi-task dependencies  \n\u2705 Monitor and debug job runs  \n\n---\n\n## Why Orchestration?\n\n**Manual execution** doesn't scale.\n\n**Databricks Jobs** provide:\n- Scheduled execution\n- Event-driven triggers\n- Task dependencies\n- Error handling and retries\n- Centralized monitoring\n\n---\n\n## Table of Contents\n\n1. [Jobs Overview](#overview)\n2. [Creating Jobs](#creating)\n3. [Scheduling](#scheduling)\n4. [File Arrival Triggers](#triggers)\n5. [Task Dependencies](#dependencies)\n6. [Monitoring and Debugging](#monitoring)\n\n---\n\n**References:**\n- [Jobs Documentation](https://docs.databricks.com/aws/en/jobs/)\n- [Jobs Quickstart](https://docs.databricks.com/aws/en/jobs/jobs-quickstart)\n- [Triggers](https://docs.databricks.com/aws/en/jobs/triggers)\n- [File Arrival Triggers](https://docs.databricks.com/aws/en/jobs/file-arrival-triggers)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Jobs Overview <a id=\"overview\"></a>\n\n### Job Components\n\n**1. Tasks**: Individual units of work\n- Notebook execution\n- SQL queries\n- Python scripts\n- JAR files\n- Delta Live Tables pipelines\n\n**2. Clusters**: Compute resources\n- Job clusters (ephemeral)\n- All-purpose clusters (shared)\n- Serverless (recommended)\n\n**3. Schedules**: When jobs run\n- Cron expressions\n- Continuous\n- Triggered by events\n\n**4. Parameters**: Dynamic inputs\n- Notebook parameters\n- SQL parameters\n- Environment variables\n\n### Job Types\n\n**Batch Jobs:**\n- Run on schedule\n- Process historical data\n- Generate reports\n\n**Streaming Jobs:**\n- Run continuously\n- Process real-time data\n- Low latency\n\n**Event-Driven:**\n- Triggered by file arrival\n- Webhook calls\n- Manual triggers\n\n---\n\n## 2. Creating Jobs <a id=\"creating\"></a>\n\n### Quick Start: Single Task Job\n\n**Step 1: Create Job**\n1. Click **Workflows** \u2192 **Jobs**\n2. Click **Create Job**\n3. Name: \"Daily IoT Data Pipeline\"\n\n**Step 2: Add Task**\n```\nTask Name: ingest_sensor_data\nType: Notebook\nNotebook Path: /Day 1/6 Data Transformation\nCluster: New job cluster (2 workers, i3.xlarge)\n```\n\n**Step 3: Configure Schedule**\n```\nTrigger: Scheduled\nSchedule: 0 6 * * * (6 AM daily)\nTimezone: America/Los_Angeles\n```\n\n**Step 4: Save and Run**\n- Click **Save**\n- Click **Run Now** to test\n- View run details\n\n### Multi-Task Workflow\n\n**IoT Processing Pipeline:**\n```\nTask 1: Ingest Data\n  \u2193\nTask 2: Data Quality Checks\n  \u2193\nTask 3: Transform to Silver\n  \u2193\nTask 4: Aggregate to Gold\n  \u2193\nTask 5: Send Email Summary\n```\n\n**Configuration:**\n```json\n{\n  \"tasks\": [\n    {\n      \"task_key\": \"ingest\",\n      \"notebook_task\": {\n        \"notebook_path\": \"/pipelines/01_ingest\"\n      }\n    },\n    {\n      \"task_key\": \"quality_check\",\n      \"notebook_task\": {\n        \"notebook_path\": \"/pipelines/02_quality\"\n      },\n      \"depends_on\": [{\"task_key\": \"ingest\"}]\n    },\n    {\n      \"task_key\": \"silver_transform\",\n      \"notebook_task\": {\n        \"notebook_path\": \"/pipelines/03_silver\"\n      },\n      \"depends_on\": [{\"task_key\": \"quality_check\"}]\n    }\n  ]\n}\n```\n\n---\n\n## 3. Scheduling <a id=\"scheduling\"></a>\n\n### Cron Expressions\n\n**Format:** `minute hour day month day_of_week`\n\n**Common Patterns:**\n```\nEvery hour:       0 * * * *\nEvery day at 6AM: 0 6 * * *\nEvery Monday:     0 0 * * 1\nFirst of month:   0 0 1 * *\nEvery 15 mins:    */15 * * * *\nBusiness hours:   0 9-17 * * 1-5\n```\n\n**Examples:**\n```\n# Daily at 2 AM\nSchedule: 0 2 * * *\nDescription: Run overnight batch processing\n\n# Every 4 hours\nSchedule: 0 */4 * * *\nDescription: Periodic data refresh\n\n# Weekdays at 8 AM\nSchedule: 0 8 * * 1-5\nDescription: Business day reports\n```\n\n### Timezone Handling\n\n```\nSchedule: 0 6 * * *\nTimezone: America/Los_Angeles\nNote: Handles daylight saving time automatically\n```\n\n---\n\n## 4. File Arrival Triggers <a id=\"triggers\"></a>\n\n### Use Case: Process New Sensor Files\n\n**Scenario**: New CSV files arrive in cloud storage throughout the day\n\n**Setup:**\n```\nTrigger Type: File Arrival\nLocation: /Volumes/default/db_crash_course/sensor_data/\nFile Pattern: *.csv\nWait Duration: 5 minutes\nMax Wait: 1 hour\n```\n\n**How it Works:**\n1. Monitor specified path\n2. Detect new files matching pattern\n3. Wait for additional files (batch processing)\n4. Trigger job after wait duration\n5. Pass file paths as parameters\n\n**Job Configuration:**\n```python\n# In notebook, access file paths\nimport json\nfile_paths = json.loads(dbutils.widgets.get(\"file_paths\"))\n\nfor path in file_paths:\n    print(f\"Processing: {path}\")\n    df = spark.read.csv(path, header=True)\n    # Process file\n```\n\n---\n\n## 5. Task Dependencies <a id=\"dependencies\"></a>\n\n### Dependency Patterns\n\n**1. Linear Pipeline**\n```\nA \u2192 B \u2192 C \u2192 D\n```\n\n**2. Parallel Processing**\n```\n    \u250c\u2192 B \u2510\nA \u2500\u2500\u253c\u2192 C \u253c\u2192 E\n    \u2514\u2192 D \u2518\n```\n\n**3. Conditional Execution**\n```\nA \u2192 B \u2192 if success: C\n         if failure: D\n```\n\n### Task Values\n\n**Pass data between tasks:**\n\nTask 1 (Python):\n```python\n# Write output value\ndbutils.jobs.taskValues.set(\"row_count\", 1000)\ndbutils.jobs.taskValues.set(\"status\", \"success\")\n```\n\nTask 2 (Consuming values):\n```python\n# Read values from previous task\nrow_count = dbutils.jobs.taskValues.get(\"ingest_task\", \"row_count\")\nstatus = dbutils.jobs.taskValues.get(\"ingest_task\", \"status\")\n\nprint(f\"Previous task processed {row_count} rows with status: {status}\")\n```\n\n---\n\n## 6. Monitoring and Debugging <a id=\"monitoring\"></a>\n\n### Job Monitoring\n\n**Metrics to Track:**\n- Success/failure rates\n- Run duration trends\n- Cost per run\n- Cluster utilization\n\n**Alerts:**\n```\nEmail on Failure: team@company.com\nSlack Notification: #data-alerts\nPagerDuty: Critical jobs only\n```\n\n### Debugging Failures\n\n**Common Issues:**\n\n**1. Timeout:**\n```\nError: Job exceeded maximum runtime (3 hours)\nSolution: Increase timeout or optimize query\n```\n\n**2. Cluster Start Failure:**\n```\nError: Cannot acquire cluster\nSolution: Check quotas, use job clusters\n```\n\n**3. Data Not Found:**\n```\nError: Table not found\nSolution: Check dependencies, verify data arrival\n```\n\n**Debug Tools:**\n- Job run logs\n- Spark UI\n- Driver logs\n- Task output\n\n---\n\n## Summary\n\n\u2705 **Jobs** - Automate notebook and pipeline execution  \n\u2705 **Scheduling** - Cron expressions for time-based triggers  \n\u2705 **File triggers** - Event-driven processing  \n\u2705 **Dependencies** - Multi-task workflows  \n\u2705 **Monitoring** - Track success and debug failures  \n\n### Best Practices:\n\n1. **Use job clusters** for cost optimization\n2. **Set retries** for transient failures\n3. **Monitor costs** - track DBU usage\n4. **Alert appropriately** - don't spam\n5. **Document workflows** - explain dependencies\n\n---\n\n**References:**\n- [Jobs Docs](https://docs.databricks.com/aws/en/jobs/)\n- [Triggers](https://docs.databricks.com/aws/en/jobs/triggers)\n- [Best Practices](https://docs.databricks.com/aws/en/jobs/jobs-best-practices)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}