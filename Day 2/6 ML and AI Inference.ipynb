{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML and AI Inference: Deploying Models to Production\n",
    "\n",
    "**The Scenario:** You've trained defect prediction models with AutoML and MLflow. Now leadership needs these models in production - scoring thousands of devices in batch, monitoring sensors in real-time streams, and serving predictions via APIs.\n",
    "\n",
    "This notebook shows you how to deploy and use your models for:\n",
    "- **Batch Inference** - Score large datasets all at once\n",
    "- **Streaming Inference** - Real-time predictions on streaming data\n",
    "- **Model Serving APIs** - REST endpoints for applications\n",
    "- **AI Query** - SQL-based inference\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "âœ… Load models from MLflow for batch predictions  \n",
    "âœ… Apply models to streaming data with Structured Streaming  \n",
    "âœ… Deploy models as REST API endpoints  \n",
    "âœ… Use AI Query for SQL-based inference  \n",
    "\n",
    "**Time to Complete:** 30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, you should have:\n",
    "- âœ… Completed Notebook 4 (AutoML) **or** Notebook 5 (MLflow)\n",
    "- âœ… At least one model registered in Unity Catalog\n",
    "- âœ… Model tagged with an alias (e.g., \"Champion\")\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [Model Inference Overview](https://docs.databricks.com/aws/en/machine-learning/model-inference/)\n",
    "- [Deep Learning Model Inference](https://docs.databricks.com/aws/en/machine-learning/model-inference/dl-model-inference)\n",
    "- [MLflow End-to-End Example](https://docs.databricks.com/aws/en/notebooks/source/mlflow/mlflow-classic-ml-e2e-mlflow-3.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THESE VALUES!\n",
    "catalog = \"your_catalog\"    # Update: Change to your catalog name\n",
    "schema = \"your_username\"    # Update: Use your username (without special characters)\n",
    "\n",
    "# Your registered model name from AutoML or MLflow notebook\n",
    "model_name = f\"{catalog}.{schema}.iot_defect_predictor\"\n",
    "\n",
    "print(f\"âœ… Using catalog: {catalog}\")\n",
    "print(f\"âœ… Using schema: {schema}\")\n",
    "print(f\"ðŸ¤– Model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch Inference: Score Large Datasets\n",
    "\n",
    "**Use Case:** You need to score all 400,000 sensor readings to identify which devices are at risk of defects.\n",
    "\n",
    "**Batch inference** processes large datasets all at once - perfect for:\n",
    "- Daily/weekly scoring runs\n",
    "- Historical analysis\n",
    "- Bulk predictions\n",
    "\n",
    "### Method 1: Load Model and Apply with PySpark\n",
    "\n",
    "This is the most common approach for large-scale batch inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "\n",
    "# Load the Champion model\n",
    "model_uri = f\"models:/{model_name}@Champion\"\n",
    "print(f\"Loading model from: {model_uri}\")\n",
    "\n",
    "# Load model as a Spark UDF (User Defined Function)\n",
    "# This lets us apply the model to Spark DataFrames at scale\n",
    "predict_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark, \n",
    "    model_uri=model_uri,\n",
    "    result_type=\"double\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded as Spark UDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sensor data for scoring\n",
    "sensor_data = spark.table(f\"{catalog}.{schema}.sensor_bronze\")\n",
    "\n",
    "# Apply model to make predictions\n",
    "# The UDF applies the model to each row in parallel across the cluster\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "predictions_df = sensor_data.withColumn(\n",
    "    \"defect_probability\",\n",
    "    predict_udf(\n",
    "        \"airflow_rate\",\n",
    "        \"rotation_speed\",\n",
    "        \"air_pressure\",\n",
    "        \"temperature\",\n",
    "        \"delay\",\n",
    "        \"density\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a binary prediction (threshold at 0.5)\n",
    "predictions_df = predictions_df.withColumn(\n",
    "    \"predicted_defect\",\n",
    "    when(col(\"defect_probability\") > 0.5, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Show high-risk devices\n",
    "print(\"ðŸ”´ High-Risk Devices (predicted defects):\")\n",
    "predictions_df.filter(\"predicted_defect = 1\") \\\n",
    "    .select(\"device_id\", \"timestamp\", \"temperature\", \"rotation_speed\", \"defect_probability\") \\\n",
    "    .orderBy(col(\"defect_probability\").desc()) \\\n",
    "    .limit(20) \\\n",
    "    .display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to a table for downstream use\n",
    "predictions_table = f\"{catalog}.{schema}.sensor_predictions_batch\"\n",
    "\n",
    "predictions_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(predictions_table)\n",
    "\n",
    "print(f\"âœ… Predictions saved to: {predictions_table}\")\n",
    "print(f\"   Total predictions: {predictions_df.count():,}\")\n",
    "print(f\"   Predicted defects: {predictions_df.filter('predicted_defect = 1').count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming Inference: Real-Time Monitoring\n",
    "\n",
    "**Use Case:** New sensor readings arrive continuously from aircraft. You want to score them in real-time and alert on predicted defects within seconds.\n",
    "\n",
    "**Structured Streaming** + MLflow lets you apply models to streaming data.\n",
    "\n",
    "### How Streaming Inference Works\n",
    "\n",
    "```\n",
    "Sensor Data Stream â†’ Load Model UDF â†’ Apply Predictions â†’ Write to Table â†’ Alert if Defect\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame (simulated - reads from table as stream)\n",
    "# In production, this would connect to Kafka, Event Hub, or Kinesis\n",
    "streaming_sensor_data = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .table(f\"{catalog}.{schema}.sensor_bronze\")\n",
    "\n",
    "# Apply the model UDF to streaming data\n",
    "streaming_predictions = streaming_sensor_data.withColumn(\n",
    "    \"defect_probability\",\n",
    "    predict_udf(\n",
    "        \"airflow_rate\",\n",
    "        \"rotation_speed\",\n",
    "        \"air_pressure\",\n",
    "        \"temperature\",\n",
    "        \"delay\",\n",
    "        \"density\"\n",
    "    )\n",
    ").withColumn(\n",
    "    \"predicted_defect\",\n",
    "    when(col(\"defect_probability\") > 0.5, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"âœ… Created streaming pipeline with model predictions\")\n",
    "print(\"   Each new sensor reading will be scored in real-time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Serving: REST API Endpoints\n",
    "\n",
    "**Use Case:** Your operations dashboard needs to call the model via API to show real-time risk scores. External applications need to integrate with your predictions.\n",
    "\n",
    "**Model Serving** deploys your model as a REST API endpoint.\n",
    "\n",
    "### Step 1: Create a Serving Endpoint (UI)\n",
    "\n",
    "**In the Databricks UI:**\n",
    "\n",
    "1. Click **Machine Learning** in the sidebar\n",
    "2. Click **Serving**\n",
    "3. Click **Create Serving Endpoint**\n",
    "4. Configure:\n",
    "   - **Name**: `iot-defect-predictor`\n",
    "   - **Model**: Select your registered model (`iot_defect_predictor`)\n",
    "   - **Model version**: Select \"Champion\" alias\n",
    "   - **Compute size**: Small (for testing)\n",
    "   - **Scale to zero**: Enabled (saves costs when idle)\n",
    "5. Click **Create**\n",
    "\n",
    "**Wait 5-10 minutes** for the endpoint to deploy.\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "- Databricks deployed your model as a managed REST API\n",
    "- Auto-scaling handles traffic spikes\n",
    "- Monitoring tracks latency and throughput\n",
    "- Authentication is handled automatically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test the API and Use AI Query\n",
    "\n",
    "Once deployed, you can call the endpoint from Python, SQL (`ai_query`), or any HTTP client.\n",
    "\n",
    "**âš ï¸ Note:** To use `ai_query` in SQL, your model must be deployed as a Model Serving endpoint first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Example: Using ai_query for SQL-based inference\n",
    "-- (Uncomment after deploying model serving endpoint)\n",
    "\n",
    "-- SELECT \n",
    "--     device_id,\n",
    "--     timestamp,\n",
    "--     temperature,\n",
    "--     rotation_speed,\n",
    "--     ai_query(\n",
    "--         'iot-defect-predictor',\n",
    "--         airflow_rate,\n",
    "--         rotation_speed,\n",
    "--         air_pressure,\n",
    "--         temperature,\n",
    "--         delay,\n",
    "--         density\n",
    "--     ) as defect_probability\n",
    "-- FROM sensor_bronze\n",
    "-- WHERE timestamp >= CURRENT_DATE - INTERVAL 1 DAY\n",
    "-- ORDER BY defect_probability DESC\n",
    "-- LIMIT 20;\n",
    "\n",
    "SELECT 'âœ… AI Query syntax ready - uncomment after deploying serving endpoint' as status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned how to deploy ML models for production inference.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "âœ… **Batch inference** - Scored large datasets with Spark UDFs  \n",
    "âœ… **Streaming inference** - Applied models to real-time data streams  \n",
    "âœ… **Model serving** - Learned how to deploy REST API endpoints  \n",
    "âœ… **AI Query** - Saw SQL-based inference examples  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Multiple deployment patterns** - Choose based on use case (batch, streaming, API)\n",
    "2. **Spark UDFs** - Scale ML inference across distributed data\n",
    "3. **Model Serving** - Managed REST APIs with auto-scaling\n",
    "4. **AI Query** - SQL-native inference for dashboards and BI tools\n",
    "5. **Same model, different patterns** - One trained model, multiple deployment options\n",
    "\n",
    "### Comparing Inference Methods\n",
    "\n",
    "| Method | Best For | Latency | Scale | Complexity |\n",
    "|--------|----------|---------|-------|------------|\n",
    "| **Batch (Spark UDF)** | Daily scoring, large datasets | Minutes | Millions of rows | Low |\n",
    "| **Streaming (Spark UDF)** | Continuous monitoring | Seconds | Thousands/sec | Medium |\n",
    "| **API Serving** | Interactive apps, low latency | Milliseconds | Hundreds/sec | Medium |\n",
    "| **AI Query (SQL)** | Dashboards, SQL tools | Milliseconds | Hundreds/sec | Low |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Your aircraft IoT ML system is now complete:\n",
    "- âœ… Data discovered and transformed (Day 1)\n",
    "- âœ… Dashboards and Genie spaces built (Day 1 & 2)\n",
    "- âœ… Models trained with AutoML and MLflow (Day 2)\n",
    "- âœ… Models deployed for inference (this notebook)\n",
    "\n",
    "**Day 3** covers production operations:\n",
    "- CI/CD for automated deployment\n",
    "- Performance tuning\n",
    "- Monitoring and governance\n",
    "\n",
    "**Leadership's deadline is in sight!** ðŸŽ¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try This Out\n",
    "\n",
    "Want to go further? Try these exercises:\n",
    "\n",
    "### 1. Schedule Batch Inference Job\n",
    "\n",
    "Automate daily scoring:\n",
    "1. Create a new notebook with just the batch inference code\n",
    "2. In **Workflows**, create a new job\n",
    "3. Add your inference notebook as a task\n",
    "4. Schedule for 2 AM daily\n",
    "5. Add notification on completion\n",
    "\n",
    "**Result:** Fresh predictions every morning before leadership arrives!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Build a Real-Time Risk Dashboard\n",
    "\n",
    "Create a dashboard that shows:\n",
    "- Current devices with high defect risk (from predictions)\n",
    "- Risk trends over time\n",
    "- Alert counts by factory\n",
    "- Model prediction distribution\n",
    "\n",
    "Use the `sensor_predictions_batch` table as the data source!\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Deploy Your Model Serving Endpoint\n",
    "\n",
    "Follow the steps in section 3 to:\n",
    "1. Deploy your model as a serving endpoint\n",
    "2. Test the API with Python\n",
    "3. Try `ai_query` from SQL\n",
    "4. Add the endpoint to your Genie space\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Create an Alert System\n",
    "\n",
    "Build a streaming query that writes high-risk predictions to a separate table:\n",
    "\n",
    "```python\n",
    "# Filter for high-risk predictions\n",
    "alerts = streaming_predictions.filter(\"defect_probability > 0.8\")\n",
    "\n",
    "# Write to alerts table\n",
    "alerts.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .toTable(f\"{catalog}.{schema}.defect_alerts\")\n",
    "```\n",
    "\n",
    "Then create a dashboard that shows current alerts!\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Model Inference Guide](https://docs.databricks.com/aws/en/machine-learning/model-inference/)\n",
    "- [Model Serving](https://docs.databricks.com/aws/en/machine-learning/model-serving/)\n",
    "- [Structured Streaming + ML](https://docs.databricks.com/aws/en/structured-streaming/apply-ml-models.html)\n",
    "\n",
    "**ðŸŽ‰ Congratulations!** You've completed the full ML lifecycle: data â†’ training â†’ deployment. You're ready for Day 3!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f37c8d-9faa-4a97-bf50-e27160a7d891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "6 ML and AI Inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
