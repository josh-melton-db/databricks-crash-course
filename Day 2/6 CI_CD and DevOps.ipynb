{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CI/CD and DevOps for Databricks\n\nImplement version control, automated testing, and deployment automation for data pipelines.\n\n## What You'll Learn\n\n\u2705 Git integration with Databricks Repos  \n\u2705 Create Databricks Asset Bundles  \n\u2705 Set up CI/CD pipelines  \n\u2705 Deploy across dev/staging/prod  \n\u2705 Manage configurations and secrets  \n\n**References:**\n- [CI/CD Best Practices](https://docs.databricks.com/aws/en/dev-tools/ci-cd/)\n- [Asset Bundles](https://docs.databricks.com/aws/en/dev-tools/bundles/)\n- [Repos](https://docs.databricks.com/aws/en/repos/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. DevOps Best Practices\n\n### Environment Strategy\n\n```\nDevelopment \u2192 Staging \u2192 Production\n```\n\n**Dev Environment:**\n- Individual developer workspaces\n- Rapid iteration\n- Sample datasets\n\n**Staging:**\n- Pre-production testing\n- Full dataset\n- Performance validation\n\n**Production:**\n- Live workloads\n- Monitoring and alerts\n- Change control\n\n### Git Workflow\n\n**Branching Strategy:**\n```\nmain (production)\n  \u2191\nstaging\n  \u2191\ndevelop\n  \u2191\nfeature/add-new-pipeline\n```\n\n**Process:**\n1. Create feature branch\n2. Develop and test locally\n3. Create PR to develop\n4. Review and merge\n5. Deploy to staging\n6. Validate\n7. Promote to production\n\n---\n\n## 2. Databricks Repos\n\n### Setup Git Integration\n\n**Step 1: Connect Git Provider**\n1. User Settings \u2192 Git Integration\n2. Add GitHub/GitLab token\n3. Configure SSH keys (optional)\n\n**Step 2: Clone Repository**\n```\nRepos \u2192 Add Repo\nURL: https://github.com/your-org/iot-pipeline\nBranch: main\nPath: /Repos/your-user/iot-pipeline\n```\n\n**Step 3: Development Workflow**\n```bash\n# Create feature branch\ngit checkout -b feature/new-transformation\n\n# Make changes in Databricks\n# Save notebooks\n\n# Commit from UI or CLI\ngit add .\ngit commit -m \"Add temperature normalization\"\ngit push origin feature/new-transformation\n\n# Create PR on GitHub/GitLab\n```\n\n---\n\n## 3. Databricks Asset Bundles\n\n### What are Bundles?\n\n**Asset Bundles** define all workspace resources as code:\n- Notebooks\n- Jobs\n- Pipelines\n- Dashboards\n- ML models\n- Permissions\n\n### Bundle Structure\n\n```\niot-pipeline/\n\u251c\u2500\u2500 databricks.yml       # Main configuration\n\u251c\u2500\u2500 resources/\n\u2502   \u251c\u2500\u2500 jobs.yml\n\u2502   \u251c\u2500\u2500 pipelines.yml\n\u2502   \u2514\u2500\u2500 experiments.yml\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 pipelines/\n\u2502   \u2514\u2500\u2500 notebooks/\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 integration/\n```\n\n### Example databricks.yml\n\n```yaml\nbundle:\n  name: iot-pipeline\n  \ntargets:\n  dev:\n    mode: development\n    workspace:\n      host: https://dev.cloud.databricks.com\n    \n  prod:\n    mode: production\n    workspace:\n      host: https://prod.cloud.databricks.com\n\nresources:\n  jobs:\n    iot_daily_pipeline:\n      name: \"IoT Daily Pipeline - ${bundle.target}\"\n      tasks:\n        - task_key: ingest\n          notebook_task:\n            notebook_path: ${workspace.file_path}/src/pipelines/ingest.py\n          new_cluster:\n            spark_version: \"13.3.x-scala2.12\"\n            node_type_id: \"i3.xlarge\"\n            num_workers: 2\n      schedule:\n        quartz_cron_expression: \"0 0 6 * * ?\"\n        timezone_id: \"America/Los_Angeles\"\n```\n\n### Deploy with Bundles\n\n```bash\n# Validate configuration\ndatabricks bundle validate\n\n# Deploy to dev\ndatabricks bundle deploy -t dev\n\n# Deploy to prod\ndatabricks bundle deploy -t prod\n\n# Run deployed job\ndatabricks bundle run iot_daily_pipeline -t prod\n```\n\n---\n\n## 4. CI/CD Pipeline\n\n### GitHub Actions Example\n\n**.github/workflows/deploy.yml:**\n```yaml\nname: Deploy Databricks Pipeline\n\non:\n  push:\n    branches: [main, staging]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n      \n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install pytest\n      \n      - name: Run tests\n        run: pytest tests/\n  \n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.event_name == 'push'\n    \n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Install Databricks CLI\n        run: pip install databricks-cli\n      \n      - name: Deploy to Staging\n        if: github.ref == 'refs/heads/staging'\n        env:\n          DATABRICKS_HOST: ${{ secrets.DATABRICKS_STAGING_HOST }}\n          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_STAGING_TOKEN }}\n        run: |\n          databricks bundle deploy -t staging\n      \n      - name: Deploy to Production\n        if: github.ref == 'refs/heads/main'\n        env:\n          DATABRICKS_HOST: ${{ secrets.DATABRICKS_PROD_HOST }}\n          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_PROD_TOKEN }}\n        run: |\n          databricks bundle deploy -t prod\n```\n\n### Automated Testing\n\n**Unit Tests:**\n```python\n# tests/test_transformations.py\nimport pytest\nfrom src.pipelines import transformations\n\ndef test_temperature_conversion():\n    result = transformations.fahrenheit_to_celsius(32)\n    assert result == 0.0\n\ndef test_anomaly_detection():\n    data = [60, 65, 70, 95, 68]  # 95 is anomaly\n    anomalies = transformations.detect_anomalies(data, threshold=2)\n    assert 95 in anomalies\n```\n\n**Integration Tests:**\n```python\n# tests/integration/test_pipeline.py\nfrom pyspark.sql import SparkSession\n\ndef test_full_pipeline(spark):\n    # Load sample data\n    input_df = spark.read.csv(\"tests/fixtures/sample_sensors.csv\")\n    \n    # Run pipeline\n    result_df = run_pipeline(input_df)\n    \n    # Assertions\n    assert result_df.count() > 0\n    assert \"temperature_celsius\" in result_df.columns\n    assert result_df.filter(\"temperature < -50\").count() == 0\n```\n\n---\n\n## 5. Configuration Management\n\n### Environment-Specific Config\n\n**config/dev.yml:**\n```yaml\ncatalog: dev_catalog\nschema: iot_dev\nwarehouse_id: dev_warehouse\ncluster_size: 2\ndata_path: /Volumes/dev_catalog/iot_dev/data\n```\n\n**config/prod.yml:**\n```yaml\ncatalog: prod_catalog\nschema: iot_prod\nwarehouse_id: prod_warehouse\ncluster_size: 10\ndata_path: /Volumes/prod_catalog/iot_prod/data\n```\n\n### Secrets Management\n\n```python\n# Access secrets in notebooks\ndb_password = dbutils.secrets.get(scope=\"prod-secrets\", key=\"database-password\")\napi_key = dbutils.secrets.get(scope=\"prod-secrets\", key=\"api-key\")\n\n# Never hardcode secrets!\n# \u274c password = \"my-secret-password\"\n# \u2705 password = dbutils.secrets.get(\"scope\", \"key\")\n```\n\n---\n\n## Summary\n\n\u2705 **Git Repos** - Version control for notebooks and code  \n\u2705 **Asset Bundles** - Infrastructure as code  \n\u2705 **CI/CD Pipelines** - Automated testing and deployment  \n\u2705 **Multi-environment** - Dev, staging, production  \n\u2705 **Configuration** - Environment-specific settings  \n\n### Key Takeaways:\n\n1. **Use Git** for all code and notebooks\n2. **Asset Bundles** for deployments\n3. **Automated testing** before production\n4. **Separate environments** with proper promotion\n5. **Never commit secrets** to Git\n\n---\n\n**Additional Resources:**\n- [CI/CD Guide](https://docs.databricks.com/aws/en/dev-tools/ci-cd/)\n- [Bundles Tutorial](https://docs.databricks.com/aws/en/dev-tools/bundles/jobs-tutorial)\n- [Repos Documentation](https://docs.databricks.com/aws/en/repos/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}